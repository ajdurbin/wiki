<https://github.com/keon/awesome-nlp>

# bag of words model #

Text is represented as the bag or multiset of its words. It disregards grammar and word order and only keeps the multiplicity of each word.

So "John likes to watch movies. Mary likes movies too." is decomposed into "John","likes","to","watch","movies","Mary","likes","movies","too" and then the bag of words representation would look like `{"John":1,"likes":2,"to":1,"watch":1,"movies":2,"Mary":1,"too":1}`.

# tfidf #

Term frequency-inverse document frequency. Statistical measure to evaluate how important a word is to a document in a corpus. Importance increase proportionally to the number of times a word appears in a document but is offset by the frequency of the word in the corpus.

tfidf can be used in a search scheme in scoring and ranking a document's relevance to a query. The simplest ranking function is summing the tfidf for each query term.

It is successively used for stop word filtering.

Composed of two terms. Term frequency is the number of times the word appears in a document, divided by the total number of words in that document. Inverse document frequency is the logarithm of the number of documents in the corpus divided by the number of documents where the term appears.

# latent semantic indexing/analysis #
Analyzes documents to find the underlying meaning or concepts of the documents. Basically SVD on tfidf/bow vectors.

Problem setting: Comparing words to find relevant documents, because we really want to do is compare meanings or concepts behind the words. LSI maps the words and documents into a vector space to do the comparison there. Concepts are obscured with different word choices, introducing noise. LSA filters out some of the noise to find the smallest set of concepts that span all the documents.

Idea:
- Documents are represented as bags of words
- Topics/concepts are represented as patterns of words that usually appear together in documents
- Words are assumed to have a single meaning to make the problem tractable
- Usually use tfidf on the bag of words befor performing LSI
- Essentially performing SVD on the matrix of word counts

Application:
- Cluster documents/words
- Feature for classification

# latent Dirichlet analysis #
Represents documents as mixtures of topics that spit out words with certain probabilities. Assumes that documents are produced following:
- N words in each document follow a distribution, maybe Poisson
- There is a topic mixture in each document following a Dirichlet distribution over K topics
- Each word in the document is generated by first picking a topic according to the multinomial distribution above. The topic then generates the word itself.
LDA then tries to use the documents to infer the set of topics that generated them.

We have a corpus with K topics to discover a priori. Using collapsed Gibbs sampling, LDA:
- goes through each document and randomly assigns each word to one of the K topics
- for each word in each document and for each topic, compute the proportion of words in the document that are assigned to that topic and the proportion of assigned words to that topic over all documents that come from this word. We then reassign the word a different topic and compute the posterior. So we're updating the assignment of our current word using our model of how all the other words are assigned.
- iterate until our assignments are stable.
- Use these assignments to estimate the topic mixtures of each document using proportion of words assigned to each topic within that document and the words associated to each topic overall.

Essential usage is dimension reduction and uncovering the underlying themes in your corpora.

# word2vec #
Learns relationships between words essentially. Uses the context in which words are used to infer their similarities. Uses a neural network with few layers to predict the current word based on the context. We use the weights of the hidden layers as the embeddings or actual word vectors. We can then use cosine similarity or other distance metrics to compute word similarities or use the word vectors for classification/clustering. Also useful for sentiment analysis, where we use the similarity scores. Uses skip gram or continuous bag of words for these probability calculations.

# doc2vec #
Similar idea to word2vec that extends the method to unsupervised learning of continuous representations for larger blocks of text like sentences, paragraphs, or entire documents. Again. clustering or similarity applications.

# Word Mover's Distance #
Submit a query and return the most relevant documents. Assess the distance between documents in a meaningful way, even when they have no words in common. It does use word2vec word embedding. Helpful that word vectors are normalized. Also uses euclidean distance instead of cosine similarity.

# Topic Modelling Terminology #
- If document classification is assigning a single category to a text, topic modelling is assigning multiple tags to a text.
- Coherence: Compute sum of pairwise scores of top n words used to describe a topic. There are a few different measures of this. Good models generate coherent topics, ie, topics with high coherence scores.
- Perplexity: Measure of how well a probability distribution or model predicts a sample. In LDA, topics are described by a probability distribution over vocabulary words. So it can be used to evaluate the topic-term distribution output. Good models have low perplexity.
- Topic Difference: Calculates the distance between two topic models. It's calculated based on the topics either using their probability distributions over vocabulary words or using the common vocabulary words between the topics from both models. By increasing epochs in both models, the distance between identical topics should decrease.
- Convergence: Sum of the difference between all identical topics from consecutive epochs. Models have converged when the convergence stops decreasing with increasing epochs.

# Topic Modelling Uses #
- text classification by grouping similar words together
- recommendation systems: Use similarity measures to build recommender systems that recommend text with a topic structure similar to what our current text
- Uncovering themes in text

# Dynamic Topic Modeling #

# Author-Topic Modeling #

# Sentiment Analysis #
Analyse the opinion or tone of text. Polarity analysis identifies tones as positive or negative. Categorisation can in addition identify confused or angry. There is also an emotion scale 'sad' to 'happy' and from 0-10.

Can attempt to answer questions such as:
- Is this review positive or negative?
- What do people think about this product?
- What do people think about this person or issue?
- Analyse attitude trends over time

# Named Entity Recognition #
Attempts to extract persons, organisations, locations, dates, monetary amounts, etc by looking at nouns and co-occurrence.

# Event Extraction #=
Step above NER. Tries to extract relational information like "Company A is acquiring company B".

# Document Clustering #
Applications in automatic document organisation, topic extraction, and information retrieval or filtering. Use standard clustering techniques like hierarchical/agglomerative or KMeans. Dimensionality reduction techniques can be applied beforehand as well using lsi/truncated svd on tfidf/lda. The next problem is selecting descriptive human-readable labels for the clusters. There are a few different techniques that examine the contents of the documents per cluster to find a labeling that summarize the topic of each cluster to distinguish it from others. There is differential cluster labeling and internal cluster labeling.

To cluster:
- tokenize
- stem/lemmatize
- stopwords/punctutation removal
- tfidf or term frequencies
- cluster
- evaluate using metrics
