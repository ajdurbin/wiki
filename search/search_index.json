{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome!\n\n\nWhat's this and what's it for?\n\n\nThis is my personal wiki. It will contain relevant information on software installations \n configurations, statistical and machine learning techniques and descriptions, and other related topics. \n\n\nWhat's it going to look like?\n\n\nThis wiki will attempt to be organized by broad topic like installation guide, statistics, etc. Longer pages will have a table of contents. Relevant links to external sources will also be provided in the pages. Code snippets and images will also be included.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome", 
            "text": "", 
            "title": "Welcome!"
        }, 
        {
            "location": "/#whats-this-and-whats-it-for", 
            "text": "This is my personal wiki. It will contain relevant information on software installations   configurations, statistical and machine learning techniques and descriptions, and other related topics.", 
            "title": "What's this and what's it for?"
        }, 
        {
            "location": "/#whats-it-going-to-look-like", 
            "text": "This wiki will attempt to be organized by broad topic like installation guide, statistics, etc. Longer pages will have a table of contents. Relevant links to external sources will also be provided in the pages. Code snippets and images will also be included.", 
            "title": "What's it going to look like?"
        }, 
        {
            "location": "/books/", 
            "text": "Machine Learning\n\n\n\n\nIntroduction To Machine Learning With Python\n\n\nIntroduction To Statistical Learning\n\n\nElements Of Statistical Learning\n\n\nDeep Learning With Python\n\n\n\n\nUnix\n\n\n\n\nMastering Regular Expressions\n\n\n\n\nThe Linux Command Line\n\n\n\n\n\n\nmining the social web\n\n\n\n\nintroduction to information retrieval\n\n\nnltk book html only\n\n\ngit book", 
            "title": "Books"
        }, 
        {
            "location": "/books/#machine-learning", 
            "text": "Introduction To Machine Learning With Python  Introduction To Statistical Learning  Elements Of Statistical Learning  Deep Learning With Python", 
            "title": "Machine Learning"
        }, 
        {
            "location": "/books/#unix", 
            "text": "Mastering Regular Expressions   The Linux Command Line    mining the social web   introduction to information retrieval  nltk book html only  git book", 
            "title": "Unix"
        }, 
        {
            "location": "/websites/", 
            "text": "Digital Ocean Tutorials\n\n\ngithub awesome pages", 
            "title": "Websites"
        }, 
        {
            "location": "/bash/git/", 
            "text": "Merge two different repositories\n\n\nBe sure to remove merge conflicts from repositories, like multiple \nREADME.md\n, \n.gitignore\n\n\ncd /projectb/path\ngit remote add projecta /projecta/path\ngit fetch projecta\ngit merge projecta/master\ngit remote remove projecta\n\n\n\n\nMultiple Remotes\n\n\nChange origin remote URL: \nhttps://help.github.com/articles/changing-a-remote-s-url/\n\n\nRemove remote: \nhttps://help.github.com/articles/removing-a-remote/\n\n\nOrigin should be set to internal Gogs server\n\n\nAdd BitBucket and Github remotes with \n\n\ngit remote add bitbucket https://bitbucket.com/ajd2/repo.git # or use SSH connection\ngit push bitbucket master\n\n\n\n\nList large files in git repositry\n\n\nhttps://confluence.atlassian.com/bitbucket/reduce-repository-size-321848262.html\n\n\nhttps://rtyley.github.io/bfg-repo-cleaner/\n\n\nAdd this shell script to repository\n\n\n#!/bin/bash\n#set -x \n\n# Shows you the largest objects in your repo's pack file.\n# Written for osx.\n#\n# @see http://stubbisms.wordpress.com/2009/07/10/git-script-to-show-largest-pack-objects-and-trim-your-waist-line/\n# @author Antony Stubbs\n\n# set the internal field spereator to line break, so that we can iterate easily over the verify-pack output\nIFS=$'\\n';\n\n# list all objects including their size, sort by size, take top 10\nobjects=`git verify-pack -v .git/objects/pack/pack-*.idx | grep -v chain | sort -k3nr | head`\n\necho \nAll sizes are in kB's. The pack column is the size of the object, compressed, inside the pack file.\n\n\noutput=\nsize,pack,SHA,location\n\nfor y in $objects\ndo\n    # extract the size in bytes\n    size=$((`echo $y | cut -f 5 -d ' '`/1024))\n    # extract the compressed size in bytes\n    compressedSize=$((`echo $y | cut -f 6 -d ' '`/1024))\n    # extract the SHA\n    sha=`echo $y | cut -f 1 -d ' '`\n    # find the objects location in the repository tree\n    other=`git rev-list --all --objects | grep $sha`\n    #lineBreak=`echo -e \n\\n\n`\n    output=\n${output}\\n${size},${compressedSize},${other}\n\ndone\n\necho -e $output | column -t -s ', '\n\n\n\n\nMake it executable with \nchmod +x git_find_big.sh\n, do garbage collection \ngit gc --auto\n, \ndu -hs .git/objects\n, and then list large files with \n./git_find_big.sh\n\n\nRemoving Large Files from repository history\n\n\nFor all those pesky \nRData\n and \ncsv\n files accidentally committed.\n\n\nBFG Repo Cleaner, \napt install default-jre\n and then \nwget\n the \n.jar\n from the documentation.\n\n\nMake a copy of your bare repo, \ngit clone --mirror https://myrepo/user/repo.git\n\n\nClean repo with \njava -jar /path/to/bfg.jar --strip-blobs-bigger-than 100M repo.git\n or \njava -jar /path/to/bfg.jar --delete-files *.ext repo.git\n\n\nWhen done making changes\n\n\ncd repo.git\ngit reflog expire --expire=now --all \n git gc --prune=now --aggressive\ngit push", 
            "title": "Git"
        }, 
        {
            "location": "/bash/git/#merge-two-different-repositories", 
            "text": "Be sure to remove merge conflicts from repositories, like multiple  README.md ,  .gitignore  cd /projectb/path\ngit remote add projecta /projecta/path\ngit fetch projecta\ngit merge projecta/master\ngit remote remove projecta", 
            "title": "Merge two different repositories"
        }, 
        {
            "location": "/bash/git/#multiple-remotes", 
            "text": "Change origin remote URL:  https://help.github.com/articles/changing-a-remote-s-url/  Remove remote:  https://help.github.com/articles/removing-a-remote/  Origin should be set to internal Gogs server  Add BitBucket and Github remotes with   git remote add bitbucket https://bitbucket.com/ajd2/repo.git # or use SSH connection\ngit push bitbucket master", 
            "title": "Multiple Remotes"
        }, 
        {
            "location": "/bash/git/#list-large-files-in-git-repositry", 
            "text": "https://confluence.atlassian.com/bitbucket/reduce-repository-size-321848262.html  https://rtyley.github.io/bfg-repo-cleaner/  Add this shell script to repository  #!/bin/bash\n#set -x \n\n# Shows you the largest objects in your repo's pack file.\n# Written for osx.\n#\n# @see http://stubbisms.wordpress.com/2009/07/10/git-script-to-show-largest-pack-objects-and-trim-your-waist-line/\n# @author Antony Stubbs\n\n# set the internal field spereator to line break, so that we can iterate easily over the verify-pack output\nIFS=$'\\n';\n\n# list all objects including their size, sort by size, take top 10\nobjects=`git verify-pack -v .git/objects/pack/pack-*.idx | grep -v chain | sort -k3nr | head`\n\necho  All sizes are in kB's. The pack column is the size of the object, compressed, inside the pack file. \n\noutput= size,pack,SHA,location \nfor y in $objects\ndo\n    # extract the size in bytes\n    size=$((`echo $y | cut -f 5 -d ' '`/1024))\n    # extract the compressed size in bytes\n    compressedSize=$((`echo $y | cut -f 6 -d ' '`/1024))\n    # extract the SHA\n    sha=`echo $y | cut -f 1 -d ' '`\n    # find the objects location in the repository tree\n    other=`git rev-list --all --objects | grep $sha`\n    #lineBreak=`echo -e  \\n `\n    output= ${output}\\n${size},${compressedSize},${other} \ndone\n\necho -e $output | column -t -s ', '  Make it executable with  chmod +x git_find_big.sh , do garbage collection  git gc --auto ,  du -hs .git/objects , and then list large files with  ./git_find_big.sh", 
            "title": "List large files in git repositry"
        }, 
        {
            "location": "/bash/git/#removing-large-files-from-repository-history", 
            "text": "For all those pesky  RData  and  csv  files accidentally committed.  BFG Repo Cleaner,  apt install default-jre  and then  wget  the  .jar  from the documentation.  Make a copy of your bare repo,  git clone --mirror https://myrepo/user/repo.git  Clean repo with  java -jar /path/to/bfg.jar --strip-blobs-bigger-than 100M repo.git  or  java -jar /path/to/bfg.jar --delete-files *.ext repo.git  When done making changes  cd repo.git\ngit reflog expire --expire=now --all   git gc --prune=now --aggressive\ngit push", 
            "title": "Removing Large Files from repository history"
        }, 
        {
            "location": "/bash/regex/", 
            "text": "Usage\n\n\n\n\ngrep regex file\n\n\ngrep -i regex file\n is case insensitive matching\n\n\nPython \nre\n module\n\n\n\n\nMetacharacter Reference\n\n\n\n\n*\n: Match anything, eg, \n*.txt$\n matches anything that ends in \n.txt\n.\n\n\n?\n: Optional match a character, eg, \n?at\n matches all of 'cat', 'rat', 'sat', \nat\n, etc.\n\n\n^\n: Start of a line. Or negation inside \n[^abc]\n\n\n$\n: End of a line.\n\n\n[..]\n: Character class. Explicitly list the characters we want to match, eg, \n[aeiou]\n matches any vowels, \n[A-Za-z]\n matches any alphabetical character, \n[0-9]\n matches any digit, \n[^ae]\n matches everything but 'a' or 'e'. Note that \n-\n inside a character class is only considered a character if it's first.\n\n\n.\n matches any single character.\n\n\n|\n means 'or' and is often used with parentheses to constrain the alternation, eg, \ngr(a|e)y\n. Note that \ngr[a|e]y\n is not valid because \n|\n is interpreted as a character and not a metacharacter inside brackets, also \ngr[ae]y\n works.\n\n\n\\\nword\\\n are word boundary metacharacters, this example matches 'word' and not 'stopwords'.\n\n\n+\n matches one or more of the immediately preceding item. \n\n\n\n\nAlternation Versus Character Class\n\n\nWhile \ngr[ae]y\n and \ngr(a|e)y\n are equivalent, character classes only match single characters. Using alternation, we can match full words, eg, \n^(From|Subject):\n matches either of those words.", 
            "title": "Regex"
        }, 
        {
            "location": "/bash/regex/#usage", 
            "text": "grep regex file  grep -i regex file  is case insensitive matching  Python  re  module", 
            "title": "Usage"
        }, 
        {
            "location": "/bash/regex/#metacharacter-reference", 
            "text": "* : Match anything, eg,  *.txt$  matches anything that ends in  .txt .  ? : Optional match a character, eg,  ?at  matches all of 'cat', 'rat', 'sat',  at , etc.  ^ : Start of a line. Or negation inside  [^abc]  $ : End of a line.  [..] : Character class. Explicitly list the characters we want to match, eg,  [aeiou]  matches any vowels,  [A-Za-z]  matches any alphabetical character,  [0-9]  matches any digit,  [^ae]  matches everything but 'a' or 'e'. Note that  -  inside a character class is only considered a character if it's first.  .  matches any single character.  |  means 'or' and is often used with parentheses to constrain the alternation, eg,  gr(a|e)y . Note that  gr[a|e]y  is not valid because  |  is interpreted as a character and not a metacharacter inside brackets, also  gr[ae]y  works.  \\ word\\  are word boundary metacharacters, this example matches 'word' and not 'stopwords'.  +  matches one or more of the immediately preceding item.", 
            "title": "Metacharacter Reference"
        }, 
        {
            "location": "/bash/regex/#alternation-versus-character-class", 
            "text": "While  gr[ae]y  and  gr(a|e)y  are equivalent, character classes only match single characters. Using alternation, we can match full words, eg,  ^(From|Subject):  matches either of those words.", 
            "title": "Alternation Versus Character Class"
        }, 
        {
            "location": "/bash/snippets/", 
            "text": "Monitor GPU Usage\n\n\nwatch -d -n 0.5 nvidia-smi\n\n\nBackground Processes\n\n\nRun commands in the background to check on later using either \ntmux\n or a combination of \nnohup command.sh \n while writing to a file.\n\n\nFree Memory\n\n\nfree -h\n\n\nAdd sudo user\n\n\nadduser username\nusermod -aG sudo username\n\n\n\n\n.vimrc\n\n\n show line numbers\nset number\n\n\n turn on syntax highlighting\nsyntax on\n\n\n tab stuff\nset tabstop=4\nset expandtab\n\n\n text wrapping\nset wrap\n\n\n highlight current line\nset cursorline\n\n\n set text width\nset textwidth=79\n\n\n automatically indent below current line\nset autoindent", 
            "title": "Snippets"
        }, 
        {
            "location": "/bash/snippets/#monitor-gpu-usage", 
            "text": "watch -d -n 0.5 nvidia-smi", 
            "title": "Monitor GPU Usage"
        }, 
        {
            "location": "/bash/snippets/#background-processes", 
            "text": "Run commands in the background to check on later using either  tmux  or a combination of  nohup command.sh   while writing to a file.", 
            "title": "Background Processes"
        }, 
        {
            "location": "/bash/snippets/#free-memory", 
            "text": "free -h", 
            "title": "Free Memory"
        }, 
        {
            "location": "/bash/snippets/#add-sudo-user", 
            "text": "adduser username\nusermod -aG sudo username", 
            "title": "Add sudo user"
        }, 
        {
            "location": "/bash/snippets/#vimrc", 
            "text": "show line numbers\nset number  turn on syntax highlighting\nsyntax on  tab stuff\nset tabstop=4\nset expandtab  text wrapping\nset wrap  highlight current line\nset cursorline  set text width\nset textwidth=79  automatically indent below current line\nset autoindent", 
            "title": ".vimrc"
        }, 
        {
            "location": "/bash/tools/", 
            "text": "top/htop\n\n\ngrep\n\n\nsed\n\n\nawk\n\n\nvim\n\n\npipe operator\n\n\nless/more\n\n\ngit\n\n\npushd/popd\n\n\n!! run last command\n\n\n!* run command with arguments passed to previous command\n\n\n!^ first argument in bash history\n\n\n!$ last arugument in bash history\n\n\n!?keyword? run last command from bash history beginning with keyword\n\n\nrsync\n\n\nps\n\n\ntee for storing/viewing output of another command\n\n\nfind\n\n\ntree", 
            "title": "Tools"
        }, 
        {
            "location": "/deep_learning/scratch/", 
            "text": "About\n\n\nThis page is for miscellaneous notes on deep learning concepts, ideas, and problems. In time these will be expanded into their own pages, but for now these serve as bookmarks.\n\n\nTypes\n\n\n\n\nNeural Network\n\n\nConvolutional Neural Network\n\n\nRecurrent Neural Network (GUR, LSTM)\n\n\nGAN\n\n\n\n\nProblems\n\n\n\n\nImage classification\n\n\nRepresentation Learning\n\n\nAutoEncoder\n\n\nImage to text/caption generation\n\n\nStarry night + photo = Starry night photo\n\n\nImage detection or outline of objects\n\n\nBounding box of faces\n\n\nHeatmap of images for classifier predictions\n\n\nGAN image/text generation\n\n\nTensorBoard\n\n\nAudio transcription, detection, etc\n\n\n\n\nConcepts\n\n\n\n\nNumber of filters, stride lengths, padding\n\n\nData augmentation, ie, rotating images, flipping, shifting, etc\n\n\nGlobal vs local pooling and min/max/average pooling\n\n\nNo pooling in between convolutional layers but at end\n\n\nbatch normalization\n\n\ndilation\n\n\nDropout versus explicit regularization\n\n\nAlternating convolutional layers beginning with descending 128 hidden layers to 64, 32, 16 before pooling with two sets of convolutional layers with stride length two first and one second\n\n\n\n\nconditional random fields\n\n\n\n\n\n\nkaggle datasets\n\n\n\n\ncv datasets on the web for image datasets in computer vision\n\n\nnot hotdog\n\n\nnetflix movies\n\n\noriginalsnapchat filter\n\n\ntwitter stream\n\n\nsignal processing\n\n\ngoogle cloud big data platform \n\n\nvci machine learning repository \n\n\ngithub awesome public datasets\n\n\nautoencoders\n\n\nplaces 365 dataset\n\n\nscikit-image bounding box segmentation\n\n\nsegnet\n\n\ntsosu technique\n\n\nvisualize outputs in CNN model network layers (ir.com blog post)\n\n\nMNIST GAN example from https://towardsdatascience.com/demystifying-generative-adversarial-networks-c076d8db8f44\n\n\ngoogle cloud ml examples, they tag stack overflow posts\n\n\ntwitter/stack-overflow/etc tagging\n\n\nreal-time human pose estimation https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5", 
            "title": "Scratch"
        }, 
        {
            "location": "/deep_learning/scratch/#about", 
            "text": "This page is for miscellaneous notes on deep learning concepts, ideas, and problems. In time these will be expanded into their own pages, but for now these serve as bookmarks.", 
            "title": "About"
        }, 
        {
            "location": "/deep_learning/scratch/#types", 
            "text": "Neural Network  Convolutional Neural Network  Recurrent Neural Network (GUR, LSTM)  GAN", 
            "title": "Types"
        }, 
        {
            "location": "/deep_learning/scratch/#problems", 
            "text": "Image classification  Representation Learning  AutoEncoder  Image to text/caption generation  Starry night + photo = Starry night photo  Image detection or outline of objects  Bounding box of faces  Heatmap of images for classifier predictions  GAN image/text generation  TensorBoard  Audio transcription, detection, etc", 
            "title": "Problems"
        }, 
        {
            "location": "/deep_learning/scratch/#concepts", 
            "text": "Number of filters, stride lengths, padding  Data augmentation, ie, rotating images, flipping, shifting, etc  Global vs local pooling and min/max/average pooling  No pooling in between convolutional layers but at end  batch normalization  dilation  Dropout versus explicit regularization  Alternating convolutional layers beginning with descending 128 hidden layers to 64, 32, 16 before pooling with two sets of convolutional layers with stride length two first and one second   conditional random fields    kaggle datasets   cv datasets on the web for image datasets in computer vision  not hotdog  netflix movies  originalsnapchat filter  twitter stream  signal processing  google cloud big data platform   vci machine learning repository   github awesome public datasets  autoencoders  places 365 dataset  scikit-image bounding box segmentation  segnet  tsosu technique  visualize outputs in CNN model network layers (ir.com blog post)  MNIST GAN example from https://towardsdatascience.com/demystifying-generative-adversarial-networks-c076d8db8f44  google cloud ml examples, they tag stack overflow posts  twitter/stack-overflow/etc tagging  real-time human pose estimation https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5", 
            "title": "Concepts"
        }, 
        {
            "location": "/installation/bittorrent_sync/", 
            "text": "Installation And Configuration\n\n\n\n\nCreate a new container on Superior\n\n\nDo \necho \"deb http://linux-packages.resilio.com/resilio-sync/deb resilio-sync non-free\" | tee /etc/apt/sources.list.d/resilio-sync.list\n\n\nDo \nwget -qO - https://linux-packages.resilio.com/resilio-sync/key.asc | apt-key add -\n\n\nDo \napt-get update \n apt-get install resilio-sync\n\n\nEdit \n/usr/lib/systemd/user/resilio-sync.service\n with \nWantedBy=default.target\n\n\nEnable the service \nsystemctl --user enable resilio-sync\n\n\nChange web GUI listening interface \n./rslsync --webui.listen 0.0.0.0:8888\n\n\nGo to \nhttp://bittorrentsyncip:8888\n with username alex and software password\n\n\nAdd syncing folder under \n/home/rslsync/btsync", 
            "title": "Bittorrent sync"
        }, 
        {
            "location": "/installation/bittorrent_sync/#installation-and-configuration", 
            "text": "Create a new container on Superior  Do  echo \"deb http://linux-packages.resilio.com/resilio-sync/deb resilio-sync non-free\" | tee /etc/apt/sources.list.d/resilio-sync.list  Do  wget -qO - https://linux-packages.resilio.com/resilio-sync/key.asc | apt-key add -  Do  apt-get update   apt-get install resilio-sync  Edit  /usr/lib/systemd/user/resilio-sync.service  with  WantedBy=default.target  Enable the service  systemctl --user enable resilio-sync  Change web GUI listening interface  ./rslsync --webui.listen 0.0.0.0:8888  Go to  http://bittorrentsyncip:8888  with username alex and software password  Add syncing folder under  /home/rslsync/btsync", 
            "title": "Installation And Configuration"
        }, 
        {
            "location": "/installation/freenas/", 
            "text": "Installation\n\n\n\n\nRequires two USB drives, for installation image and installation target\n\n\nChoose FreeNAS installer, select USB drive for installation target\n\n\nFresh install, format boot device\n\n\nHardware password\n\n\nBIOS booting\n\n\nReboot and remove installation media\n\n\nDisplays menu with IP for web GUI\n\n\nOpen web GUI, sign in with root, hardware password\n\n\nExit initial wizard\n\n\nChange hostname to \nhuron.localdomain\n, timezone to America/Detroit\n\n\nStorage -\n Volumes -\n Volume Manager\n to create volume using all disks and ZFS RAID 0 or ZFS Striped\n\n\nCreate datasets, change user and group permissions to nobody, add write privileges to user/owner/group\n\n\nGo to Sharing, select dataset, NSF share, change MapAll user and MapAll groups to nobody so it doesn't matter what user connects to the shares\n\n\nTo mount in Unix, \nmount -t nfs freenasip:/mnt/volumenname/datasetname /path/to/mount\n\n\nThen unmount with \numount /path/to/mount\n\n\n\n\nPlex Plugin\n\n\n\n\nInstall the Plex plugin\n\n\nGo to Jails -\n plexmediaserver -\n Add storage and add the necessary directories, be sure to choose 'create directory'\n\n\nGo to Plugins -\n Installed -\n plexmediaserver and turn on\n\n\nThen go to \nhttp://plexpluginip:32400/web/\n to access server\n\n\nNote that first install attempt did not initiate a plex 'server' only a plex client and reinstallation was necessary\n\n\n\n\nResilio Sync Plugin\n\n\n\n\nInstall BTsync plugin\n\n\nCreate new dataset with write permissions for group, other with nobody:nobody the owner:group users\n\n\nAdd the new dataset under \n/mnt\n \n\n\nOpen the GUI, username alex with software password\n\n\nAdd a read-only key from another host", 
            "title": "Freenas"
        }, 
        {
            "location": "/installation/freenas/#installation", 
            "text": "Requires two USB drives, for installation image and installation target  Choose FreeNAS installer, select USB drive for installation target  Fresh install, format boot device  Hardware password  BIOS booting  Reboot and remove installation media  Displays menu with IP for web GUI  Open web GUI, sign in with root, hardware password  Exit initial wizard  Change hostname to  huron.localdomain , timezone to America/Detroit  Storage -  Volumes -  Volume Manager  to create volume using all disks and ZFS RAID 0 or ZFS Striped  Create datasets, change user and group permissions to nobody, add write privileges to user/owner/group  Go to Sharing, select dataset, NSF share, change MapAll user and MapAll groups to nobody so it doesn't matter what user connects to the shares  To mount in Unix,  mount -t nfs freenasip:/mnt/volumenname/datasetname /path/to/mount  Then unmount with  umount /path/to/mount", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/freenas/#plex-plugin", 
            "text": "Install the Plex plugin  Go to Jails -  plexmediaserver -  Add storage and add the necessary directories, be sure to choose 'create directory'  Go to Plugins -  Installed -  plexmediaserver and turn on  Then go to  http://plexpluginip:32400/web/  to access server  Note that first install attempt did not initiate a plex 'server' only a plex client and reinstallation was necessary", 
            "title": "Plex Plugin"
        }, 
        {
            "location": "/installation/freenas/#resilio-sync-plugin", 
            "text": "Install BTsync plugin  Create new dataset with write permissions for group, other with nobody:nobody the owner:group users  Add the new dataset under  /mnt    Open the GUI, username alex with software password  Add a read-only key from another host", 
            "title": "Resilio Sync Plugin"
        }, 
        {
            "location": "/installation/pfsense/", 
            "text": "Installation\n\n\n\n\nChoose 64-bit memstick version\n\n\nusername admin, hardware password\n\n\nhostname: \nerie\n, domain: \nlocaldomain\n\n\nAdd \nOPT1, OPT2\n interfaces and copy firewall rules \n\n\nCopy \nLAN\n interface configuration to \nOPT\n interfaces\n\n\nChange DNS servers to Google, OpenDNS\n\n\nChange DHCP server available range\n\n\nDo not allow DNS server list to be overridden\n\n\n\n\nPFBlockerNG\n\n\n\n\nFirewall -\n PFBlockerNG\n\n\nControl/Command click all outbound firewall rules except for lan and save\n\n\nGo to \nDNSBL\n\n\nAdd new \nDNSBL\n feeds, add DNS group name, eg pi-hole, description uses default pi-hole lists, add each pi-hole list to \nDNSBL\n and add 'test' for header/description\n\n\nList actio switched to unbound and update every hour \n\n\nSave, update, force update and watch lists download\n\n\nEnable and save\n\n\nDo same with EasyList \nDNSBL\n, check all, label as test and force update again\n\n\n\n\nUse hostnames for SSH\n\n\nFrom \nhttps://www.bytesizedalex.com/pfsense-dns-resolution-for-dhcp-leases/\n. Go to DNS Resolver and check Register DHCP leases and Register DHCP static mappings.", 
            "title": "Pfsense"
        }, 
        {
            "location": "/installation/pfsense/#installation", 
            "text": "Choose 64-bit memstick version  username admin, hardware password  hostname:  erie , domain:  localdomain  Add  OPT1, OPT2  interfaces and copy firewall rules   Copy  LAN  interface configuration to  OPT  interfaces  Change DNS servers to Google, OpenDNS  Change DHCP server available range  Do not allow DNS server list to be overridden", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/pfsense/#pfblockerng", 
            "text": "Firewall -  PFBlockerNG  Control/Command click all outbound firewall rules except for lan and save  Go to  DNSBL  Add new  DNSBL  feeds, add DNS group name, eg pi-hole, description uses default pi-hole lists, add each pi-hole list to  DNSBL  and add 'test' for header/description  List actio switched to unbound and update every hour   Save, update, force update and watch lists download  Enable and save  Do same with EasyList  DNSBL , check all, label as test and force update again", 
            "title": "PFBlockerNG"
        }, 
        {
            "location": "/installation/pfsense/#use-hostnames-for-ssh", 
            "text": "From  https://www.bytesizedalex.com/pfsense-dns-resolution-for-dhcp-leases/ . Go to DNS Resolver and check Register DHCP leases and Register DHCP static mappings.", 
            "title": "Use hostnames for SSH"
        }, 
        {
            "location": "/installation/proxmox/", 
            "text": "Installation\n\n\n\n\nGrab latest Proxmox iso and boot from USB\n\n\nChoose install\n\n\nSelect ZFS RAID 0 and choose hard drives, be sure to not include USB drive here\n\n\nChange timezone to America/Detroit\n\n\nEnter hardware password and email address\n\n\nChange FQDN to \nsuperior.localdomain\n\n\n\n\nConfiguration\n\n\n\n\nComment out \ndeb https://enterprise.proxmox.com/debian/pve stretch pve-enterprise\n in \n/etc/apt/sources.list.d/pve-enterprise.list\n\n\nAdd \ndeb http://download.proxmox.com/debian/pve stretch pve-no-subscription\n to \n/etc/apt/sources.list\n\n\napt-get update \n apt-get dist-upgrade\n\n\nAdd LXC templates at \nDatacenter -\n Superior -\n local (superior) -\n Content -\n Templates\n\n\nIf TurnKey templates are missing, do \npveam update\n\n\n\n\nMount FreeNAS NFS Shares On Boot\n\n\n\n\nMake subdirectories in \n/mnt\n named after datasets to be mounted, ie, \nmkdir /mnt/torrents\n\n\nEdit \n/etc/fstab\n with \nfreenasip:/mnt/volume/datasetname   /mnt/datasetname   nfs    _netdev,auto  0  0\n\n\nReboot and check file systems mounted properly\n\n\n\n\nContainer Bind Mounts\n\n\n\n\nEdit \n/etc/pve/lxc/\ncontainerid\n.conf\n with \nmp0: /proxmox/mount/path,mp=/container/mount/path\n\n\nRestart container and check \ndu -h\n to see available storage\n\n\nFrom \nhttps://pve.proxmox.com/wiki/Linux_Container#_bind_mount_points\n and \nhttps://www.jamescoyle.net/how-to/2019-proxmox-4-x-bind-mount-mount-storage-in-an-lxc-container\n\n\n\n\nContainer SSH\n\n\nSSH isn't enabled in containers by default. For using a 'native' terminal instead of that included in the Proxmox GUI to SSH into a container, SSH into Proxmox then \nlxc-attach --name \ncontainerid\n. From \nhttps://ping.flenny.net/2016/ssh-into-a-proxmox-lxc-container/\n.\n\n\nLocale errors in lxc containers\n\n\nlocale-gen en_US.UTF-8\n\n\naccidents\n\n\ninstalled git and mysql-server outside of container", 
            "title": "Proxmox"
        }, 
        {
            "location": "/installation/proxmox/#installation", 
            "text": "Grab latest Proxmox iso and boot from USB  Choose install  Select ZFS RAID 0 and choose hard drives, be sure to not include USB drive here  Change timezone to America/Detroit  Enter hardware password and email address  Change FQDN to  superior.localdomain", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/proxmox/#configuration", 
            "text": "Comment out  deb https://enterprise.proxmox.com/debian/pve stretch pve-enterprise  in  /etc/apt/sources.list.d/pve-enterprise.list  Add  deb http://download.proxmox.com/debian/pve stretch pve-no-subscription  to  /etc/apt/sources.list  apt-get update   apt-get dist-upgrade  Add LXC templates at  Datacenter -  Superior -  local (superior) -  Content -  Templates  If TurnKey templates are missing, do  pveam update", 
            "title": "Configuration"
        }, 
        {
            "location": "/installation/proxmox/#mount-freenas-nfs-shares-on-boot", 
            "text": "Make subdirectories in  /mnt  named after datasets to be mounted, ie,  mkdir /mnt/torrents  Edit  /etc/fstab  with  freenasip:/mnt/volume/datasetname   /mnt/datasetname   nfs    _netdev,auto  0  0  Reboot and check file systems mounted properly", 
            "title": "Mount FreeNAS NFS Shares On Boot"
        }, 
        {
            "location": "/installation/proxmox/#container-bind-mounts", 
            "text": "Edit  /etc/pve/lxc/ containerid .conf  with  mp0: /proxmox/mount/path,mp=/container/mount/path  Restart container and check  du -h  to see available storage  From  https://pve.proxmox.com/wiki/Linux_Container#_bind_mount_points  and  https://www.jamescoyle.net/how-to/2019-proxmox-4-x-bind-mount-mount-storage-in-an-lxc-container", 
            "title": "Container Bind Mounts"
        }, 
        {
            "location": "/installation/proxmox/#container-ssh", 
            "text": "SSH isn't enabled in containers by default. For using a 'native' terminal instead of that included in the Proxmox GUI to SSH into a container, SSH into Proxmox then  lxc-attach --name  containerid . From  https://ping.flenny.net/2016/ssh-into-a-proxmox-lxc-container/ .", 
            "title": "Container SSH"
        }, 
        {
            "location": "/installation/proxmox/#locale-errors-in-lxc-containers", 
            "text": "locale-gen en_US.UTF-8", 
            "title": "Locale errors in lxc containers"
        }, 
        {
            "location": "/installation/proxmox/#accidents", 
            "text": "installed git and mysql-server outside of container", 
            "title": "accidents"
        }, 
        {
            "location": "/installation/raspberry_pi/", 
            "text": "Installation\n\n\n\n\nInstall latest Raspbian Lite image\n\n\nsudo raspi-config\n to change username and enable ssh\n\n\n\n\nDokuWiki Configuration\n\n\n\n\nsudo apt-get update \n apt-get install vim -y\n\n\nsudo apt-get install dokuwiki -y\n. Will need to enter admin password and purge pages on removal.\n\n\nsudo dpkg-reconfigure dokuwiki\n. Here will be asked which webserer to use, document root address, local network or global network, purge packages on removal, make configuration/plugin directories web writeable, wiki title, license, and acl. Found at \nhttp://techblog.danielpellarini.com/sysadmin/how-to-quickly-install-dokuwiki-on-debian/\n.\n\n\nAfterwards edit \napache.conf\n with \nAllow from all\n\n\nRestart Raspberry Pi \n\n\nGo to \nhttp://raspberrypiip/dokuwiki/\n and install MathJax plugin\n\n\n\n\nDokuWiki Backup\n\n\nCreate \nwikibackup.sh\n with the following contents: \n\n\n#!/bin/bash\n# add namespaces as created\nsudo sh -c \ncp -r /var/lib/dokuwiki/data/pages/*.txt /home/pi/wiki/\n\nsudo sh -c \ncp -r /var/lib/dokuwiki/data/pages/installation /home/pi/wiki/\n\ncd /home/pi/wiki\ngit add .\ngit commit -m 'wiki commit'\ngit push origin master\n\n\n\n\nThen SSH into Raspberry Pi and backup with \n/home/pi/wiki/wikibackup.sh\n. Making this a daily cron job is difficult because of the \nsudo sh -c\n commands. Will continue to look for solutions for auto-commiting  wiki updates.", 
            "title": "Raspberry pi"
        }, 
        {
            "location": "/installation/raspberry_pi/#installation", 
            "text": "Install latest Raspbian Lite image  sudo raspi-config  to change username and enable ssh", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/raspberry_pi/#dokuwiki-configuration", 
            "text": "sudo apt-get update   apt-get install vim -y  sudo apt-get install dokuwiki -y . Will need to enter admin password and purge pages on removal.  sudo dpkg-reconfigure dokuwiki . Here will be asked which webserer to use, document root address, local network or global network, purge packages on removal, make configuration/plugin directories web writeable, wiki title, license, and acl. Found at  http://techblog.danielpellarini.com/sysadmin/how-to-quickly-install-dokuwiki-on-debian/ .  Afterwards edit  apache.conf  with  Allow from all  Restart Raspberry Pi   Go to  http://raspberrypiip/dokuwiki/  and install MathJax plugin", 
            "title": "DokuWiki Configuration"
        }, 
        {
            "location": "/installation/raspberry_pi/#dokuwiki-backup", 
            "text": "Create  wikibackup.sh  with the following contents:   #!/bin/bash\n# add namespaces as created\nsudo sh -c  cp -r /var/lib/dokuwiki/data/pages/*.txt /home/pi/wiki/ \nsudo sh -c  cp -r /var/lib/dokuwiki/data/pages/installation /home/pi/wiki/ \ncd /home/pi/wiki\ngit add .\ngit commit -m 'wiki commit'\ngit push origin master  Then SSH into Raspberry Pi and backup with  /home/pi/wiki/wikibackup.sh . Making this a daily cron job is difficult because of the  sudo sh -c  commands. Will continue to look for solutions for auto-commiting  wiki updates.", 
            "title": "DokuWiki Backup"
        }, 
        {
            "location": "/installation/torrent_server/", 
            "text": "Installation\n\n\n\n\nCreate new container using the TurnKey Torrent Server template with default HDD size and 4GB RAM, 1GB swap with software password\n\n\nStarting container and logging in as root with software password begins the TurnKey GUI setup, use software password, skip automatic backups, skip email notifications, and install security updates\n\n\nruTorrent GUI is found at \nhttps://torrentserverip:12322\n with username admin and software password\n\n\nDownload directory is \n/srv/storage/download\n\n\nCreate mountpoint to FreeNAS dataset through Proxmox NFS mount by editing \n/etc/pve/lxc/\ncontainerid\n.conf\n with \nmp0: /proxmox/nfs/mount,mp=/srv/storage/download\n\n\nRestart container\n\n\nCheck \ndu -h\n afterwards to see newly available storage", 
            "title": "Torrent server"
        }, 
        {
            "location": "/installation/torrent_server/#installation", 
            "text": "Create new container using the TurnKey Torrent Server template with default HDD size and 4GB RAM, 1GB swap with software password  Starting container and logging in as root with software password begins the TurnKey GUI setup, use software password, skip automatic backups, skip email notifications, and install security updates  ruTorrent GUI is found at  https://torrentserverip:12322  with username admin and software password  Download directory is  /srv/storage/download  Create mountpoint to FreeNAS dataset through Proxmox NFS mount by editing  /etc/pve/lxc/ containerid .conf  with  mp0: /proxmox/nfs/mount,mp=/srv/storage/download  Restart container  Check  du -h  afterwards to see newly available storage", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/ubuntu/", 
            "text": "Installation\n\n\n\n\nDownload latest Ubuntu LTS image and burn to USB\n\n\nBoot PC into BIOS and under Boot Options choose UEFI: USB_NAME to boot Ubuntu live image, then choose try Ubuntu without installing\n\n\nOpen GParted and ignore the Libparted warning about physical block size \n\n\nCarefully choose /dev/sd* corresponding to the SSD boot drive\n\n\nChoose the linux-swap partition and right-click and select Swapoff, this will redo the Libparted warning about physical block size after rescanning the partitions\n\n\nRight-click and delete both partitions separately, then hit the green check mark in GUI to apply these pending operations\n\n\nIt deletes partitions and again raises the Libparted warning\n\n\nExit GParted\n\n\n\n\n\n\nDouble click on install ubuntu icon\n\n\nEnglish -\n Continue\n\n\nDownload updates or proprietary drivers during installation (not doing so gives broken screen on login)\n\n\nChoose install ubuntu alongside windows boot manager\n\n\nInstall now, continue to write changes to disk\n\n\nSet Detroit, MI\n\n\nGive name, pc name, username, hardware password, require password to login\n\n\n\n\n\n\nRestart when completed, it will prompt for removal of installation media before rebooting\n\n\nWindows 10 is accidentally prioritized on boot, so need to boot back into BIOS and choose the GRUB partition as highest priority.\n\n\nWill possibly get a black or broken screen on restarting\n\n\nPer \nhttps://askubuntu.com/questions/760934/graphics-issues-after-while-installing-ubuntu-16-04-16-10-with-nvidia-graphics\n follow option 3\n\n\nReboot into GRUB, highlight Ubuntu option and press \ne\n\n\nAdd \nnouveau.modeset=0\n to the end of the line beginning with \nlinux\n \n\n\nPress \nF10\n to boot\n\n\n\n\n\n\nIf still black or broken screen after rebooting\n\n\nCtrl Alt F1\n opens a new console\n\n\nsudo apt-get purge nvidia-*\n\n\nsudo add-apt-repository ppa:graphics-drivers/ppa\n\n\nsudo apt-get update\n\n\nsudo apt-get install nvidia-384\n (or the latest Nvidia driver)\n\n\nReboot and graphics issues should be solved\n\n\n\n\n\n\nFix miscellaneous settings\n\n\nInstall Google Chrome \n.deb\n with \nsudo dpkg -i google-chrome...\n with an intermediate install fix with \nsudo apt-get install -f\n after first failing\n\n\nInstall Vim, Git, KeePass, HTop, Tmux with \nsudo apt-get install -y vim htop tmux keepass2 git\n\n\nSound may not work immediately, open All Settings -\n Sound and 'Test Sound' usually fixes this\n\n\nChoosing Detroit as location at installation results in package updates and installation from Ubuntu's Cananda mirrors, going to Software \n Updates and choosing 'Download from: Main server' fixes this\n\n\n\n\nCUDA 9.0 Installation\n\n\n\n\nDownload Cuda 9.0 network \n.deb\n file\n\n\nFollow installation guide at \nhttp://developer.download.nvidia.com/compute/cuda/9.0/Prod/docs/sidebar/CUDA_Installation_Guide_Linux.pdf\n\n\nVerify CUDA-capable GPU with \nlspci | grep -i nvidia\n\n\nVerify support Linux version with \nuname -m \n cat  /etc/*release\n\n\nVerify gcc is installed \ngcc --version\n\n\nVerify correct kernel headers and development packages with \nuname -r\n then do \nsudo apt-get install linux-headers-$(uname -r)\n\n\nInstall repository meda-data with \nsudo dpkg -i cuda-repo \ndistro\n_\nversion\n_\narchitecture\n.deb\n\n\nInstall CUDA public GPG key with \nsudo apt-key adv --fetch-keys\nhttp://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\n\n\nsudo  apt-get  update  \n sudo apt-get install cuda-9-0\n (or whatever version TensorFlow requires)\n\n\ncuda-command-line-tools\n will be installed\n\n\nOlder \nnvidia- libcuda1 nvidia-opencl-icd\n will be removed and replaced with newer versions\n\n\n\n\n\n\nAdd path to PATH variable \nexport PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\n\n\nSince we used \n.deb\n installation, the following doesn't modify the path, but to be safe do \nexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\\\n ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\n\nInstall writable samples  with \ncuda-install-samples-9.0.sh \ndir\n, change into the directory, and \nmake\n\n\nCompiling will take a file and some warnings or errors will be present, most are the result of the wrong CUDA version or environment variable in the code\n\n\nThe old \nnvidia-\n graphics driver will still be present, so running \ndeviceQuery\n will fail, restarting will show the newly installed \nnvidia-\n driver and \ndeviceQuery\n will pass\n\n\n\n\n\n\nAdd the PATH export statemnts to \n.bashrc\n\n\nSkip the Nsight plugin step\n\n\nThird-party libraries should already be installed\n\n\nSkip cuda-gdb installation\n\n\nIn the 9.0 guide, ther Persistence Daemon installation is not included, but is in version 9.1, if necessary add \n/usr/bin/nvidia-persistenced --verbose\n\n\n\n\ncuDNN 7.0 Installation\n\n\n\n\nDownload all \n.deb\n files, ie, runtime, developer, and example files\n\n\nDo \nsudo dpkg  -i libcudnn7...\n for each starting with the runtime library, then developer library, then documentation and examples\n\n\nVerify installation with example\n\n\ncp -r /usr/src/cudnn_samples_v7/ $HOME\n\n\ncd $HOME/cudnn_samples_v7/mnistCUDNN\n\n\nmake clean \n make\n\n\n./mnistCUDNN\n\n\nIf properly running, should see a \"Test passed!\" or similar message\n\n\n\n\n\n\nAdd \nexport CUDA_HOME=/usr/local/cuda\n to \n.bashrc\n\n\n\n\nTensorFlow With GPU Support Installation\n\n\n\n\nFollow \nhttps://www.tensorflow.org/install/install_linux\n\n\ncuda-command-line-tools\n were already installed, so need only add its path to environment, \nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/extras/CUPTI/lib64\n in \n.bashrc\n\n\nUse \"Virtualenv\" installation mechanism\n\n\nInstall pip, virtualenv, and python-dev with \nsudo apt-get install python3-pip python3-dev python-virtualenv\n\n\nCreate a virtual environment  \nvirtualenv --system-site-packages -p python3 targetdirectory\n\n\nActivate the environement \nsource ~/tensorflow/bin/activate\n\n\nInstall with \npip3 install --upgrade tensorflow-gpu\n\n\nValidate the installation with \nimport tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\nprint(sess.run(hello))\n\n\nDeactivate environment \ndeactivate\n\n\nTo uninstall TensorFlow, \nrm -r targetdirectory\n\n\n\n\nHDF5\n\n\nFor Keras features such as model saving to disk and using pre-trained models, this file system is necessary. Install it with \nsudo apt-get install libhdf5-serial-dev\n, which should install all the different HDF5 packages necessary. Then do \npip install hd5py\n in your virtual environment. A warning will appear when using TensorFlow and models can be saved, however doing \nhd5py.run_tests()\n will fail. So this appears to be a non-issue for our uses.\n\n\ncv2\n\n\nAlso needed for deep learning is the \nOpenCV\n package. In source directory install with \npip install opencv-python\n.", 
            "title": "Ubuntu"
        }, 
        {
            "location": "/installation/ubuntu/#installation", 
            "text": "Download latest Ubuntu LTS image and burn to USB  Boot PC into BIOS and under Boot Options choose UEFI: USB_NAME to boot Ubuntu live image, then choose try Ubuntu without installing  Open GParted and ignore the Libparted warning about physical block size   Carefully choose /dev/sd* corresponding to the SSD boot drive  Choose the linux-swap partition and right-click and select Swapoff, this will redo the Libparted warning about physical block size after rescanning the partitions  Right-click and delete both partitions separately, then hit the green check mark in GUI to apply these pending operations  It deletes partitions and again raises the Libparted warning  Exit GParted    Double click on install ubuntu icon  English -  Continue  Download updates or proprietary drivers during installation (not doing so gives broken screen on login)  Choose install ubuntu alongside windows boot manager  Install now, continue to write changes to disk  Set Detroit, MI  Give name, pc name, username, hardware password, require password to login    Restart when completed, it will prompt for removal of installation media before rebooting  Windows 10 is accidentally prioritized on boot, so need to boot back into BIOS and choose the GRUB partition as highest priority.  Will possibly get a black or broken screen on restarting  Per  https://askubuntu.com/questions/760934/graphics-issues-after-while-installing-ubuntu-16-04-16-10-with-nvidia-graphics  follow option 3  Reboot into GRUB, highlight Ubuntu option and press  e  Add  nouveau.modeset=0  to the end of the line beginning with  linux    Press  F10  to boot    If still black or broken screen after rebooting  Ctrl Alt F1  opens a new console  sudo apt-get purge nvidia-*  sudo add-apt-repository ppa:graphics-drivers/ppa  sudo apt-get update  sudo apt-get install nvidia-384  (or the latest Nvidia driver)  Reboot and graphics issues should be solved    Fix miscellaneous settings  Install Google Chrome  .deb  with  sudo dpkg -i google-chrome...  with an intermediate install fix with  sudo apt-get install -f  after first failing  Install Vim, Git, KeePass, HTop, Tmux with  sudo apt-get install -y vim htop tmux keepass2 git  Sound may not work immediately, open All Settings -  Sound and 'Test Sound' usually fixes this  Choosing Detroit as location at installation results in package updates and installation from Ubuntu's Cananda mirrors, going to Software   Updates and choosing 'Download from: Main server' fixes this", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/ubuntu/#cuda-90-installation", 
            "text": "Download Cuda 9.0 network  .deb  file  Follow installation guide at  http://developer.download.nvidia.com/compute/cuda/9.0/Prod/docs/sidebar/CUDA_Installation_Guide_Linux.pdf  Verify CUDA-capable GPU with  lspci | grep -i nvidia  Verify support Linux version with  uname -m   cat  /etc/*release  Verify gcc is installed  gcc --version  Verify correct kernel headers and development packages with  uname -r  then do  sudo apt-get install linux-headers-$(uname -r)  Install repository meda-data with  sudo dpkg -i cuda-repo  distro _ version _ architecture .deb  Install CUDA public GPG key with  sudo apt-key adv --fetch-keys\nhttp://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub  sudo  apt-get  update    sudo apt-get install cuda-9-0  (or whatever version TensorFlow requires)  cuda-command-line-tools  will be installed  Older  nvidia- libcuda1 nvidia-opencl-icd  will be removed and replaced with newer versions    Add path to PATH variable  export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}  Since we used  .deb  installation, the following doesn't modify the path, but to be safe do  export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\\\n ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}  Install writable samples  with  cuda-install-samples-9.0.sh  dir , change into the directory, and  make  Compiling will take a file and some warnings or errors will be present, most are the result of the wrong CUDA version or environment variable in the code  The old  nvidia-  graphics driver will still be present, so running  deviceQuery  will fail, restarting will show the newly installed  nvidia-  driver and  deviceQuery  will pass    Add the PATH export statemnts to  .bashrc  Skip the Nsight plugin step  Third-party libraries should already be installed  Skip cuda-gdb installation  In the 9.0 guide, ther Persistence Daemon installation is not included, but is in version 9.1, if necessary add  /usr/bin/nvidia-persistenced --verbose", 
            "title": "CUDA 9.0 Installation"
        }, 
        {
            "location": "/installation/ubuntu/#cudnn-70-installation", 
            "text": "Download all  .deb  files, ie, runtime, developer, and example files  Do  sudo dpkg  -i libcudnn7...  for each starting with the runtime library, then developer library, then documentation and examples  Verify installation with example  cp -r /usr/src/cudnn_samples_v7/ $HOME  cd $HOME/cudnn_samples_v7/mnistCUDNN  make clean   make  ./mnistCUDNN  If properly running, should see a \"Test passed!\" or similar message    Add  export CUDA_HOME=/usr/local/cuda  to  .bashrc", 
            "title": "cuDNN 7.0 Installation"
        }, 
        {
            "location": "/installation/ubuntu/#tensorflow-with-gpu-support-installation", 
            "text": "Follow  https://www.tensorflow.org/install/install_linux  cuda-command-line-tools  were already installed, so need only add its path to environment,  export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/extras/CUPTI/lib64  in  .bashrc  Use \"Virtualenv\" installation mechanism  Install pip, virtualenv, and python-dev with  sudo apt-get install python3-pip python3-dev python-virtualenv  Create a virtual environment   virtualenv --system-site-packages -p python3 targetdirectory  Activate the environement  source ~/tensorflow/bin/activate  Install with  pip3 install --upgrade tensorflow-gpu  Validate the installation with  import tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\nprint(sess.run(hello))  Deactivate environment  deactivate  To uninstall TensorFlow,  rm -r targetdirectory", 
            "title": "TensorFlow With GPU Support Installation"
        }, 
        {
            "location": "/installation/ubuntu/#hdf5", 
            "text": "For Keras features such as model saving to disk and using pre-trained models, this file system is necessary. Install it with  sudo apt-get install libhdf5-serial-dev , which should install all the different HDF5 packages necessary. Then do  pip install hd5py  in your virtual environment. A warning will appear when using TensorFlow and models can be saved, however doing  hd5py.run_tests()  will fail. So this appears to be a non-issue for our uses.", 
            "title": "HDF5"
        }, 
        {
            "location": "/installation/ubuntu/#cv2", 
            "text": "Also needed for deep learning is the  OpenCV  package. In source directory install with  pip install opencv-python .", 
            "title": "cv2"
        }, 
        {
            "location": "/installation/windows_10/", 
            "text": "Installation\n\n\n\n\nubuntu live usb and use gparted to remove partitions and format as ntfs\n\n\nboot windows 10 usb and select education edition - full name and select basic security question\n\n\nreconnect hard drives and reboot, if extra drives formatted as ntsf, they\n\n\nshould automatically be added as the remaining drive letters, D,E,F,G, etc\n\n\ngo to disk management and name them if necessary\n\n\ngo to settings - notifications \n actions, turn off notifications frm apps and other senders\n\n\nsettings - storage: change where new content is saved and move everything to\n\n\nother drive (pictures, documents, etc)\n\n\nsettings - remote desktop enabled\n\n\nsettings - shared experiences - disable share across devices\n\n\nsettings - printers \n scanners: disable windows manage default printer\n\n\nsettings - typing: turn off everything basically, same with pen \n windows ink, autoplay\n\n\nsettings - personalization: turn everything off basically unpin all junk from taskbar\n\n\nsettings - personalization - taskbar: turn off contacts settings\n\n\nsettings - privacy - basically turn everything off\n\n\nsettings - cortanna: basically turn everything off\n\n\nsettings - gaming: turn everything off\n\n\ndownloads: chrome, keepass, btsync, notepad++, steam, gog, flux, rclone, geforce experience, google drive\n\n\nubuntu dual boot: go to diskmgmt.msc and select C: partition and shrink volume, then boot ubuntu and install, should only select the C: drive and give no options unlike before. followed guide from: https://www.tecmint.com/install-ubuntu-16-04-alongside-with-windows-10-or-8-in-dual-boot/\n\n\nserver naming scheme: great lakes, main pc is michigan", 
            "title": "Windows 10"
        }, 
        {
            "location": "/installation/windows_10/#installation", 
            "text": "ubuntu live usb and use gparted to remove partitions and format as ntfs  boot windows 10 usb and select education edition - full name and select basic security question  reconnect hard drives and reboot, if extra drives formatted as ntsf, they  should automatically be added as the remaining drive letters, D,E,F,G, etc  go to disk management and name them if necessary  go to settings - notifications   actions, turn off notifications frm apps and other senders  settings - storage: change where new content is saved and move everything to  other drive (pictures, documents, etc)  settings - remote desktop enabled  settings - shared experiences - disable share across devices  settings - printers   scanners: disable windows manage default printer  settings - typing: turn off everything basically, same with pen   windows ink, autoplay  settings - personalization: turn everything off basically unpin all junk from taskbar  settings - personalization - taskbar: turn off contacts settings  settings - privacy - basically turn everything off  settings - cortanna: basically turn everything off  settings - gaming: turn everything off  downloads: chrome, keepass, btsync, notepad++, steam, gog, flux, rclone, geforce experience, google drive  ubuntu dual boot: go to diskmgmt.msc and select C: partition and shrink volume, then boot ubuntu and install, should only select the C: drive and give no options unlike before. followed guide from: https://www.tecmint.com/install-ubuntu-16-04-alongside-with-windows-10-or-8-in-dual-boot/  server naming scheme: great lakes, main pc is michigan", 
            "title": "Installation"
        }, 
        {
            "location": "/machine_learning/methods/", 
            "text": "Need standard set of techniques to do when approaching new problems. \n\n\nMethods\n\n\n\n\nregression\n\n\nlogistic regression\n\n\nlasso\n\n\nridge\n\n\nelastic net\n\n\ncart\n\n\nrandom forest\n\n\nadaboost\n\n\nextratrees\n\n\ngcForest\n\n\nk means\n\n\ndbscan\n\n\npcr\n\n\npca\n\n\nsvm\n\n\nwls\n\n\nfeature importance plots\n\n\nroc/auc plots\n\n\nisolation forest\n\n\n\n\nScoring Metrics\n\n\nDifferent ways to score if groups/unbalanced classes\n\n\nDifferent Shuffling Methods\n\n\nHow to split data properly for grid searching model to be used over before training and then testing?", 
            "title": "Methods"
        }, 
        {
            "location": "/machine_learning/methods/#methods", 
            "text": "regression  logistic regression  lasso  ridge  elastic net  cart  random forest  adaboost  extratrees  gcForest  k means  dbscan  pcr  pca  svm  wls  feature importance plots  roc/auc plots  isolation forest", 
            "title": "Methods"
        }, 
        {
            "location": "/machine_learning/methods/#scoring-metrics", 
            "text": "Different ways to score if groups/unbalanced classes", 
            "title": "Scoring Metrics"
        }, 
        {
            "location": "/machine_learning/methods/#different-shuffling-methods", 
            "text": "How to split data properly for grid searching model to be used over before training and then testing?", 
            "title": "Different Shuffling Methods"
        }, 
        {
            "location": "/natural_language_processing/scratch/", 
            "text": "About\n\n\nThis wiki is for natural language processing methods and ideas.\n\n\nPackages\n\n\n\n\ngensim\n\n\nnltk\n\n\nspacy\n\n\n\n\nGeneral\n\n\n\n\ntext summarization (different algorithms than bag of words)\n\n\nbag of words\n\n\ntf-idf\n\n\nword2vec\n\n\ndoc2vec\n\n\nngram\n\n\ntopic modeling\n\n\ntopic segmentation\n\n\nlda/lsi\n\n\ntext tiling\n\n\nwordclouds\n\n\nkeyword/keyphrase extraction\n\n\nGensim for modeling and named entity stuff. First stream tokenized text and then construct phrases and then from there train a word2vec/doc2vec model\n\n\ngensim phrases model\n\n\nsvm text classification on tfidf vectors\n\n\nsklearn has a couple good text articles\n\n\nreally good tutorial on text svm classification at https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n\n\ncan also do lds/lsi/neuralnetwork/naive bayes on these results from tfidf, clustering too for unsupervised problems (example on sklearn)\n\n\ngensim has common phrases utility to be included in phrases function, so if we are bigramming and want to to identify words in a common sequence like 'of' in 'secretary of state' without trigramming. \n\n\nGrid search best topic models from https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n\n\nPossible method to be used in searching a large corpus of documents for keywords and returning the documents where the probability of certain topics is highest: https://stackoverflow.com/questions/15377290/unsupervised-automatic-tagging-algorithms\n\n\nLooking at most important words in SVM text classification https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d", 
            "title": "Scratch"
        }, 
        {
            "location": "/natural_language_processing/scratch/#about", 
            "text": "This wiki is for natural language processing methods and ideas.", 
            "title": "About"
        }, 
        {
            "location": "/natural_language_processing/scratch/#packages", 
            "text": "gensim  nltk  spacy", 
            "title": "Packages"
        }, 
        {
            "location": "/natural_language_processing/scratch/#general", 
            "text": "text summarization (different algorithms than bag of words)  bag of words  tf-idf  word2vec  doc2vec  ngram  topic modeling  topic segmentation  lda/lsi  text tiling  wordclouds  keyword/keyphrase extraction  Gensim for modeling and named entity stuff. First stream tokenized text and then construct phrases and then from there train a word2vec/doc2vec model  gensim phrases model  svm text classification on tfidf vectors  sklearn has a couple good text articles  really good tutorial on text svm classification at https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a  can also do lds/lsi/neuralnetwork/naive bayes on these results from tfidf, clustering too for unsupervised problems (example on sklearn)  gensim has common phrases utility to be included in phrases function, so if we are bigramming and want to to identify words in a common sequence like 'of' in 'secretary of state' without trigramming.   Grid search best topic models from https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/  Possible method to be used in searching a large corpus of documents for keywords and returning the documents where the probability of certain topics is highest: https://stackoverflow.com/questions/15377290/unsupervised-automatic-tagging-algorithms  Looking at most important words in SVM text classification https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d", 
            "title": "General"
        }, 
        {
            "location": "/python/environments/", 
            "text": "Usage\n\n\nTo resolve package conflict issues, it's best to make a separate Python environment for each project. This way, we can explicitly specify our package versions and not worry about global package or package version conflicts.\n\n\nCreate a directory for your project and then create the virtual environment with \nvirtualenv -p python3 targetdirectory\n\n\nActivate the environment with \nsource targetdirectory/bin/activate\n\n\nFrom here, install packages using \npip3 install packagename\n\n\nTo then make a file with all installed packages and their exact version numbers do \npip3 freeze -r \n requirements.txt\n\n\nThen when creating a new environment and want to install the same exact packages, use \npip install -r requirements.txt\n\n\nLXC Containers\n\n\nLXC Ubuntu containers should have Python 3.5 installed, but PIP will also need to be installed by install locales with \nlocale-gen en_US.UTF-8\n and then \napt install python3-pip\n.", 
            "title": "Environments"
        }, 
        {
            "location": "/python/environments/#usage", 
            "text": "To resolve package conflict issues, it's best to make a separate Python environment for each project. This way, we can explicitly specify our package versions and not worry about global package or package version conflicts.  Create a directory for your project and then create the virtual environment with  virtualenv -p python3 targetdirectory  Activate the environment with  source targetdirectory/bin/activate  From here, install packages using  pip3 install packagename  To then make a file with all installed packages and their exact version numbers do  pip3 freeze -r   requirements.txt  Then when creating a new environment and want to install the same exact packages, use  pip install -r requirements.txt", 
            "title": "Usage"
        }, 
        {
            "location": "/python/environments/#lxc-containers", 
            "text": "LXC Ubuntu containers should have Python 3.5 installed, but PIP will also need to be installed by install locales with  locale-gen en_US.UTF-8  and then  apt install python3-pip .", 
            "title": "LXC Containers"
        }, 
        {
            "location": "/python/packages/", 
            "text": "NumPy\n\n\nSciPy\n\n\npandas\n\n\nmatplotlib\n\n\nSeaborn\n\n\nHistograms, density, plots, etc. Easier plotting of common statistical plots using dataframes.\n\n\n\n\n\n\nscikit-learn\n\n\nMachine learning toolkit. Consistent syntax for various classification and regression techniques.\n\n\n\n\n\n\nStatsModels\n\n\nCompared to scikit-learn, more statistically oriented with output of p-values, etc. Output similar to \nR\n  statistical tests. Contains many statistical tests not found in scipy.stats or scikit-learn.\n\n\n\n\n\n\nTensorFlow\n\n\nDeep learning library, uses GPU for acceleration.\n\n\n\n\n\n\nKeras\n\n\nWrapper for TensorFlow with common utilities.\n\n\n\n\n\n\nXGBoost\n\n\nPopular library for implementing boosted trees. Faster than scikit-learn implementation. Can offload on GPU.\n\n\n\n\n\n\nPyODBC\n\n\nDatabase connection utility.\n\n\n\n\n\n\nsmtplib\n\n\nUsed for sending emails given a SMTP server.\n\n\n\n\n\n\nMultiprocessing\n\n\nRelatively straightforward to use parallel processing module. Most used components are \nPool, map, starmap\n.\n\n\n\n\n\n\nscikit-image\n\n\nImage processing module with various utilities for transforming images\n\n\n\n\n\n\nNumba\n\n\nJust-in-time compiler for array-oriented, math-heavy python code. Can offload calculations to GPU.\n\n\n\n\n\n\nPySpark\n\n\nSpark connector for python. Offers modelling modules as well.\n\n\n\n\n\n\nlocust\n\n\nHTTP server load testing tool, useful for testing APIs.\n\n\n\n\n\n\nflask/flask_redis/flask_api\n\n\nMicroservice for writing web applications and web APIs. Redis module is for server-side caching.\n\n\n\n\n\n\ntqdm\n\n\nUseful progress bar for looping operations\n\n\n\n\n\n\nhue\n\n\nPrettier terminal printing\n\n\n\n\n\n\nRequests\n\n\nHTTP post/get operations for web servers\n\n\n\n\n\n\nretrying\n\n\nUseful for retrying connections to web servers, database servers, etc.\n\n\n\n\n\n\nshapely\n\n\nPython geometry library for pip tests, overlapping polygons, etc.\n\n\n\n\n\n\nPickle\n\n\npython serialization module\n\n\n\n\n\n\nhashlib\n\n\nnltk\n\n\nNLP library\n\n\n\n\n\n\nGensim\n\n\nNLP library with Doc2Vec\n\n\n\n\n\n\nSpaCy\n\n\nHigh performance NLP library\n\n\n\n\n\n\nLogging\n\n\nlogging module\n\n\n\n\n\n\nre\n\n\nRegular expressions module\n\n\n\n\n\n\nshutil\n\n\nShell utilities, like copying files from one directory to another. Somewhat overlaps with os module\n\n\n\n\n\n\npytesseract\n\n\nOCR of images\n\n\n\n\n\n\ntextract\n\n\nAble to read 'text' PDFs, docx files, etc.\n\n\n\n\n\n\nopencv-python\n\n\ntox\n\n\ntest suite for programs\n\n\n\n\n\n\ntempfile\n\n\ncreate temporary files/directories\n\n\n\n\n\n\nfaker\n\n\ncreate fake data like names, addresses, phone numbers text for population of databases or testing\n\n\n\n\n\n\njupyter widgets\n\n\nnot quite R Shiny\n\n\n\n\n\n\ndateutil\n\n\nAdditional datetime functionality, like relative deltas\n\n\n\n\n\n\nlimeml\n\n\nexplain predictions of classifiers graphically - have not looked into more\n\n\n\n\n\n\npyowm\n\n\nPython wrapper for OpenWeatherMap API\n\n\n\n\n\n\nplotly\n\n\nInteractive graphics. Complicated syntax and need API\n\n\n\n\n\n\nBokeh\n\n\nSlightly interactive graphics\n\n\n\n\n\n\nnose\n\n\nsimilar testing to tox\n\n\n\n\n\n\nast\n\n\nabstract syntax trees. Turn a string of a command into the actual command. \n\n\n\n\n\n\naffinity\n\n\nsometimes os parallel processes are weird\n\n\n\n\n\n\nitertools\n\n\nlots of useful functions in here, repea and chain.from_iterable to unpack nested lists\n\n\n\n\n\n\njoblib\n\n\nmore advanced distributed computing\n\n\n\n\n\n\nscrapy\n\n\nweb scraper\n\n\n\n\n\n\ngensim sklearn api\n\n\njupyter notebook slides\n\n\npipenv\n\n\npip + venv\n\n\n\n\n\n\nopencv-python\n\n\ndash, plotly, bokeh\n\n\nrise is jupyter notebook slideshows\n\n\nreportlab\n\n\njinja2\n\n\noptparse\n\n\npyqt\n\n\nitertools dropwhile, takewhile\n\n\npprint\n\n\ndocker\n\n\nwarnings\n\n\nhugraphs\n\n\ntkinter\n\n\nlightgbm\n\n\nglob - good for listing files in nested subdirectories than os.walk\n\n\nsqlalchemy - another sql agent\n\n\nuswgi web server", 
            "title": "Packages"
        }, 
        {
            "location": "/python/scratch/", 
            "text": "text tags\n\n\nsorting algorithms/techniques\n\n\nsetup tools\n\n\nmap, filter\n\n\nsorted(zip()) for sort one list byanother list\n\n\ncollections.Counter\n\n\nglobal vars\n\n\nnext\n\n\niter\n\n\nitertools\n\n\ncombinations\n\n\nmaybe put repeat in second argument during starmap calls, problem at work not parallelizing this\n\n\ncontinuously write to log files,temporary files for progress, grid search iterations completed, loop progress, etc.\n\n\nyaml, pyqt, pathlib\n\n\ndocstrings for auto documentation using sphinx\n\n\ntornado web server, gunicorn\n\n\nsetuptools for making setup.py files\n\n\nsmooth plots with exponential moving average\n\n\nset(list) to get unique list elements\n\n\nuse tox (integrate with testing in jenkins, travis-ci)\n\n\nuse threading within class (like Ryan's automodeler)", 
            "title": "Scratch"
        }, 
        {
            "location": "/python/snippets/", 
            "text": "Send Email\n\n\nUnfortunately requires the use of password in plain text, but so long as used internally this is fine. \n\n\nimport smtplib\nsmtp_server = smtplib.SMTP('smtp.gmail.com', 587)\nsmtp_server.ehlo()\nsmtp_server.starttls()\nsmtp_server.login('email', 'password')\nsmtp_server.sendmail('from', ['to'], 'message')\nsmtp_server.quit()\n\n\n\n\nParallelism In Almost One Line\n\n\nFor a function taking a single argument:\n\n\nfrom multiprocessing import cpu_count, Pool\n\ndef f(x):\n    return x*x\n\npool = Pool(cpu_count())\nresults = pool.map(f, [1, 2, 3])\npool.close()\npool.join()\ndel pool\n\n\n\n\nFor a function taking multiple arguments:\n\n\nfrom multiprocessing import cpu_count,  Pool\n\ndef f(a, b):\n    return a + b\n\npool = Pool(cpu_count())\ndata = [(1, 2), (3, 4)]\nresults = pool.starmap(f, data)\npool.close()\npool.join()\ndel pool\n\n\n\n\nFor use with pandas DataFrames, create a wrapper for your function:\n\n\nimport pandas as pd\nimport numpy as np\nfrom multiprocessing import cpu_count, Pool\n\ndf = pd.DataFrame({'a': ['chop', 'me', 'up'],\n                   'b': ['3', '6', 'mafia']})\n\ndef join_rows(df):\n    df['joined_column'] = df.apply(lambda x: ' '.join(x), axis = 1)\n    return df\n\ndef join_rows_parallel(df, num_cores):\n    df_split = np.array_split(df, num_cores)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(join_rows, df_split))\n    pool.close()\n    pool.join()\n    del pool\n    return df\n\n\n\n\nLogging Output\n\n\nInstead of including lots of print statements to be excluded later, logging to the console or a  file can be useful for debugging.\n\n\nimport logging\n\nlogging.basicConfig(filename = 'mylogfile.log',\n    level = logging.DEBUG,\n    format = '%(asctime)s:%(levelname)s:%(message)s')\n\ndef foo(x):\n    logging.debug('calling foo {}'.format(x))\n    # do stuff\n\ndef bar(x):\n    logging.debug('calling bar {}'.format(x))\n    if x \n y:\n        logging.warning('x \n y')\n    # do stuff\n\nif __name__ == '__main__':\n    data = foo(x)\n    logging.info('foo is still working')\n    transform = bar(x) \n    logging.shutdown()\n\n\n\n\nTry-Except-Else-Finally\n\n\nFor more complicated try-except blocks, there's the \nelse\n and \nfinally\n additions. \nelse\n will be executed if there are \nno\n exceptions, interrupted by a \nreturn\n, \ncontinue\n, or \nbreak\n statement. \nfinally\n is executed no matter the result, so this construct can be useful for logging information and returning purposes.\n\n\ndef foo(x):\n    try:\n        f = open(x, 'r')\n    except IOError:\n        logging.debug('file x not found')\n        # do something after error with package to return\n    else:\n        print(x, 'has', len(f.readlines()), 'lines')\n        # continue processing with package to return\n    finally:\n        logging.debug('we finished this function')\n        return package\n\n\n\n\nBreak During Function Execution\n\n\nSometimes it can be useful to break out of a function that takes too long to execute.\n\n\nimport signal\nimport time\n\nclass TimeoutException(Exception):\n    pass\n\ndef timeout_handler(signum, frame):\n    raise TimeoutException\n\nif __name__ == '__main__':\n    signal.signal(signal.SIGALRM, timeout_handler)\n\n    # raise TimeoutException after 5 seconds\n    signal.alarm(5)\n    try:\n        time.sleep(10)\n        # possibly long function\n    except TimeoutException:\n        print('function took too long, moving to a shorter function')\n        # some shorter function\n        # logging.info('...')\n    # except TimeoutException:\n        # continue\n    else:\n        # reset alarm\n        signal.alarm(0)\n        # continue processing long function\n    finally:\n        return result\n\n\n\n\nargs, \n*kwargs\n\n\n*args\n unpacks all arguments into a function and can be used for shorthand function calls or variable length function calls, eg,\n\n\ndef f(z, *args):\n    for num in args:\n        z *= num\n    return z\n\n\n\n\n**kwargs\n is a dictionary of optional parameters for a function, it is an alternative to having \n\n\ndef f(...param = None...): \n    ....\n    if param:\n        ...\n    ...\n\n\n\n\nretrying module\n\n\nUseful module for handling 'unreliable' functions, like connecting to SQL servers for instance. Instead of throwing an exception, \nretrying\n will keep retrying to connect.\n\n\nfrom retrying import retry\n\ndef retry_if_result_none(result):\n     \n\n     Return True if we should retry\n     \n\n     return result is None\n\n@retry(wait_fixed = , stop_max_attempt_number = , wait_random_min = , wait_random_max =, retry_on_result = retry_if_result_none) \ndef unreliable_function_might_return_none():\n    print('retry if result is None, throw exception otherwise')\n\n\n\n\nThis snippet will continue to retry the function so long as the \nretry_on_result\n function is true.\n\n\nSuppress Warnings\n\n\nFor cases when we want to suppress warnings\n\n\nimport warnings \n...\ndef warn(*arg, **kwargs):\n    pass\n...\nif __name__ == '__main__':\n    warnings.warn = warn\n    ...", 
            "title": "Snippets"
        }, 
        {
            "location": "/python/snippets/#send-email", 
            "text": "Unfortunately requires the use of password in plain text, but so long as used internally this is fine.   import smtplib\nsmtp_server = smtplib.SMTP('smtp.gmail.com', 587)\nsmtp_server.ehlo()\nsmtp_server.starttls()\nsmtp_server.login('email', 'password')\nsmtp_server.sendmail('from', ['to'], 'message')\nsmtp_server.quit()", 
            "title": "Send Email"
        }, 
        {
            "location": "/python/snippets/#parallelism-in-almost-one-line", 
            "text": "For a function taking a single argument:  from multiprocessing import cpu_count, Pool\n\ndef f(x):\n    return x*x\n\npool = Pool(cpu_count())\nresults = pool.map(f, [1, 2, 3])\npool.close()\npool.join()\ndel pool  For a function taking multiple arguments:  from multiprocessing import cpu_count,  Pool\n\ndef f(a, b):\n    return a + b\n\npool = Pool(cpu_count())\ndata = [(1, 2), (3, 4)]\nresults = pool.starmap(f, data)\npool.close()\npool.join()\ndel pool  For use with pandas DataFrames, create a wrapper for your function:  import pandas as pd\nimport numpy as np\nfrom multiprocessing import cpu_count, Pool\n\ndf = pd.DataFrame({'a': ['chop', 'me', 'up'],\n                   'b': ['3', '6', 'mafia']})\n\ndef join_rows(df):\n    df['joined_column'] = df.apply(lambda x: ' '.join(x), axis = 1)\n    return df\n\ndef join_rows_parallel(df, num_cores):\n    df_split = np.array_split(df, num_cores)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(join_rows, df_split))\n    pool.close()\n    pool.join()\n    del pool\n    return df", 
            "title": "Parallelism In Almost One Line"
        }, 
        {
            "location": "/python/snippets/#logging-output", 
            "text": "Instead of including lots of print statements to be excluded later, logging to the console or a  file can be useful for debugging.  import logging\n\nlogging.basicConfig(filename = 'mylogfile.log',\n    level = logging.DEBUG,\n    format = '%(asctime)s:%(levelname)s:%(message)s')\n\ndef foo(x):\n    logging.debug('calling foo {}'.format(x))\n    # do stuff\n\ndef bar(x):\n    logging.debug('calling bar {}'.format(x))\n    if x   y:\n        logging.warning('x   y')\n    # do stuff\n\nif __name__ == '__main__':\n    data = foo(x)\n    logging.info('foo is still working')\n    transform = bar(x) \n    logging.shutdown()", 
            "title": "Logging Output"
        }, 
        {
            "location": "/python/snippets/#try-except-else-finally", 
            "text": "For more complicated try-except blocks, there's the  else  and  finally  additions.  else  will be executed if there are  no  exceptions, interrupted by a  return ,  continue , or  break  statement.  finally  is executed no matter the result, so this construct can be useful for logging information and returning purposes.  def foo(x):\n    try:\n        f = open(x, 'r')\n    except IOError:\n        logging.debug('file x not found')\n        # do something after error with package to return\n    else:\n        print(x, 'has', len(f.readlines()), 'lines')\n        # continue processing with package to return\n    finally:\n        logging.debug('we finished this function')\n        return package", 
            "title": "Try-Except-Else-Finally"
        }, 
        {
            "location": "/python/snippets/#break-during-function-execution", 
            "text": "Sometimes it can be useful to break out of a function that takes too long to execute.  import signal\nimport time\n\nclass TimeoutException(Exception):\n    pass\n\ndef timeout_handler(signum, frame):\n    raise TimeoutException\n\nif __name__ == '__main__':\n    signal.signal(signal.SIGALRM, timeout_handler)\n\n    # raise TimeoutException after 5 seconds\n    signal.alarm(5)\n    try:\n        time.sleep(10)\n        # possibly long function\n    except TimeoutException:\n        print('function took too long, moving to a shorter function')\n        # some shorter function\n        # logging.info('...')\n    # except TimeoutException:\n        # continue\n    else:\n        # reset alarm\n        signal.alarm(0)\n        # continue processing long function\n    finally:\n        return result", 
            "title": "Break During Function Execution"
        }, 
        {
            "location": "/python/snippets/#args-kwargs", 
            "text": "*args  unpacks all arguments into a function and can be used for shorthand function calls or variable length function calls, eg,  def f(z, *args):\n    for num in args:\n        z *= num\n    return z  **kwargs  is a dictionary of optional parameters for a function, it is an alternative to having   def f(...param = None...): \n    ....\n    if param:\n        ...\n    ...", 
            "title": "args, *kwargs"
        }, 
        {
            "location": "/python/snippets/#retrying-module", 
            "text": "Useful module for handling 'unreliable' functions, like connecting to SQL servers for instance. Instead of throwing an exception,  retrying  will keep retrying to connect.  from retrying import retry\n\ndef retry_if_result_none(result):\n      \n     Return True if we should retry\n      \n     return result is None\n\n@retry(wait_fixed = , stop_max_attempt_number = , wait_random_min = , wait_random_max =, retry_on_result = retry_if_result_none) \ndef unreliable_function_might_return_none():\n    print('retry if result is None, throw exception otherwise')  This snippet will continue to retry the function so long as the  retry_on_result  function is true.", 
            "title": "retrying module"
        }, 
        {
            "location": "/python/snippets/#suppress-warnings", 
            "text": "For cases when we want to suppress warnings  import warnings \n...\ndef warn(*arg, **kwargs):\n    pass\n...\nif __name__ == '__main__':\n    warnings.warn = warn\n    ...", 
            "title": "Suppress Warnings"
        }, 
        {
            "location": "/python/sklearn/snippets/", 
            "text": "Usage\n\n\nGridSearchCV, Pipeline, fit_transform vs fit, AUC, ROC, how to do hyperparameter search before training/cross-validation and then testing?\n\n\nGridSearchCV Over Multiple Models\n\n\nNormally \nGridSearchCV\n is for hyperparameter optimization. This method is extended using the following class, where you specify a dictionary of models and a dictionary of dictionaries of their parameters to search over. After fitting this helper, call \nhelper.score_summary(sort_by = 'mean_score')\n to return a dataframe of every model fit with parameters and scoring metrics. \n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\n\nclass EstimatorSelectionHelper:\n\n    def __init__(self, models, params):\n        if not set(models.keys()).issubset(set(params.keys())):\n            missing_params = list(set(models.keys()) - set(params.keys()))\n            raise ValueError(\nSome estimators are missing parameters: %s\n % missing_params)\n        self.models = models\n        self.params = params\n        self.keys = models.keys()\n        self.grid_searches = {}\n\n    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n        for key in self.keys:\n            print(\nRunning GridSearchCV for %s.\n % key)\n            model = self.models[key]\n            params = self.params[key]\n            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n                              verbose=verbose, scoring=scoring, refit=refit,\n                              return_train_score=True)\n            gs.fit(X,y)\n            self.grid_searches[key] = gs    \n\n    def score_summary(self, sort_by='mean_score'):\n        def row(key, scores, params):\n            d = {\n                 'estimator': key,\n                 'min_score': min(scores),\n                 'max_score': max(scores),\n                 'mean_score': np.mean(scores),\n                 'std_score': np.std(scores),\n            }\n            return pd.Series({**params,**d})\n\n        rows = []\n        for k in self.grid_searches:\n            print(k)\n            params = self.grid_searches[k].cv_results_['params']\n            scores = []\n            for i in range(self.grid_searches[k].cv):\n                key = \nsplit{}_test_score\n.format(i)\n                r = self.grid_searches[k].cv_results_[key]        \n                scores.append(r.reshape(len(params),1))\n\n            all_scores = np.hstack(scores)\n            for p, s in zip(params,all_scores):\n                rows.append((row(k, s, p)))\n\n        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n\n        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n        columns = columns + [c for c in df.columns if c not in columns]\n\n        return df[columns]\n\n\n\n\n\nExample use:\n\n\n\nfrom sklearn import datasets\n\nbreast_cancer = datasets.load_breast_cancer()\nX_cancer = breast_cancer.data\ny_cancer = breast_cancer.target\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVC\n\nmodels = {\n    'ExtraTreesClassifier': ExtraTreesClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'SVC': SVC()\n}\n\nparams = {\n    'ExtraTreesClassifier': { 'n_estimators': [16, 32] },\n    'RandomForestClassifier': { 'n_estimators': [16, 32] },\n    'AdaBoostClassifier':  { 'n_estimators': [16, 32] },\n    'GradientBoostingClassifier': { 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },\n    'SVC': [\n        {'kernel': ['linear'], 'C': [1, 10]},\n        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]},\n    ]\n}\n\nhelper = EstimatorSelectionHelper(models, params)\nhelper.fit(X_cancer, y_cancer, scoring = 'f1', n_jobs = -1)\nhelper.score_summary(sort_by = 'mean_score')\n\n\n\n\n\nSparse Matrix Passed, Dense Matrix Required\n\n\n\n\nhttps://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required\n\n\nhttp://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n\n\n\n\nfrom sklearn.base import TransformerMixin\n...\nclass DenseTransformer(TransformerMixin):\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y, **fit_params)\n        return self.transform(X)\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n...\npipeline = Pipeline([\n     ('vectorizer', CountVectorizer()), \n     ('to_dense', DenseTransformer()), \n     ('classifier', RandomForestClassifier())\n])", 
            "title": "Snippets"
        }, 
        {
            "location": "/python/sklearn/snippets/#usage", 
            "text": "GridSearchCV, Pipeline, fit_transform vs fit, AUC, ROC, how to do hyperparameter search before training/cross-validation and then testing?", 
            "title": "Usage"
        }, 
        {
            "location": "/python/sklearn/snippets/#gridsearchcv-over-multiple-models", 
            "text": "Normally  GridSearchCV  is for hyperparameter optimization. This method is extended using the following class, where you specify a dictionary of models and a dictionary of dictionaries of their parameters to search over. After fitting this helper, call  helper.score_summary(sort_by = 'mean_score')  to return a dataframe of every model fit with parameters and scoring metrics.   import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\n\nclass EstimatorSelectionHelper:\n\n    def __init__(self, models, params):\n        if not set(models.keys()).issubset(set(params.keys())):\n            missing_params = list(set(models.keys()) - set(params.keys()))\n            raise ValueError( Some estimators are missing parameters: %s  % missing_params)\n        self.models = models\n        self.params = params\n        self.keys = models.keys()\n        self.grid_searches = {}\n\n    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n        for key in self.keys:\n            print( Running GridSearchCV for %s.  % key)\n            model = self.models[key]\n            params = self.params[key]\n            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n                              verbose=verbose, scoring=scoring, refit=refit,\n                              return_train_score=True)\n            gs.fit(X,y)\n            self.grid_searches[key] = gs    \n\n    def score_summary(self, sort_by='mean_score'):\n        def row(key, scores, params):\n            d = {\n                 'estimator': key,\n                 'min_score': min(scores),\n                 'max_score': max(scores),\n                 'mean_score': np.mean(scores),\n                 'std_score': np.std(scores),\n            }\n            return pd.Series({**params,**d})\n\n        rows = []\n        for k in self.grid_searches:\n            print(k)\n            params = self.grid_searches[k].cv_results_['params']\n            scores = []\n            for i in range(self.grid_searches[k].cv):\n                key =  split{}_test_score .format(i)\n                r = self.grid_searches[k].cv_results_[key]        \n                scores.append(r.reshape(len(params),1))\n\n            all_scores = np.hstack(scores)\n            for p, s in zip(params,all_scores):\n                rows.append((row(k, s, p)))\n\n        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n\n        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n        columns = columns + [c for c in df.columns if c not in columns]\n\n        return df[columns]  Example use:  \nfrom sklearn import datasets\n\nbreast_cancer = datasets.load_breast_cancer()\nX_cancer = breast_cancer.data\ny_cancer = breast_cancer.target\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVC\n\nmodels = {\n    'ExtraTreesClassifier': ExtraTreesClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'SVC': SVC()\n}\n\nparams = {\n    'ExtraTreesClassifier': { 'n_estimators': [16, 32] },\n    'RandomForestClassifier': { 'n_estimators': [16, 32] },\n    'AdaBoostClassifier':  { 'n_estimators': [16, 32] },\n    'GradientBoostingClassifier': { 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },\n    'SVC': [\n        {'kernel': ['linear'], 'C': [1, 10]},\n        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]},\n    ]\n}\n\nhelper = EstimatorSelectionHelper(models, params)\nhelper.fit(X_cancer, y_cancer, scoring = 'f1', n_jobs = -1)\nhelper.score_summary(sort_by = 'mean_score')", 
            "title": "GridSearchCV Over Multiple Models"
        }, 
        {
            "location": "/python/sklearn/snippets/#sparse-matrix-passed-dense-matrix-required", 
            "text": "https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required  http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html   from sklearn.base import TransformerMixin\n...\nclass DenseTransformer(TransformerMixin):\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y, **fit_params)\n        return self.transform(X)\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n...\npipeline = Pipeline([\n     ('vectorizer', CountVectorizer()), \n     ('to_dense', DenseTransformer()), \n     ('classifier', RandomForestClassifier())\n])", 
            "title": "Sparse Matrix Passed, Dense Matrix Required"
        }, 
        {
            "location": "/software/artifactory/", 
            "text": "Installation\n\n\nCreate a new container with 4GB memory and 100GB storage and follow \nhttps://www.jfrog.com/confluence/display/RTF/Installing+on+Linux+Solaris+or+Mac+OS#InstallingonLinuxSolarisorMacOS-RPMorDebianInstallation\n\n\napt install default-jre -y\necho \ndeb https://jfrog.bintray.com/artifactory-debs {distribution} {components}\n | sudo tee -a /etc/apt/sources.list\n\nNote: If you are unsure, components should be \nmain.\n To determine your distribution, run lsb_release -c\n\nExample: echo \ndeb https://jfrog.bintray.com/artifactory-debs xenial main\n | sudo tee -a /etc/apt/sources.list\n\ncurl https://bintray.com/user/downloadSubjectPublicKey?username=jfrog | sudo apt-key add -\n\nsudo apt-get update\nsudo apt-get install jfrog-artifactory-oss\nsystemctl start artifactory.service\nsystemctl status artifactory.service\n\n\n\n\nThen go to \nhttp://ip:8081/artifactory\n to see the web GUI. Create an admin user and skip the rest of the steps.\n\n\nDocker Registry\n\n\nhttps://www.jfrog.com/confluence/display/RTF/Docker+Registry\n\n\nLooks to be the case that this is a paid feature. There is a generic repository feature that I can look into. Alternatively we can directly use the docker server as a docker registry.", 
            "title": "Artifactory"
        }, 
        {
            "location": "/software/artifactory/#installation", 
            "text": "Create a new container with 4GB memory and 100GB storage and follow  https://www.jfrog.com/confluence/display/RTF/Installing+on+Linux+Solaris+or+Mac+OS#InstallingonLinuxSolarisorMacOS-RPMorDebianInstallation  apt install default-jre -y\necho  deb https://jfrog.bintray.com/artifactory-debs {distribution} {components}  | sudo tee -a /etc/apt/sources.list\n\nNote: If you are unsure, components should be  main.  To determine your distribution, run lsb_release -c\n\nExample: echo  deb https://jfrog.bintray.com/artifactory-debs xenial main  | sudo tee -a /etc/apt/sources.list\n\ncurl https://bintray.com/user/downloadSubjectPublicKey?username=jfrog | sudo apt-key add -\n\nsudo apt-get update\nsudo apt-get install jfrog-artifactory-oss\nsystemctl start artifactory.service\nsystemctl status artifactory.service  Then go to  http://ip:8081/artifactory  to see the web GUI. Create an admin user and skip the rest of the steps.", 
            "title": "Installation"
        }, 
        {
            "location": "/software/artifactory/#docker-registry", 
            "text": "https://www.jfrog.com/confluence/display/RTF/Docker+Registry  Looks to be the case that this is a paid feature. There is a generic repository feature that I can look into. Alternatively we can directly use the docker server as a docker registry.", 
            "title": "Docker Registry"
        }, 
        {
            "location": "/software/bittorrent_sync/", 
            "text": "Installation And Configuration\n\n\n\n\nCreate a new container on Superior\n\n\nDo \necho \"deb http://linux-packages.resilio.com/resilio-sync/deb resilio-sync non-free\" | tee /etc/apt/sources.list.d/resilio-sync.list\n\n\nDo \nwget -qO - https://linux-packages.resilio.com/resilio-sync/key.asc | apt-key add -\n\n\nDo \napt-get update \n apt-get install resilio-sync\n\n\nEdit \n/usr/lib/systemd/user/resilio-sync.service\n with \nWantedBy=default.target\n\n\nEnable the service \nsystemctl --user enable resilio-sync\n\n\nInstead enable the service with \nsystemctl enable resilio-sync\n to use user \nrslsync\n and then start it with \nsystemctl start resilio-sync\n\n\nChange web GUI listening interface \n./rslsync --webui.listen 0.0.0.0:8888\n\n\nEdit \n/etc/resilio-sync/config.json\n and \n/etc/resilio-sync/user_config.json\n And change the listening address to \n0.0.0.0:8888\n as an alternative to above\n\n\nGo to \nhttp://bittorrentsyncip:8888\n with username alex and software password\n\n\nAdd syncing folder under \n/home/rslsync/\n\n\nUseful articles \nhttps://help.resilio.com/hc/en-us/articles/206178924-Installing-Sync-package-on-Linux\n and \nhttps://help.resilio.com/hc/en-us/articles/204762449-Guide-to-Linux", 
            "title": "Bittorrent sync"
        }, 
        {
            "location": "/software/bittorrent_sync/#installation-and-configuration", 
            "text": "Create a new container on Superior  Do  echo \"deb http://linux-packages.resilio.com/resilio-sync/deb resilio-sync non-free\" | tee /etc/apt/sources.list.d/resilio-sync.list  Do  wget -qO - https://linux-packages.resilio.com/resilio-sync/key.asc | apt-key add -  Do  apt-get update   apt-get install resilio-sync  Edit  /usr/lib/systemd/user/resilio-sync.service  with  WantedBy=default.target  Enable the service  systemctl --user enable resilio-sync  Instead enable the service with  systemctl enable resilio-sync  to use user  rslsync  and then start it with  systemctl start resilio-sync  Change web GUI listening interface  ./rslsync --webui.listen 0.0.0.0:8888  Edit  /etc/resilio-sync/config.json  and  /etc/resilio-sync/user_config.json  And change the listening address to  0.0.0.0:8888  as an alternative to above  Go to  http://bittorrentsyncip:8888  with username alex and software password  Add syncing folder under  /home/rslsync/  Useful articles  https://help.resilio.com/hc/en-us/articles/206178924-Installing-Sync-package-on-Linux  and  https://help.resilio.com/hc/en-us/articles/204762449-Guide-to-Linux", 
            "title": "Installation And Configuration"
        }, 
        {
            "location": "/software/calibre/", 
            "text": "Calibre Installation\n\n\nsudo -v \n wget -nv -O- https://download.calibre-ebook.com/linux-installer.sh | sudo sh /dev/stdin\n gives a broken pipe error. Alternatively, use \napt-get install calibre\n. Then start the application with \ncalibre-server\n and go to port \n8080\n.\n\n\nCalibre-Web Installation\n\n\nClone \nhttps://github.com/janeczku/calibre-web\n and then \npython-pip\n with \npip install --target vendor -r requirements.txt\n then \nnohup python cps.py\n and go to port \n8083\n with username admin and password admin123. Set the location of the Calibre database \nmetadata.db\n to \n/mnt/books/\n and submit. Edit \n/etc/rc.local\n with the appropriate python call to start server on boot. Make sure to add upload books option in configuration. Upload books using GUI and be sure to use find metadata option to get correct description, titles, book cover, etc.", 
            "title": "Calibre"
        }, 
        {
            "location": "/software/calibre/#calibre-installation", 
            "text": "sudo -v   wget -nv -O- https://download.calibre-ebook.com/linux-installer.sh | sudo sh /dev/stdin  gives a broken pipe error. Alternatively, use  apt-get install calibre . Then start the application with  calibre-server  and go to port  8080 .", 
            "title": "Calibre Installation"
        }, 
        {
            "location": "/software/calibre/#calibre-web-installation", 
            "text": "Clone  https://github.com/janeczku/calibre-web  and then  python-pip  with  pip install --target vendor -r requirements.txt  then  nohup python cps.py  and go to port  8083  with username admin and password admin123. Set the location of the Calibre database  metadata.db  to  /mnt/books/  and submit. Edit  /etc/rc.local  with the appropriate python call to start server on boot. Make sure to add upload books option in configuration. Upload books using GUI and be sure to use find metadata option to get correct description, titles, book cover, etc.", 
            "title": "Calibre-Web Installation"
        }, 
        {
            "location": "/software/ci_cd/", 
            "text": "Idea\n\n\nhttps://github.com/ajdurbin/test contains a sample application that follows all of the CI/CD concepts. We build a web application using Flask, create a Docker image for it, have a test suite, use Travis-CI's GitHub integration to build the image and run tests before deploying to Docker Hub, then use Heroku's GitHub integration to deploy the application online after Travis-CI passes. Travis-CI allows us to define private environment variables in each repository's settings that can be referenced in any scripts. This is very useful for passwords. \n\n\n\n\nhttps://docs.travis-ci.com/user/docker/\n\n\nhttps://docs.travis-ci.com/user/deployment/heroku/\n\n\nhttps://devcenter.heroku.com/articles/github-integration\n\n\nhttps://medium.com/bettercode/how-to-build-a-modern-ci-cd-pipeline-5faa01891a5b\n\n\nhttps://docs.travis-ci.com/user/getting-started/\n\n\nhttps://medium.com/craft-academy/deploying-heroku-and-travis-ci-69d998ad408a\n\n\nhttps://codeburst.io/ci-cd-with-github-travis-ci-and-heroku-e088a24f32ef\n\n\n\n\nSelfhosted\n\n\nTravis-CI does not really have a local image to be used. The major open-source alternative is Jenkins, but I have yet to be able to set it up appropriately using Docker to provision the slaves. Alternatives include Drone, CircleCI, Gitlab CI/CD. Drone is packaged as a Docker image and also uses YAML files like Travis-CI. Heroku is an excellent service for running Docker web applications and has 500 hours free per month usage. It has a command-line utility that is easily installed to test applications to be deployed before committing changes and triggering a build. But this is a supervised method, so it can be useful for developing first. Heroku also uses a Procfile that we need to find out more about.\n\n\n\n\nhttp://docs.drone.io/getting-started/\n\n\nhttps://devcenter.heroku.com/articles/procfile\n\n\n\n\nSample .travis.yml\n\n\nsudo: required\nservices:\n - docker\nlanguage: python\npython:\n - \n3.5\n\ninstall:\n - pip install -r requirements.txt\nscript:\n - python -m pytest -v\nafter_success:\n - echo \n$DOCKER_PASS\n | docker login -u \n$DOCKER_USER\n --password-stdin\n - export APP=test\n - export REPO=$DOCKER_USER/$APP\n - export TAG=`if [ \n$TRAVIS_BRANCH\n == \nmaster\n ]; then echo \nlatest\n; else echo $TRAVIS_BRANCH ; fi`\n - docker build -f Dockerfile -t $REPO:$TAG -t $REPO:travis-$TRAVIS_BUILD_NUMBER .\n - docker push $REPO:$TAG\n - docker push $REPO:travis-$TRAVIS_BUILD_NUMBER", 
            "title": "Ci cd"
        }, 
        {
            "location": "/software/ci_cd/#idea", 
            "text": "https://github.com/ajdurbin/test contains a sample application that follows all of the CI/CD concepts. We build a web application using Flask, create a Docker image for it, have a test suite, use Travis-CI's GitHub integration to build the image and run tests before deploying to Docker Hub, then use Heroku's GitHub integration to deploy the application online after Travis-CI passes. Travis-CI allows us to define private environment variables in each repository's settings that can be referenced in any scripts. This is very useful for passwords.    https://docs.travis-ci.com/user/docker/  https://docs.travis-ci.com/user/deployment/heroku/  https://devcenter.heroku.com/articles/github-integration  https://medium.com/bettercode/how-to-build-a-modern-ci-cd-pipeline-5faa01891a5b  https://docs.travis-ci.com/user/getting-started/  https://medium.com/craft-academy/deploying-heroku-and-travis-ci-69d998ad408a  https://codeburst.io/ci-cd-with-github-travis-ci-and-heroku-e088a24f32ef", 
            "title": "Idea"
        }, 
        {
            "location": "/software/ci_cd/#selfhosted", 
            "text": "Travis-CI does not really have a local image to be used. The major open-source alternative is Jenkins, but I have yet to be able to set it up appropriately using Docker to provision the slaves. Alternatives include Drone, CircleCI, Gitlab CI/CD. Drone is packaged as a Docker image and also uses YAML files like Travis-CI. Heroku is an excellent service for running Docker web applications and has 500 hours free per month usage. It has a command-line utility that is easily installed to test applications to be deployed before committing changes and triggering a build. But this is a supervised method, so it can be useful for developing first. Heroku also uses a Procfile that we need to find out more about.   http://docs.drone.io/getting-started/  https://devcenter.heroku.com/articles/procfile", 
            "title": "Selfhosted"
        }, 
        {
            "location": "/software/ci_cd/#sample-travisyml", 
            "text": "sudo: required\nservices:\n - docker\nlanguage: python\npython:\n -  3.5 \ninstall:\n - pip install -r requirements.txt\nscript:\n - python -m pytest -v\nafter_success:\n - echo  $DOCKER_PASS  | docker login -u  $DOCKER_USER  --password-stdin\n - export APP=test\n - export REPO=$DOCKER_USER/$APP\n - export TAG=`if [  $TRAVIS_BRANCH  ==  master  ]; then echo  latest ; else echo $TRAVIS_BRANCH ; fi`\n - docker build -f Dockerfile -t $REPO:$TAG -t $REPO:travis-$TRAVIS_BUILD_NUMBER .\n - docker push $REPO:$TAG\n - docker push $REPO:travis-$TRAVIS_BUILD_NUMBER", 
            "title": "Sample .travis.yml"
        }, 
        {
            "location": "/software/docker/", 
            "text": "Do I want separate docker image from jenkins and git server? Still looking for good tutorial to tie together. Probably also okay if they're all on single image too. Need to find a good tutorial on this, how does docker get notified to run the image? \n\n\nIdeally would want a git push to trigger a jenkins build that pushes to artifactory (internal docker registry) that then deploys it on the docker server. All of these ideally would be separate components. \n\n\nUbuntu Server Image\n\n\nDocker cannot run inside LXC containers, so we need to make a full virtual machine for it. Ubuntu server is minimal enough for this task.\n\n\nStart the VM with ample disk space and memory and follow installation. Skip HTTP proxy. Manual network installation. Select standard system utilities and OpenSSH server.\n\n\nDocker Installation\n\n\nhttps://docs.docker.com/install/linux/docker-ce/ubuntu/\n\n\nsudo apt-get update\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nsudo add-apt-repository \\\n   \ndeb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\n\nsudo apt-get update\nsudo apt-get install docker-ce\nsudo docker run hello-world\nsudo systemctl enable docker\n\n\n\n\nFrom \n  - http://www.littlebigextra.com/how-to-enable-remote-rest-api-on-docker-host/\n  - https://success.docker.com/article/how-do-i-enable-the-remote-api-for-dockerd\n\n\nTo enable the docker API from the server, edit \n/lib/systemd/system/docker.service\n with \nExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2376\n and then do \nsudo systemctl daemon-reload \n sudo service docker restart\n. Then test the server is accessible with \ncurl http://dockerserverip:2376/images/json\n to get a JSON string of the docker containers installed. \n\n\nTo manage Docker without \nsudo\n:\n\n\nsudo groupadd docker\nsudo usermod -aG docker $USER\nexit # then log back in\n\n\n\n\nDocker Hub\n\n\nCreate an account on Docker Hub. Create a private repository //homelab//.\n\n\nDocker Registry\n\n\n\n\nhttps://docs.docker.com/registry/deploying/#get-a-certificate\n\n\nhttps://docs.docker.com/registry/deploying/\n\n\nhttp://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/\n\n\nhttp://tech.paulcz.net/2016/01/secure-docker-with-tls/\n\n\n\n\nIt's probably easiest to do this if we host Jenkins on the Docker host. This way we don't need to worry about setting up SSL/TLS for the Docker registry. Also that removes a step from the Jenkins pipeline, since we can wrap the build/push Docker image in a single step and the final step can just be pulling the image from the Docker registry and deploying. Next question is if we want to host the git server on the Docker host too. \n\n\nDocker Compose\n\n\nTool for defining and running multi-container Docker applications \nhttps://docs.docker.com/compose/\n\n\nDocker Swarm\n\n\nFor a cluster of Docker containers. Though almost everyone uses Kubernetes for this now.", 
            "title": "Docker"
        }, 
        {
            "location": "/software/docker/#ubuntu-server-image", 
            "text": "Docker cannot run inside LXC containers, so we need to make a full virtual machine for it. Ubuntu server is minimal enough for this task.  Start the VM with ample disk space and memory and follow installation. Skip HTTP proxy. Manual network installation. Select standard system utilities and OpenSSH server.", 
            "title": "Ubuntu Server Image"
        }, 
        {
            "location": "/software/docker/#docker-installation", 
            "text": "https://docs.docker.com/install/linux/docker-ce/ubuntu/  sudo apt-get update\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nsudo add-apt-repository \\\n    deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable \nsudo apt-get update\nsudo apt-get install docker-ce\nsudo docker run hello-world\nsudo systemctl enable docker  From \n  - http://www.littlebigextra.com/how-to-enable-remote-rest-api-on-docker-host/\n  - https://success.docker.com/article/how-do-i-enable-the-remote-api-for-dockerd  To enable the docker API from the server, edit  /lib/systemd/system/docker.service  with  ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2376  and then do  sudo systemctl daemon-reload   sudo service docker restart . Then test the server is accessible with  curl http://dockerserverip:2376/images/json  to get a JSON string of the docker containers installed.   To manage Docker without  sudo :  sudo groupadd docker\nsudo usermod -aG docker $USER\nexit # then log back in", 
            "title": "Docker Installation"
        }, 
        {
            "location": "/software/docker/#docker-hub", 
            "text": "Create an account on Docker Hub. Create a private repository //homelab//.", 
            "title": "Docker Hub"
        }, 
        {
            "location": "/software/docker/#docker-registry", 
            "text": "https://docs.docker.com/registry/deploying/#get-a-certificate  https://docs.docker.com/registry/deploying/  http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/  http://tech.paulcz.net/2016/01/secure-docker-with-tls/   It's probably easiest to do this if we host Jenkins on the Docker host. This way we don't need to worry about setting up SSL/TLS for the Docker registry. Also that removes a step from the Jenkins pipeline, since we can wrap the build/push Docker image in a single step and the final step can just be pulling the image from the Docker registry and deploying. Next question is if we want to host the git server on the Docker host too.", 
            "title": "Docker Registry"
        }, 
        {
            "location": "/software/docker/#docker-compose", 
            "text": "Tool for defining and running multi-container Docker applications  https://docs.docker.com/compose/", 
            "title": "Docker Compose"
        }, 
        {
            "location": "/software/docker/#docker-swarm", 
            "text": "For a cluster of Docker containers. Though almost everyone uses Kubernetes for this now.", 
            "title": "Docker Swarm"
        }, 
        {
            "location": "/software/emby/", 
            "text": "Installation\n\n\n\n\nCreate an Ubuntu container with the appropriate mount points\n\n\nDownload the Emby server \n.deb\n\n\ndpkg -i emby-server...\n\n\nGo to \nhttp://localhost:8096", 
            "title": "Emby"
        }, 
        {
            "location": "/software/emby/#installation", 
            "text": "Create an Ubuntu container with the appropriate mount points  Download the Emby server  .deb  dpkg -i emby-server...  Go to  http://localhost:8096", 
            "title": "Installation"
        }, 
        {
            "location": "/software/gitea/", 
            "text": "Installation\n\n\napt install git -y\nwget -O gitea https://dl.gitea.io/gitea/1.4/gitea-1.4-linux-amd64\nchmod +x gitea\n./gitea web\n\n\n\n\nChoose SQLite3 database and install after setting up admin account, also change Application URL, Domain to IP address instead of localhost.\n\n\nRoot user okay for running, but setup ssh keys or else you cannot clone/push to a repository over SSH.\n\n\nAdd \n/root/gitea web\n to \n/etc/rc.local\n before \nexit 0\n to start on boot", 
            "title": "Gitea"
        }, 
        {
            "location": "/software/gitea/#installation", 
            "text": "apt install git -y\nwget -O gitea https://dl.gitea.io/gitea/1.4/gitea-1.4-linux-amd64\nchmod +x gitea\n./gitea web  Choose SQLite3 database and install after setting up admin account, also change Application URL, Domain to IP address instead of localhost.  Root user okay for running, but setup ssh keys or else you cannot clone/push to a repository over SSH.  Add  /root/gitea web  to  /etc/rc.local  before  exit 0  to start on boot", 
            "title": "Installation"
        }, 
        {
            "location": "/software/gogs/", 
            "text": "Installation\n\n\napt-get install mysql-server -y\n with software password\n\n\nAccess SQL server with \nmysql -u root -p\n and type in password, then \ndrop table if exists gogs; create database if not exists gogs character set utf8 collate utf8_general_ci; exit\n\n\napt install git -y\n\n\nwget\n the tar file from gogs binary files and \ntar zxf gogs.gz\n\n\ncd gogs\n and \n./gogs web\n\n\nGo to \nhttp://gogsip:3000\n\n\nChange run user to root\n\n\nAdd admin account settings with software password\n\n\nInstall and refresh page, will be logged in as admin account\n\n\nFor cloning/pushing/etc use HTTP instead of SSH and provide admin account and password until SSH keys are setup\n\n\nLook into mirroring repository option instead of multiple pushes to BitBucket and GitHub", 
            "title": "Gogs"
        }, 
        {
            "location": "/software/gogs/#installation", 
            "text": "apt-get install mysql-server -y  with software password  Access SQL server with  mysql -u root -p  and type in password, then  drop table if exists gogs; create database if not exists gogs character set utf8 collate utf8_general_ci; exit  apt install git -y  wget  the tar file from gogs binary files and  tar zxf gogs.gz  cd gogs  and  ./gogs web  Go to  http://gogsip:3000  Change run user to root  Add admin account settings with software password  Install and refresh page, will be logged in as admin account  For cloning/pushing/etc use HTTP instead of SSH and provide admin account and password until SSH keys are setup  Look into mirroring repository option instead of multiple pushes to BitBucket and GitHub", 
            "title": "Installation"
        }, 
        {
            "location": "/software/ideas/", 
            "text": "About\n\n\nThis page is to log different software packages with short descriptions for possible later inclusion in my home lab. A lot of these will be from \nhttps://github.com/Kickball/awesome-selfhosted\n.\n\n\nVirtualization\n\n\n\n\nDocker\n\n\nlxc\n\n\nProxmox\n\n\n\n\nFile Syncing\n\n\n\n\nResilio-Sync: Has Android/iOS applications and FreeNas plugin. Works well enough even though user permissions on installation are weird. Closed source.\n\n\nSyncthing: Has Android/iOS applications. Documentation not great for steps after installation to actually sync the folders from one site to another.\n\n\nRClone: Powerful tool for syncing folders/files to Google Drive. Great documentation.\n\n\nrsync: Command line tool for syncing folders or files one way or both ways with deletion or not.\n\n\nNextCloud: Selfhosted Google Drive replacement with Calendar, instant messaging support, etc. \n\n\n\n\nMachine Learning and Statistics\n\n\n\n\ncomet.ml: Cool utility for sharing models with other's including documentation, git support, etc.\n\n\ngluon: AWS/Microsoft tool similar to comet.ml\n\n\nTensorBoard: Visualize statistics from TensorFlow models.\n\n\nnbviewer: view notebooks from git repository as a website or using a static site generator\n\n\njupyter hub: Jupyter notebook server and Python kernel for multiple users\n\n\nRStudio Server: R server with web GUI\n\n\nR Shiny server: Server for selfhosting R Shiny applications\n\n\n\n\nVisualizations\n\n\n\n\nGrafana\n\n\n\n\nDatabases\n\n\n\n\nGraphite\n\n\ninfluxdb\n\n\nKafka\n\n\nCassandra\n\n\nHadoop/Spark\n\n\n\n\nWeb services\n\n\n\n\nreverse proxy to safely expose applications to outside world or just static site\n\n\nRedis for server side caching for web applications\n\n\n\n\nData Collection\n\n\n\n\nOwntracks\n\n\nOpenWeatherMap\n\n\nopenstreetmap\n\n\npersonal statistics\n\n\nCTV\n\n\ntimed still-shot photos of outdoors\n\n\n\n\nCI/CD\n\n\n\n\nArtifactory to store images\n\n\nJenkins to build images from git repository\n\n\nkubernetes to deploy docker applications\n\n\nInternal pypi repository\n\n\n\n\nEntertainment\n\n\n\n\nPlex\n\n\nplex channels\n\n\nSonarr\n\n\nJacket\n\n\nRadarr\n\n\nCalibre\n\n\nMinecraft server\n\n\nInsurgency server?\n\n\n\n\ntools\n\n\n\n\nzabbix for monitoring hosts\n\n\ninternal dns server besides pfsense host?\n\n\nauthentication like FreeIPA\n\n\nIP phones, FreePBX\n\n\n\n\nHPC\n\n\n\n\nslurm/torque/whatever job schedular\n\n\nenvironment modules for different software versions\n\n\nmpi\n\n\nmp\n\n\n\n\nCommunication\n\n\n\n\nDiscord\n\n\nIRC\n\n\nRSS\n\n\n\n\nword salad ###\n\n\n\n\ntodo list outside wiki\n\n\nuse Google Calendar more, contacts for address book\n\n\nchatbot\n\n\nMkdocs wiki behind reverse proxy and pushed to github pages or readthedocs (can remove signature that bugs me) with notebooks on github provided to nbviewer in some notebook repository\n\n\nowntracks behind https\n\n\nzabbix statistics\n\n\ngraphite stats exported from freenas, proxmox very easily, push to grafana\n\n\nshiny reverse proxy\n\n\ngithub tags for model passing pypi, etc\n\n\nselfhosted git and push to bitbucket, github also\n\n\nmerge both data analytics repositories\n\n\nmake a notebook repository and copy files there \n\n\nmake notebook viewer application\n\n\nlink notebooks from github to nbviewer online\n\n\nupload all of grad school stuff to repos, do some cleaning up of miscellaneous notebook ones with better names\n\n\nraspberry pi mkdocs installation\n\n\nstatistics and deep learning should all be single directory in new wiki\n\n\nget back into journal\n\n\ncustomizing bashrc, put bashrc rcprofile, any relevant config files in unix directory, reverse proxy files\n\n\nDUO two factor\n\n\nmkdocs gh-deploy for new documentation, poor performance on raspberry pi and annoying editing documents in real time with log files printed to screen, okay with running in vm or not? Deploying to github messed up personal website root, so also need to see if I'm fine not having personal website or not or including in new documentation.\n\n\nUse gitea instead of gogs and figure out ssh key situation for pushing since it's getting annoying using passwords\n\n\nreorganize python folders for notebook viewer\n\n\ncreate snippets repository like Bob Settlage?\n\n\nRstudio/rshiny server in single machine\n\n\njekyll for github pages, maybe that's okay to have github root page jekyll with wiki backend if single page?\n\n\nTry reverse proxy of shiny applications, wiki\n\n\nFetch ASC code from Peter's repository since we worked on these things and there are lots of examples\n\n\nJournal? switch to python package or keep using Rmd\n\n\nGraphite/grafana fetch statistics from huron, superior maybe separate instance for open weather map\n\n\nown tracks\n\n\ntravic ci instead of jenkins, need to figure out local installation\n\n\nkubernetes on superior root\n\n\nci/cd pipeline with git -\n jenkins/travisci -\n artifactory -\n kubernetes deployment\n\n\nunit testing with tox, other python packages\n\n\nlist of standard R packages\n\n\nheroku\n\n\nredis server for web caching\n\n\nalpine linux images\n\n\nfix annoying windows/ubuntu dual boot time issue\n\n\nblender for 3d modeling\n\n\nlook more into python parallel/distributed computing packages\n\n\nif pushing code to github always make a gh-pages branch with some docs in it to be pushed too, at least for some longer projects for nicer to read stuff\n\n\ngit web hooks\n\n\ncombine statistics/ml/dl/nlp pages into single directory\n\n\ngithub pages personal page can't be with subdirectories, so try having only single top level page. mkdocs routing to convert dokuwiki text, need to rename start to index\n\n\nManual mkdocs gh-deploy routine with ssh keys and checkout\n\n\nsample project to test sphinx functionality?\n\n\ncontinue using dokuwiki on pi (just so it can be used), and then convert files to mkdocs for github pages for outside usage?\n\n\nwebhooks to trigger ci/cd pipelines\n\n\nweb service for hosting applications from flask/tornado/gnuicorn\n\n\nheroku/pythonanywhere\n\n\nmongo/mariadb instead of mysql - recall https://www.reddit.com/r/AskReddit/comments/8fztrk/what_was_the_removing_the_headphone_jack_of/ and discussion of Oracle no longer developing mysql basically\n\n\nfull virtualmachine with docker installed as docker host\n\n\njenkins dockerfile github to artifactory and docker using travis ci when can\n\n\nappears that marathon is only for mesos-based systems\n\n\nkubernetes?\n\n\ntraefik\n\n\n\"hacking\" fake terminal\n\n\ncalibre server for ebooks\n\n\nget docker jenkins git pipeline first before worrying about artifactory\n\n\nload balancer\n\n\ninternal dns\n\n\ndocker swarm/compose versus a kubernetes solution\n\n\nsome sort of docker load balancing\n\n\ndocker traefik\n\n\ngitlab has their own ci/cd stuff similar to jenkins/travis-ci\n\n\nbettercodehub\n\n\nstatic site generators like Hugo\n\n\nreverse proxy for non-static sites, like dokuwiki for instance or any other php web application\n\n\ndigital ocean tutorials\n\n\nteraform\n\n\ncaddy server\n\n\nhaproxy\n\n\nfreeipa\n\n\nfreepbx\n\n\nreddit raspberry pi ups server tutorial\n\n\nrecipe manager\n\n\ndocker spark/hadoop images\n\n\nsonarqube\n\n\n\n\nMain goal is to get working git-jenkins-artifactory-docker plugin going. So we'd have a remote git repository to trigger a jenkins build that tests the software before sending to artifactory and then deploying on the docker server. So the jenkins file will be split up to be build the image using docker server, then push image to docker registry, then pull image from docker registry and run. \n\n\nDo more scientific computing so go back to learning a compiled code framework, more R shiny stuff, MCMC, Bayesian stats, maybe go back to C++ to do some CUDA, more web development with Flask/Django\n\n\nThis is a great tutorial: \nhttps://medium.com/bettercode/how-to-build-a-modern-ci-cd-pipeline-5faa01891a5b\n and somewhat related \nhttps://medium.com/@evheniybystrov/continuous-delivery-of-react-app-with-jenkins-and-docker-8a1ae1511b86\n\n\nPlan\n: Put Jenkins on Docker host and use Docker's built in registry, look into Docker git server that's lightweight enough. Use these for development and then push applications to Heroku and use travis-ci for Github.", 
            "title": "Ideas"
        }, 
        {
            "location": "/software/ideas/#about", 
            "text": "This page is to log different software packages with short descriptions for possible later inclusion in my home lab. A lot of these will be from  https://github.com/Kickball/awesome-selfhosted .", 
            "title": "About"
        }, 
        {
            "location": "/software/ideas/#virtualization", 
            "text": "Docker  lxc  Proxmox", 
            "title": "Virtualization"
        }, 
        {
            "location": "/software/ideas/#file-syncing", 
            "text": "Resilio-Sync: Has Android/iOS applications and FreeNas plugin. Works well enough even though user permissions on installation are weird. Closed source.  Syncthing: Has Android/iOS applications. Documentation not great for steps after installation to actually sync the folders from one site to another.  RClone: Powerful tool for syncing folders/files to Google Drive. Great documentation.  rsync: Command line tool for syncing folders or files one way or both ways with deletion or not.  NextCloud: Selfhosted Google Drive replacement with Calendar, instant messaging support, etc.", 
            "title": "File Syncing"
        }, 
        {
            "location": "/software/ideas/#machine-learning-and-statistics", 
            "text": "comet.ml: Cool utility for sharing models with other's including documentation, git support, etc.  gluon: AWS/Microsoft tool similar to comet.ml  TensorBoard: Visualize statistics from TensorFlow models.  nbviewer: view notebooks from git repository as a website or using a static site generator  jupyter hub: Jupyter notebook server and Python kernel for multiple users  RStudio Server: R server with web GUI  R Shiny server: Server for selfhosting R Shiny applications", 
            "title": "Machine Learning and Statistics"
        }, 
        {
            "location": "/software/ideas/#visualizations", 
            "text": "Grafana", 
            "title": "Visualizations"
        }, 
        {
            "location": "/software/ideas/#databases", 
            "text": "Graphite  influxdb  Kafka  Cassandra  Hadoop/Spark", 
            "title": "Databases"
        }, 
        {
            "location": "/software/ideas/#web-services", 
            "text": "reverse proxy to safely expose applications to outside world or just static site  Redis for server side caching for web applications", 
            "title": "Web services"
        }, 
        {
            "location": "/software/ideas/#data-collection", 
            "text": "Owntracks  OpenWeatherMap  openstreetmap  personal statistics  CTV  timed still-shot photos of outdoors", 
            "title": "Data Collection"
        }, 
        {
            "location": "/software/ideas/#cicd", 
            "text": "Artifactory to store images  Jenkins to build images from git repository  kubernetes to deploy docker applications  Internal pypi repository", 
            "title": "CI/CD"
        }, 
        {
            "location": "/software/ideas/#entertainment", 
            "text": "Plex  plex channels  Sonarr  Jacket  Radarr  Calibre  Minecraft server  Insurgency server?", 
            "title": "Entertainment"
        }, 
        {
            "location": "/software/ideas/#tools", 
            "text": "zabbix for monitoring hosts  internal dns server besides pfsense host?  authentication like FreeIPA  IP phones, FreePBX", 
            "title": "tools"
        }, 
        {
            "location": "/software/ideas/#hpc", 
            "text": "slurm/torque/whatever job schedular  environment modules for different software versions  mpi  mp", 
            "title": "HPC"
        }, 
        {
            "location": "/software/ideas/#communication", 
            "text": "Discord  IRC  RSS", 
            "title": "Communication"
        }, 
        {
            "location": "/software/ideas/#word-salad", 
            "text": "todo list outside wiki  use Google Calendar more, contacts for address book  chatbot  Mkdocs wiki behind reverse proxy and pushed to github pages or readthedocs (can remove signature that bugs me) with notebooks on github provided to nbviewer in some notebook repository  owntracks behind https  zabbix statistics  graphite stats exported from freenas, proxmox very easily, push to grafana  shiny reverse proxy  github tags for model passing pypi, etc  selfhosted git and push to bitbucket, github also  merge both data analytics repositories  make a notebook repository and copy files there   make notebook viewer application  link notebooks from github to nbviewer online  upload all of grad school stuff to repos, do some cleaning up of miscellaneous notebook ones with better names  raspberry pi mkdocs installation  statistics and deep learning should all be single directory in new wiki  get back into journal  customizing bashrc, put bashrc rcprofile, any relevant config files in unix directory, reverse proxy files  DUO two factor  mkdocs gh-deploy for new documentation, poor performance on raspberry pi and annoying editing documents in real time with log files printed to screen, okay with running in vm or not? Deploying to github messed up personal website root, so also need to see if I'm fine not having personal website or not or including in new documentation.  Use gitea instead of gogs and figure out ssh key situation for pushing since it's getting annoying using passwords  reorganize python folders for notebook viewer  create snippets repository like Bob Settlage?  Rstudio/rshiny server in single machine  jekyll for github pages, maybe that's okay to have github root page jekyll with wiki backend if single page?  Try reverse proxy of shiny applications, wiki  Fetch ASC code from Peter's repository since we worked on these things and there are lots of examples  Journal? switch to python package or keep using Rmd  Graphite/grafana fetch statistics from huron, superior maybe separate instance for open weather map  own tracks  travic ci instead of jenkins, need to figure out local installation  kubernetes on superior root  ci/cd pipeline with git -  jenkins/travisci -  artifactory -  kubernetes deployment  unit testing with tox, other python packages  list of standard R packages  heroku  redis server for web caching  alpine linux images  fix annoying windows/ubuntu dual boot time issue  blender for 3d modeling  look more into python parallel/distributed computing packages  if pushing code to github always make a gh-pages branch with some docs in it to be pushed too, at least for some longer projects for nicer to read stuff  git web hooks  combine statistics/ml/dl/nlp pages into single directory  github pages personal page can't be with subdirectories, so try having only single top level page. mkdocs routing to convert dokuwiki text, need to rename start to index  Manual mkdocs gh-deploy routine with ssh keys and checkout  sample project to test sphinx functionality?  continue using dokuwiki on pi (just so it can be used), and then convert files to mkdocs for github pages for outside usage?  webhooks to trigger ci/cd pipelines  web service for hosting applications from flask/tornado/gnuicorn  heroku/pythonanywhere  mongo/mariadb instead of mysql - recall https://www.reddit.com/r/AskReddit/comments/8fztrk/what_was_the_removing_the_headphone_jack_of/ and discussion of Oracle no longer developing mysql basically  full virtualmachine with docker installed as docker host  jenkins dockerfile github to artifactory and docker using travis ci when can  appears that marathon is only for mesos-based systems  kubernetes?  traefik  \"hacking\" fake terminal  calibre server for ebooks  get docker jenkins git pipeline first before worrying about artifactory  load balancer  internal dns  docker swarm/compose versus a kubernetes solution  some sort of docker load balancing  docker traefik  gitlab has their own ci/cd stuff similar to jenkins/travis-ci  bettercodehub  static site generators like Hugo  reverse proxy for non-static sites, like dokuwiki for instance or any other php web application  digital ocean tutorials  teraform  caddy server  haproxy  freeipa  freepbx  reddit raspberry pi ups server tutorial  recipe manager  docker spark/hadoop images  sonarqube   Main goal is to get working git-jenkins-artifactory-docker plugin going. So we'd have a remote git repository to trigger a jenkins build that tests the software before sending to artifactory and then deploying on the docker server. So the jenkins file will be split up to be build the image using docker server, then push image to docker registry, then pull image from docker registry and run.   Do more scientific computing so go back to learning a compiled code framework, more R shiny stuff, MCMC, Bayesian stats, maybe go back to C++ to do some CUDA, more web development with Flask/Django  This is a great tutorial:  https://medium.com/bettercode/how-to-build-a-modern-ci-cd-pipeline-5faa01891a5b  and somewhat related  https://medium.com/@evheniybystrov/continuous-delivery-of-react-app-with-jenkins-and-docker-8a1ae1511b86  Plan : Put Jenkins on Docker host and use Docker's built in registry, look into Docker git server that's lightweight enough. Use these for development and then push applications to Heroku and use travis-ci for Github.", 
            "title": "word salad ###"
        }, 
        {
            "location": "/software/jenkins/", 
            "text": "Installation\n\n\nFollowing documentation at \nhttps://jenkins.io/doc/book/installing/\n, create a new container with 50GB storage and 1GB memory. Follow Ubuntu instructions on the documentation\n\n\nwget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\nsudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ \n /etc/apt/sources.list.d/jenkins.list'\nsudo apt-get update\nsudo apt-get install jenkins\n\n\n\n\nThen go to \nhttp://jenkinsip:8080\n to finish the installation. Grab the Jenkins password from \n/var/jenkins_home/secrets/initialAdminPassword\n, choose install suggested plugins, and then create first administrator user with username alex. \n\n\n\n\nhttps://jenkins.io/doc/book/pipeline/docker/\n\n\nhttps://go.cloudbees.com/docs/cloudbees-documentation/cje-user-guide/index.html#docker-workflow\n\n\nhttps://getintodevops.com/blog/building-your-first-docker-image-with-jenkins-2-guide-for-developers\n\n\nhttp://www.littlebigextra.com/using-jenkins-to-build-and-deploy-docker-images/\n\n\nhttps://stackoverflow.com/questions/48769362/jenkins-pipeline-should-is-removing-container-on-remote-daemon-after-deployment\n\n\nhttps://blog.philipphauer.de/tutorial-continuous-delivery-with-docker-jenkins/\n\n\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-continuous-integration-pipelines-in-jenkins-on-ubuntu-16-04\n\n\nhttps://www.thepolyglotdeveloper.com/2017/04/continuous-deployment-of-web-application-containers-with-jenkins-and-docker/\n\n\nhttps://docs.docker.com/registry/deploying/#run-a-local-registry\n\n\nhttps://jenkins.io/doc/book/pipeline/docker/\n\n\n\n\nSample Jenkinsfiles\n\n\nIdeal workflow is to build on the remote docker server, push to artifactory the built image, and then pull that new image to the docker server all from jenkins. \n\n\nSomething like:\n\n\n stage {\n            steps {\n                     script {\n                               docker.withServer('tcp://10.10.10.10:2375') {\n                               docker.withRegistry('https://registry.my.com/','jenkins-registry') {\n                               docker.image('registry.my.com/image-my/my:latest').run(' -p 9090:80 -i -t --name harpal ') \n\n    }\n                           }\n                   }\n            }\n\n\n\n\nSo first we would need to build the image with the server and with the registry to push it. From here we then pull the image and then rerun with server. \n\n\nFrom work: \n\n\nnode {\n\n    checkout scm\n\n\n\n    stage('Build')\n\n    {   \n\n        docker.withRegistry('artifactory', 'credentials')\n\n        {\n\n            def prdImage = docker.build(\nproject/repository\n)\n\n\n\n            prdImage.push(\nlatest\n)\n\n        }\n\n    }\n\n    stage('Test / Deploy')\n\n    {\n\n        marathon(\n\n            url: \n${env.MARATHON_ENDPOINT}\n,\n\n            forceUpdate: true,\n\n            id: '/service_name/branch',\n\n            definitionFile: 'marathon.json')\n\n\n\n                def restart_uri = \n/v2/apps/service_name/branch/restart\n\n\n                def reponse = httpRequest acceptType: 'APPLICATION_JSON', ignoreSslErrors: true, contentType: 'APPLICATION_JSON', httpMode: 'POST', requestBody: '{\nforce\n:false}', url: \n${env.MARATHON_ENDPOINT}\n + restart_uri\n\n    }\n\n}\n\n\n\nnode {\n\n    checkout scm\n\n\n\n    // Define job specific variables\n\n    def IMAGE = \nproject/repository\n\n\n    def REPOSITORY = \nrepository.example.com\n\n\n    def SERVICE_NAME = '/service/name/api'\n\n    def BUILD_TAG = \nlatest\n\n\n\n\n    def prdImage = \n\n\n\n\n    stage('Build')\n\n    {  \n\n        prdImage = docker.build(IMAGE)\n\n    }\n\n    stage('Test')\n\n    {\n\n        // Test cases needed\n\n    }\n\n    stage(\nPush\n)\n\n    {\n\n        docker.withRegistry('https://' + REPOSITORY, 'credentials')\n\n        {\n\n            prdImage.push(BUILD_TAG)\n\n        }\n\n    }\n\n    stage('Deploy App')\n\n    {\n\n        marathon(\n\n            url: \n${env.MARATHON_ENDPOINT}\n,\n\n            forceUpdate: true,\n\n            id: SERVICE_NAME,\n\n            filename: 'marathon.json',\n\n            docker: REPOSITORY + \n/\n + IMAGE + \n:\n + BUILD_TAG,\n\n            dockerForcePull: true)\n\n\n\n        try {\n\n            def restart_uri = \n/v2/apps\n + SERVICE_NAME + \n/restart\n\n\n            def response = httpRequest acceptType: 'APPLICATION_JSON', contentType: 'APPLICATION_JSON', httpMode: 'POST', ignoreSslErrors: true, requestBody: '{\nforce\n:false}' , url: \n${env.MARATHON_ENDPOINT}\n + restart_uri\n\n        }\n\n        catch (exc) {\n\n            echo 'Service restart failed, perhaps there was nothing to restart.'\n\n        }\n\n\n\n    }\n\n    stage(\nDeploy Autoscaler\n)\n\n    {\n\n        marathon(\n\n            url: \n${env.MARATHON_ENDPOINT}\n,\n\n            forceUpdate: true,\n\n            filename: 'autoscaler.json',\n\n            dockerForcePull: true)\n\n    }\n\n    stage(\nDeploy Redis\n)\n\n    {\n\n        marathon(\n\n            url: \n${env.MARATHON_ENDPOINT}\n,\n\n            forceUpdate: true,\n\n            filename: 'redis.json',\n\n            dockerForcePull: true)\n\n    }\n\n}\n\n\n\nnode {\n\n    checkout scm\n\n\n\n    // Define job specific variables\n\n    def IMAGE = \nrepo/service-name\n\n\n                def REPOSITORY = \nartifactory.example.com\n\n\n    def BUILD_TAG = \nprd_${env.BUILD_ID}\n\n\n    def SERVICE_NAME = '/service_name'\n\n\n\n    stage('Build')\n\n    {   \n\n        docker.withRegistry('https://' + REPOSITORY, 'credentials')\n\n        {\n\n            def prdImage = docker.build(IMAGE)\n\n\n\n            prdImage.push(BUILD_TAG)\n\n            prdImage.push(\nlatest\n)\n\n        }\n\n    }\n\n    stage('Test / Deploy')\n\n    {\n\n        marathon(\n\n            url: \n${env.MARATHON_ENDPOINT}\n,\n\n            forceUpdate: true,\n\n            id: SERVICE_NAME,\n\n            definitionFile: 'marathon.json',\n\n            docker: REPOSITORY + \n/\n + IMAGE + \n:\n + BUILD_TAG,\n\n            dockerForcePull: true)\n\n    }\n\n}\n\nnode {\n\n    checkout scm\n\n\n\n    stage('Build')\n\n    {   \n\n        docker.withRegistry('https://artifactory.com', 'credentials')\n\n        {\n\n            def prdImage = docker.build(\nbrach/service_name\n)\n\n\n\n            prdImage.push(\nlatest\n)\n\n        }\n\n    }\n\n                # will need to translate above to docker.buidlwithserver then with registry push then with registry pull and redeploy for my environment\n\n                # dont need marathon parts\n\n    stage('Test / Deploy')\n\n    {\n\n        marathon(\n\n            url: \n${env.MARATHON_ENDPOINT}\n,\n\n            forceUpdate: true,\n\n            id: '/kafka-to-cassandra/prd',\n\n            definitionFile: 'marathon.json')\n\n\n\n                def restart_uri = \n/v2/apps/service_name/branch/restart\n\n\n                def reponse = httpRequest acceptType: 'APPLICATION_JSON', ignoreSslErrors: true, contentType: 'APPLICATION_JSON', httpMode: 'POST', requestBody: '{\nforce\n:false}', url: \n${env.MARATHON_ENDPOINT}\n + restart_uri\n\n    }\n\n}", 
            "title": "Jenkins"
        }, 
        {
            "location": "/software/jenkins/#installation", 
            "text": "Following documentation at  https://jenkins.io/doc/book/installing/ , create a new container with 50GB storage and 1GB memory. Follow Ubuntu instructions on the documentation  wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -\nsudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/   /etc/apt/sources.list.d/jenkins.list'\nsudo apt-get update\nsudo apt-get install jenkins  Then go to  http://jenkinsip:8080  to finish the installation. Grab the Jenkins password from  /var/jenkins_home/secrets/initialAdminPassword , choose install suggested plugins, and then create first administrator user with username alex.    https://jenkins.io/doc/book/pipeline/docker/  https://go.cloudbees.com/docs/cloudbees-documentation/cje-user-guide/index.html#docker-workflow  https://getintodevops.com/blog/building-your-first-docker-image-with-jenkins-2-guide-for-developers  http://www.littlebigextra.com/using-jenkins-to-build-and-deploy-docker-images/  https://stackoverflow.com/questions/48769362/jenkins-pipeline-should-is-removing-container-on-remote-daemon-after-deployment  https://blog.philipphauer.de/tutorial-continuous-delivery-with-docker-jenkins/  https://www.digitalocean.com/community/tutorials/how-to-set-up-continuous-integration-pipelines-in-jenkins-on-ubuntu-16-04  https://www.thepolyglotdeveloper.com/2017/04/continuous-deployment-of-web-application-containers-with-jenkins-and-docker/  https://docs.docker.com/registry/deploying/#run-a-local-registry  https://jenkins.io/doc/book/pipeline/docker/", 
            "title": "Installation"
        }, 
        {
            "location": "/software/jenkins/#sample-jenkinsfiles", 
            "text": "Ideal workflow is to build on the remote docker server, push to artifactory the built image, and then pull that new image to the docker server all from jenkins.   Something like:   stage {\n            steps {\n                     script {\n                               docker.withServer('tcp://10.10.10.10:2375') {\n                               docker.withRegistry('https://registry.my.com/','jenkins-registry') {\n                               docker.image('registry.my.com/image-my/my:latest').run(' -p 9090:80 -i -t --name harpal ') \n\n    }\n                           }\n                   }\n            }  So first we would need to build the image with the server and with the registry to push it. From here we then pull the image and then rerun with server.   From work:   node {\n\n    checkout scm\n\n\n\n    stage('Build')\n\n    {   \n\n        docker.withRegistry('artifactory', 'credentials')\n\n        {\n\n            def prdImage = docker.build( project/repository )\n\n\n\n            prdImage.push( latest )\n\n        }\n\n    }\n\n    stage('Test / Deploy')\n\n    {\n\n        marathon(\n\n            url:  ${env.MARATHON_ENDPOINT} ,\n\n            forceUpdate: true,\n\n            id: '/service_name/branch',\n\n            definitionFile: 'marathon.json')\n\n\n\n                def restart_uri =  /v2/apps/service_name/branch/restart \n\n                def reponse = httpRequest acceptType: 'APPLICATION_JSON', ignoreSslErrors: true, contentType: 'APPLICATION_JSON', httpMode: 'POST', requestBody: '{ force :false}', url:  ${env.MARATHON_ENDPOINT}  + restart_uri\n\n    }\n\n}\n\n\n\nnode {\n\n    checkout scm\n\n\n\n    // Define job specific variables\n\n    def IMAGE =  project/repository \n\n    def REPOSITORY =  repository.example.com \n\n    def SERVICE_NAME = '/service/name/api'\n\n    def BUILD_TAG =  latest \n\n\n\n    def prdImage =  \n\n\n\n    stage('Build')\n\n    {  \n\n        prdImage = docker.build(IMAGE)\n\n    }\n\n    stage('Test')\n\n    {\n\n        // Test cases needed\n\n    }\n\n    stage( Push )\n\n    {\n\n        docker.withRegistry('https://' + REPOSITORY, 'credentials')\n\n        {\n\n            prdImage.push(BUILD_TAG)\n\n        }\n\n    }\n\n    stage('Deploy App')\n\n    {\n\n        marathon(\n\n            url:  ${env.MARATHON_ENDPOINT} ,\n\n            forceUpdate: true,\n\n            id: SERVICE_NAME,\n\n            filename: 'marathon.json',\n\n            docker: REPOSITORY +  /  + IMAGE +  :  + BUILD_TAG,\n\n            dockerForcePull: true)\n\n\n\n        try {\n\n            def restart_uri =  /v2/apps  + SERVICE_NAME +  /restart \n\n            def response = httpRequest acceptType: 'APPLICATION_JSON', contentType: 'APPLICATION_JSON', httpMode: 'POST', ignoreSslErrors: true, requestBody: '{ force :false}' , url:  ${env.MARATHON_ENDPOINT}  + restart_uri\n\n        }\n\n        catch (exc) {\n\n            echo 'Service restart failed, perhaps there was nothing to restart.'\n\n        }\n\n\n\n    }\n\n    stage( Deploy Autoscaler )\n\n    {\n\n        marathon(\n\n            url:  ${env.MARATHON_ENDPOINT} ,\n\n            forceUpdate: true,\n\n            filename: 'autoscaler.json',\n\n            dockerForcePull: true)\n\n    }\n\n    stage( Deploy Redis )\n\n    {\n\n        marathon(\n\n            url:  ${env.MARATHON_ENDPOINT} ,\n\n            forceUpdate: true,\n\n            filename: 'redis.json',\n\n            dockerForcePull: true)\n\n    }\n\n}\n\n\n\nnode {\n\n    checkout scm\n\n\n\n    // Define job specific variables\n\n    def IMAGE =  repo/service-name \n\n                def REPOSITORY =  artifactory.example.com \n\n    def BUILD_TAG =  prd_${env.BUILD_ID} \n\n    def SERVICE_NAME = '/service_name'\n\n\n\n    stage('Build')\n\n    {   \n\n        docker.withRegistry('https://' + REPOSITORY, 'credentials')\n\n        {\n\n            def prdImage = docker.build(IMAGE)\n\n\n\n            prdImage.push(BUILD_TAG)\n\n            prdImage.push( latest )\n\n        }\n\n    }\n\n    stage('Test / Deploy')\n\n    {\n\n        marathon(\n\n            url:  ${env.MARATHON_ENDPOINT} ,\n\n            forceUpdate: true,\n\n            id: SERVICE_NAME,\n\n            definitionFile: 'marathon.json',\n\n            docker: REPOSITORY +  /  + IMAGE +  :  + BUILD_TAG,\n\n            dockerForcePull: true)\n\n    }\n\n}\n\nnode {\n\n    checkout scm\n\n\n\n    stage('Build')\n\n    {   \n\n        docker.withRegistry('https://artifactory.com', 'credentials')\n\n        {\n\n            def prdImage = docker.build( brach/service_name )\n\n\n\n            prdImage.push( latest )\n\n        }\n\n    }\n\n                # will need to translate above to docker.buidlwithserver then with registry push then with registry pull and redeploy for my environment\n\n                # dont need marathon parts\n\n    stage('Test / Deploy')\n\n    {\n\n        marathon(\n\n            url:  ${env.MARATHON_ENDPOINT} ,\n\n            forceUpdate: true,\n\n            id: '/kafka-to-cassandra/prd',\n\n            definitionFile: 'marathon.json')\n\n\n\n                def restart_uri =  /v2/apps/service_name/branch/restart \n\n                def reponse = httpRequest acceptType: 'APPLICATION_JSON', ignoreSslErrors: true, contentType: 'APPLICATION_JSON', httpMode: 'POST', requestBody: '{ force :false}', url:  ${env.MARATHON_ENDPOINT}  + restart_uri\n\n    }\n\n}", 
            "title": "Sample Jenkinsfiles"
        }, 
        {
            "location": "/software/jupyterhub/", 
            "text": "Installation\n\n\nFollowing the documentation from \n  - https://github.com/jupyterhub/jupyterhub\n  - https://github.com/jupyterhub/jupyterhub/wiki/Installation-of-Jupyterhub-on-remote-server\n  - https://jupyterhub.readthedocs.io/en/latest/quickstart.html#installation\n\n\nadduser alex\nusermod -aG sudo alex\napt-get install python3-pip npm nodejs-legacy -y\nnpm install -g configurable-http-proxy\npip3 install jupyterhub\npip3 install --upgrade notebook\njupyterhub --no-ssl\n\n\n\n\nInstance is at \nhttp://ipaddress:8000\n.\n\n\nAdd \njupyterhub --no-ssl\n to \n/etc/rc.local\n to start on boot\n\n\nThere are further options in a configuration file that require more reading on authentication measures, etc. But this was more of a want to do then need to do, probably won't use this often with michigan having a GPU.", 
            "title": "Jupyterhub"
        }, 
        {
            "location": "/software/jupyterhub/#installation", 
            "text": "Following the documentation from \n  - https://github.com/jupyterhub/jupyterhub\n  - https://github.com/jupyterhub/jupyterhub/wiki/Installation-of-Jupyterhub-on-remote-server\n  - https://jupyterhub.readthedocs.io/en/latest/quickstart.html#installation  adduser alex\nusermod -aG sudo alex\napt-get install python3-pip npm nodejs-legacy -y\nnpm install -g configurable-http-proxy\npip3 install jupyterhub\npip3 install --upgrade notebook\njupyterhub --no-ssl  Instance is at  http://ipaddress:8000 .  Add  jupyterhub --no-ssl  to  /etc/rc.local  to start on boot  There are further options in a configuration file that require more reading on authentication measures, etc. But this was more of a want to do then need to do, probably won't use this often with michigan having a GPU.", 
            "title": "Installation"
        }, 
        {
            "location": "/software/mkdocs/", 
            "text": "Installation\n\n\nUsing pip, \npip install mkdocs\n\n\nCreate a project with \nmkdocs new my-project\n and \ncd my-project\n\n\nStart the server with \nmkdocs serve -dev-addr=0.0.0.0:80 \n or whatever port you want\n\n\nBuild the site with mkdocs build to create the \nsite\n directory\n\n\nEdit \nmkdocs.yml\n to change the theme to readthedocs, title, author, name pages differently, etc.\n\n\nAdd new \n.md\n files in separate directories as necessary for desired layout\n\n\nAdd MathJaX extensions with \nextra_javascript:\n  - \"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML\"\n in \nmkdocs.yml", 
            "title": "Mkdocs"
        }, 
        {
            "location": "/software/mkdocs/#installation", 
            "text": "Using pip,  pip install mkdocs  Create a project with  mkdocs new my-project  and  cd my-project  Start the server with  mkdocs serve -dev-addr=0.0.0.0:80   or whatever port you want  Build the site with mkdocs build to create the  site  directory  Edit  mkdocs.yml  to change the theme to readthedocs, title, author, name pages differently, etc.  Add new  .md  files in separate directories as necessary for desired layout  Add MathJaX extensions with  extra_javascript:\n  - \"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML\"  in  mkdocs.yml", 
            "title": "Installation"
        }, 
        {
            "location": "/software/nbviewer/", 
            "text": "Installation\n\n\n\n\nhttps://github.com/jupyter/nbviewer\n\n\nhttps://hub.docker.com/r/jupyter/nbviewer/~/dockerfile/\n\n\n\n\nThe repository isn't the best resource, but through trial and error I got this to work. \n\n\n\napt-get update\napt-get install -y -q \\\n    build-essential \\\n    gcc \\\n    git \\\n    libcurl4-openssl-dev \\\n    libmemcached-dev \\\n    libsqlite3-dev \\\n    libzmq3-dev \\\n    make \\\n    nodejs \\\n    nodejs-legacy \\\n    npm \\\n    pandoc \\\n    python3-dev \\\n    python3-pip \\\n    sqlite3 \\\n    zlib1g-dev \\\napt-get clean\ngit clone https://github.com/jupyter/nbviewer.git\ncd nbviewer\napt-get install python3-pip -y\npip3 install -r requirements.txt\npip3 install -r requirements-dev.txt\npip3 install markdown\npip3 install statsd\nnpm install\ninvoke bower # raises exception for UnicodeDecodeError but works all the same?\ninvoke less\npython3 -m nbviewer --debug --no-cache\n\n\n\n\nTried to manually install the npm packages (bower, less, less-plugin-autoprefix, less-plugin-clean-css, watch) using -g switch, but still UnicodeDecodeError persists. \n\n\nAdded the following \n/etc/rc.local/\n, \ncd nbviewer; python3 -m nbviewer --debug --no-cache\n to start on boot, but not able to access the console afterwards with this method, need to read more on running things as a background process here.", 
            "title": "Nbviewer"
        }, 
        {
            "location": "/software/nbviewer/#installation", 
            "text": "https://github.com/jupyter/nbviewer  https://hub.docker.com/r/jupyter/nbviewer/~/dockerfile/   The repository isn't the best resource, but through trial and error I got this to work.   \napt-get update\napt-get install -y -q \\\n    build-essential \\\n    gcc \\\n    git \\\n    libcurl4-openssl-dev \\\n    libmemcached-dev \\\n    libsqlite3-dev \\\n    libzmq3-dev \\\n    make \\\n    nodejs \\\n    nodejs-legacy \\\n    npm \\\n    pandoc \\\n    python3-dev \\\n    python3-pip \\\n    sqlite3 \\\n    zlib1g-dev \\\napt-get clean\ngit clone https://github.com/jupyter/nbviewer.git\ncd nbviewer\napt-get install python3-pip -y\npip3 install -r requirements.txt\npip3 install -r requirements-dev.txt\npip3 install markdown\npip3 install statsd\nnpm install\ninvoke bower # raises exception for UnicodeDecodeError but works all the same?\ninvoke less\npython3 -m nbviewer --debug --no-cache  Tried to manually install the npm packages (bower, less, less-plugin-autoprefix, less-plugin-clean-css, watch) using -g switch, but still UnicodeDecodeError persists.   Added the following  /etc/rc.local/ ,  cd nbviewer; python3 -m nbviewer --debug --no-cache  to start on boot, but not able to access the console afterwards with this method, need to read more on running things as a background process here.", 
            "title": "Installation"
        }, 
        {
            "location": "/software/nexus_repository/", 
            "text": "Introduction\n\n\nThis is supposed to be a free alternative to Artifactory that has Docker support and PYPI support. There are also community plugins for CRAN too. \n\n\n\n\nhttps://gist.github.com/diegopacheco/0d6dcb6771166a87d5eb\n\n\nhttps://asadbukhariblog.wordpress.com/2015/08/31/sonatype-nexus-oss-installation-on-ubuntu-14-04-lts/\n\n\nhttps://www.build-business-websites.co.uk/install-nexus-on-ubuntu-16-04/\n\n\nhttps://help.sonatype.com/repomanager3/configuration\n\n\nhttps://help.sonatype.com/repomanager3/private-registry-for-docker\n\n\nhttps://help.sonatype.com/repomanager3/pypi-repositories\n\n\n\n\nThis requires SSL certification to setup properly. Probably just going to use Docker's built in private registry on the host VM, so long as images are pulled from that same machine it should be fine. Probably requires that we set jenkins up on that server as well?", 
            "title": "Nexus repository"
        }, 
        {
            "location": "/software/nexus_repository/#introduction", 
            "text": "This is supposed to be a free alternative to Artifactory that has Docker support and PYPI support. There are also community plugins for CRAN too.    https://gist.github.com/diegopacheco/0d6dcb6771166a87d5eb  https://asadbukhariblog.wordpress.com/2015/08/31/sonatype-nexus-oss-installation-on-ubuntu-14-04-lts/  https://www.build-business-websites.co.uk/install-nexus-on-ubuntu-16-04/  https://help.sonatype.com/repomanager3/configuration  https://help.sonatype.com/repomanager3/private-registry-for-docker  https://help.sonatype.com/repomanager3/pypi-repositories   This requires SSL certification to setup properly. Probably just going to use Docker's built in private registry on the host VM, so long as images are pulled from that same machine it should be fine. Probably requires that we set jenkins up on that server as well?", 
            "title": "Introduction"
        }, 
        {
            "location": "/software/plex/", 
            "text": "Installation\n\n\n\n\nDownload the \n.deb\n from Plex\n\n\nsudo dpkg -i plexmedia...\n\n\nGo to \nhttp://containerip:32400/web\n\n\nAdd NFS media directories as necessary\n\n\nConfiguring server name and media directories directly after installation may result in 'There was a problem saving these settings' warnings, but after a few minutes will disappear.\n\n\nlook for guides on downloading subtitles", 
            "title": "Plex"
        }, 
        {
            "location": "/software/plex/#installation", 
            "text": "Download the  .deb  from Plex  sudo dpkg -i plexmedia...  Go to  http://containerip:32400/web  Add NFS media directories as necessary  Configuring server name and media directories directly after installation may result in 'There was a problem saving these settings' warnings, but after a few minutes will disappear.  look for guides on downloading subtitles", 
            "title": "Installation"
        }, 
        {
            "location": "/software/rclone/", 
            "text": "Installation\n\n\n\n\nCreate a new Ubuntu container and follow the directions from \nhttps://rclone.org/install/\n after installing \ncurl, unzip\n\n\nCopy \n.rclone.conf\n to the home directory\n\n\nAdd the appropriate NFS directories as necessary for uploading\n\n\nInstall \ntmux\n to attach sessions as needed (Not working in LXC containers for some reason)\n\n\nUpload data using \nnohup rclone copy /source/path remote:/dest/path", 
            "title": "Rclone"
        }, 
        {
            "location": "/software/rclone/#installation", 
            "text": "Create a new Ubuntu container and follow the directions from  https://rclone.org/install/  after installing  curl, unzip  Copy  .rclone.conf  to the home directory  Add the appropriate NFS directories as necessary for uploading  Install  tmux  to attach sessions as needed (Not working in LXC containers for some reason)  Upload data using  nohup rclone copy /source/path remote:/dest/path", 
            "title": "Installation"
        }, 
        {
            "location": "/software/reverse_proxy/", 
            "text": "//\n__Did not finish - not wanting to expose network to WAN__\n//\n\n\nInstallation\n\n\nFollowing \nhttps://www.htpcguides.com/secure-nginx-reverse-proxy-with-lets-encrypt-on-ubuntu-16-04-lts/\n and \nhttps://calvin.me/port-forward-web-servers-in-pfsense-2/\n and port forwarding 80, 443 in pfsense while also changing web GUI port to not expose it on WAN. \n\n\napt-get update\napt-get install nginx -y\nunlink /etc/nginx/sites-enabled/default\n\n\n\n\nAnd fill it with \n\n\nserver {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    server_name ajdurbin2.ddns.net 192.168.1.207;\n    root /var/www/html;\n    index index.html index.htm index.nginx-debian.html;\n\n    location ~ /.well-known {\n        allow all;\n    }\n}", 
            "title": "Reverse proxy"
        }, 
        {
            "location": "/software/reverse_proxy/#installation", 
            "text": "Following  https://www.htpcguides.com/secure-nginx-reverse-proxy-with-lets-encrypt-on-ubuntu-16-04-lts/  and  https://calvin.me/port-forward-web-servers-in-pfsense-2/  and port forwarding 80, 443 in pfsense while also changing web GUI port to not expose it on WAN.   apt-get update\napt-get install nginx -y\nunlink /etc/nginx/sites-enabled/default  And fill it with   server {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    server_name ajdurbin2.ddns.net 192.168.1.207;\n    root /var/www/html;\n    index index.html index.htm index.nginx-debian.html;\n\n    location ~ /.well-known {\n        allow all;\n    }\n}", 
            "title": "Installation"
        }, 
        {
            "location": "/software/rstudio_rshiny/", 
            "text": "Installation\n\n\nFollowing the excellent tutorial from \nhttps://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/#safety-first\n\n\nadduser alex\ngpasswd -a alex sudo\nsu - alex\n\n# install R\nsudo sh -c 'echo \ndeb http://cran.rstudio.com/bin/linux/ubuntu xenial/\n \n /etc/apt/sources.list'\ngpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9\ngpg -a --export E084DAB9 | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install r-base -y\n\n# common dependencies for R packages\nsudo apt-get install libcurl4-gnutls-dev libxml2-dev libssl-dev -y\n\n# install packages using this method so they're able to be installed for shiny user\nsudo su - -c \nR -e \\\ninstall.packages('devtools', repos='http://cran.rstudio.com/')\\\n\nsudo su - -c \nR -e \\\ninstall.packages('rmarkdown', repos='http://cran.rstudio.com/')\\\n\n...\n\n# install rstudio server\nsudo apt-get install gdebi-core -y\nwget https://download2.rstudio.org/rstudio-server-1.1.442-amd64.deb\nsudo gdebi rstudio-server-1.1.442-amd64.deb\n# go to http//ip:8787 for gui\n\n# install shiny server  \nsudo su - -c \nR -e \\\ninstall.packages('shiny', repos='http://cran.rstudio.com/')\\\n\nwget https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.7.907-amd64.deb\nsudo gdebi shiny-server-1.5.6.875-amd64.deb\n# check homepage at http://ip:3838\n\n# set up proper user permissions to share /srv/shiny-server/ between user and shiny user\nsudo groupadd shiny-apps\nsudo usermod -aG shiny-apps alex\nsudo usermod -aG shiny-apps shiny\ncd /srv/shiny-server\nsudo chown -R alex:shiny-apps .\nsudo chmod g+w .\nsudo chmod g+s .\n\n# make /srv/shiny-server/ a git repository for pulling apps\n# have shiny-server repo in home folder and push to repo before pulling in /srv/shiny-server/ for running\nsudo apt-get install git -y\ncd /srv/shiny-server/ \ngit init\n\n\n\n\nWe can also host interactive R Markdown documents if they're named \nindex.Rmd\n with \nruntime: shiny\n in the YAML header.", 
            "title": "Rstudio rshiny"
        }, 
        {
            "location": "/software/rstudio_rshiny/#installation", 
            "text": "Following the excellent tutorial from  https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/#safety-first  adduser alex\ngpasswd -a alex sudo\nsu - alex\n\n# install R\nsudo sh -c 'echo  deb http://cran.rstudio.com/bin/linux/ubuntu xenial/    /etc/apt/sources.list'\ngpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9\ngpg -a --export E084DAB9 | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install r-base -y\n\n# common dependencies for R packages\nsudo apt-get install libcurl4-gnutls-dev libxml2-dev libssl-dev -y\n\n# install packages using this method so they're able to be installed for shiny user\nsudo su - -c  R -e \\ install.packages('devtools', repos='http://cran.rstudio.com/')\\ \nsudo su - -c  R -e \\ install.packages('rmarkdown', repos='http://cran.rstudio.com/')\\ \n...\n\n# install rstudio server\nsudo apt-get install gdebi-core -y\nwget https://download2.rstudio.org/rstudio-server-1.1.442-amd64.deb\nsudo gdebi rstudio-server-1.1.442-amd64.deb\n# go to http//ip:8787 for gui\n\n# install shiny server  \nsudo su - -c  R -e \\ install.packages('shiny', repos='http://cran.rstudio.com/')\\ \nwget https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.7.907-amd64.deb\nsudo gdebi shiny-server-1.5.6.875-amd64.deb\n# check homepage at http://ip:3838\n\n# set up proper user permissions to share /srv/shiny-server/ between user and shiny user\nsudo groupadd shiny-apps\nsudo usermod -aG shiny-apps alex\nsudo usermod -aG shiny-apps shiny\ncd /srv/shiny-server\nsudo chown -R alex:shiny-apps .\nsudo chmod g+w .\nsudo chmod g+s .\n\n# make /srv/shiny-server/ a git repository for pulling apps\n# have shiny-server repo in home folder and push to repo before pulling in /srv/shiny-server/ for running\nsudo apt-get install git -y\ncd /srv/shiny-server/ \ngit init  We can also host interactive R Markdown documents if they're named  index.Rmd  with  runtime: shiny  in the YAML header.", 
            "title": "Installation"
        }, 
        {
            "location": "/software/torrent_server/", 
            "text": "Installation\n\n\n\n\nCreate new container using the TurnKey Torrent Server template with default HDD size and 4GB RAM, 1GB swap with software password\n\n\nStarting container and logging in as root with software password begins the TurnKey GUI setup, use software password, skip automatic backups, skip email notifications, and install security updates\n\n\nruTorrent GUI is found at \nhttps://torrentserverip:12322\n with username admin and software password\n\n\nDownload directory is \n/srv/storage/download\n\n\nCreate mountpoint to FreeNAS dataset through Proxmox NFS mount by editing \n/etc/pve/lxc/\ncontainerid\n.conf\n with \nmp0: /proxmox/nfs/mount,mp=/srv/storage/download\n\n\nRestart container\n\n\nCheck \ndu -h\n afterwards to see newly available storage\n\n\nFrom \nhttp://www.beginninglinux.com/home/data-compression/unrar-all-files-from-multiple-subdirectories-at-once\n, unrar a directory into another directory \nunrar e -r -o- /path/to/torrents/*.rar /path/to/save/\n.", 
            "title": "Torrent server"
        }, 
        {
            "location": "/software/torrent_server/#installation", 
            "text": "Create new container using the TurnKey Torrent Server template with default HDD size and 4GB RAM, 1GB swap with software password  Starting container and logging in as root with software password begins the TurnKey GUI setup, use software password, skip automatic backups, skip email notifications, and install security updates  ruTorrent GUI is found at  https://torrentserverip:12322  with username admin and software password  Download directory is  /srv/storage/download  Create mountpoint to FreeNAS dataset through Proxmox NFS mount by editing  /etc/pve/lxc/ containerid .conf  with  mp0: /proxmox/nfs/mount,mp=/srv/storage/download  Restart container  Check  du -h  afterwards to see newly available storage  From  http://www.beginninglinux.com/home/data-compression/unrar-all-files-from-multiple-subdirectories-at-once , unrar a directory into another directory  unrar e -r -o- /path/to/torrents/*.rar /path/to/save/ .", 
            "title": "Installation"
        }, 
        {
            "location": "/software/travis-ci/", 
            "text": "Travis-CI is the most popular continuous integration/continuous development service. It easily integrates with GitHub, though there doesn't really seem to be a local server image. The syntax is YAML and easier to write than Jenkins' pipeline syntax, plus we don't need to worry about having a local Jenkins server and build slave configured for Docker builds. \n\n\n\n\nhttps://docs.travis-ci.com/user/docker/\n\n\nhttps://docs.travis-ci.com/user/deployment/heroku/", 
            "title": "Travis ci"
        }, 
        {
            "location": "/software/docker/general_use/", 
            "text": "Resources\n\n\n\n\nhttps://www.digitalocean.com/community/tutorials/how-to-remove-docker-images-containers-and-volumes\n\n\nhttps://docs.docker.com/engine/reference/commandline/volume_rm/#usage\n\n\nhttps://docs.docker.com/docker-hub/repos/\n\n\nhttps://docs.docker.com/engine/reference/commandline/build/#tarball-contexts", 
            "title": "General use"
        }, 
        {
            "location": "/software/docker/general_use/#resources", 
            "text": "https://www.digitalocean.com/community/tutorials/how-to-remove-docker-images-containers-and-volumes  https://docs.docker.com/engine/reference/commandline/volume_rm/#usage  https://docs.docker.com/docker-hub/repos/  https://docs.docker.com/engine/reference/commandline/build/#tarball-contexts", 
            "title": "Resources"
        }, 
        {
            "location": "/software/docker/installation/", 
            "text": "Ubuntu Server Image\n\n\nDocker cannot run inside LXC containers, so we need to make a full virtual machine for it. Ubuntu server is minimal enough for this task.\n\n\nStart the VM with ample disk space and memory and follow installation. Skip HTTP proxy. Manual network installation. Select standard system utilities and OpenSSH server.\n\n\nDocker Installation\n\n\nhttps://docs.docker.com/install/linux/docker-ce/ubuntu/\n\n\nsudo apt-get update\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nsudo add-apt-repository \\\n   \ndeb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\n\nsudo apt-get update\nsudo apt-get install docker-ce\nsudo docker run hello-world\nsudo systemctl enable docker\n\n\n\n\nFrom \n  - http://www.littlebigextra.com/how-to-enable-remote-rest-api-on-docker-host/\n  - https://success.docker.com/article/how-do-i-enable-the-remote-api-for-dockerd\n\n\nTo enable the docker API from the server, edit \n/lib/systemd/system/docker.service\n with \nExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2376\n and then do \nsudo systemctl daemon-reload \n sudo service docker restart\n. Then test the server is accessible with \ncurl http://dockerserverip:2376/images/json\n to get a JSON string of the docker containers installed. \n\n\nTo manage Docker without \nsudo\n:\n\n\nsudo groupadd docker\nsudo usermod -aG docker $USER\nexit # then log back in\n\n\n\n\nDocker Hub\n\n\nCreate an account on Docker Hub. Create a private repository //homelab//.\n\n\nDocker Registry\n\n\n\n\nhttps://docs.docker.com/registry/deploying/#get-a-certificate\n\n\nhttps://docs.docker.com/registry/deploying/\n\n\nhttp://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/\n\n\nhttp://tech.paulcz.net/2016/01/secure-docker-with-tls/\n\n\n\n\nDocker Compose\n\n\nTool for defining and running multi-container Docker applications \nhttps://docs.docker.com/compose/\n\n\nDocker Swarm\n\n\nFor a cluster of Docker containers. Though almost everyone uses Kubernetes for this now.", 
            "title": "Installation"
        }, 
        {
            "location": "/software/docker/installation/#ubuntu-server-image", 
            "text": "Docker cannot run inside LXC containers, so we need to make a full virtual machine for it. Ubuntu server is minimal enough for this task.  Start the VM with ample disk space and memory and follow installation. Skip HTTP proxy. Manual network installation. Select standard system utilities and OpenSSH server.", 
            "title": "Ubuntu Server Image"
        }, 
        {
            "location": "/software/docker/installation/#docker-installation", 
            "text": "https://docs.docker.com/install/linux/docker-ce/ubuntu/  sudo apt-get update\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nsudo add-apt-repository \\\n    deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable \nsudo apt-get update\nsudo apt-get install docker-ce\nsudo docker run hello-world\nsudo systemctl enable docker  From \n  - http://www.littlebigextra.com/how-to-enable-remote-rest-api-on-docker-host/\n  - https://success.docker.com/article/how-do-i-enable-the-remote-api-for-dockerd  To enable the docker API from the server, edit  /lib/systemd/system/docker.service  with  ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2376  and then do  sudo systemctl daemon-reload   sudo service docker restart . Then test the server is accessible with  curl http://dockerserverip:2376/images/json  to get a JSON string of the docker containers installed.   To manage Docker without  sudo :  sudo groupadd docker\nsudo usermod -aG docker $USER\nexit # then log back in", 
            "title": "Docker Installation"
        }, 
        {
            "location": "/software/docker/installation/#docker-hub", 
            "text": "Create an account on Docker Hub. Create a private repository //homelab//.", 
            "title": "Docker Hub"
        }, 
        {
            "location": "/software/docker/installation/#docker-registry", 
            "text": "https://docs.docker.com/registry/deploying/#get-a-certificate  https://docs.docker.com/registry/deploying/  http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/  http://tech.paulcz.net/2016/01/secure-docker-with-tls/", 
            "title": "Docker Registry"
        }, 
        {
            "location": "/software/docker/installation/#docker-compose", 
            "text": "Tool for defining and running multi-container Docker applications  https://docs.docker.com/compose/", 
            "title": "Docker Compose"
        }, 
        {
            "location": "/software/docker/installation/#docker-swarm", 
            "text": "For a cluster of Docker containers. Though almost everyone uses Kubernetes for this now.", 
            "title": "Docker Swarm"
        }, 
        {
            "location": "/software/docker/jenkins/", 
            "text": "Installation\n\n\nSimilar to the bare metal version, \ndocker run -i --name jenkins -p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home --restart=always jenkins/jenkins:lts\n to pull the image and then start it at port 8080 on the Docker host. We then follow all the same instruction steps: install all the suggested packages and username alex.\n\n\n\n\nhttps://jenkins.io/doc/book/pipeline/docker/\n\n\nhttps://hub.docker.com/r/jenkins/jenkins/\n\n\nhttps://github.com/jenkinsci/docker/blob/master/README.md\n\n\n\n\nnode {\n        checkout svm\n        stage('Build')\n        {\n                docker.withRegistry('http://localhost:5000')\n                {\n                        def prdImage = docker.build('nbviewer')\n                        prdImage.push()\n                }\n        }\n        stage('Test')\n        {\n                docker.withRegitstry('http://localhost:5000')\n                {\n                        docker.image('nbviewer:latest').withRun('-p 8081:8081 --name nbviewer')\n                        {\n                                sh 'docker ps -a'\n                        }\n                }\n        }\n}\n\n\n\n\nWe also need to install a slave node for Docker too! https://piotrminkowski.wordpress.com/2017/03/13/jenkins-nodes-on-docker-containers/ \n https://www.docker.com/use-cases/cicd", 
            "title": "Jenkins"
        }, 
        {
            "location": "/software/docker/jenkins/#installation", 
            "text": "Similar to the bare metal version,  docker run -i --name jenkins -p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home --restart=always jenkins/jenkins:lts  to pull the image and then start it at port 8080 on the Docker host. We then follow all the same instruction steps: install all the suggested packages and username alex.   https://jenkins.io/doc/book/pipeline/docker/  https://hub.docker.com/r/jenkins/jenkins/  https://github.com/jenkinsci/docker/blob/master/README.md   node {\n        checkout svm\n        stage('Build')\n        {\n                docker.withRegistry('http://localhost:5000')\n                {\n                        def prdImage = docker.build('nbviewer')\n                        prdImage.push()\n                }\n        }\n        stage('Test')\n        {\n                docker.withRegitstry('http://localhost:5000')\n                {\n                        docker.image('nbviewer:latest').withRun('-p 8081:8081 --name nbviewer')\n                        {\n                                sh 'docker ps -a'\n                        }\n                }\n        }\n}  We also need to install a slave node for Docker too! https://piotrminkowski.wordpress.com/2017/03/13/jenkins-nodes-on-docker-containers/   https://www.docker.com/use-cases/cicd", 
            "title": "Installation"
        }, 
        {
            "location": "/software/docker/registry/", 
            "text": "Installation\n\n\nWithout an SSL certifcate, this registry is only accessible from the Docker host. For our purposes this is okay. https://docs.docker.com/registry/deploying/#copy-an-image-from-docker-hub-to-your-registry\n\n\ndocker run -d \\\n  -p 5000:5000 \\\n  --restart=always \\\n  --name registry \\\n  registry:2\n\ndocker pull ubuntu:16.04\ndocker tag ubuntu:16.04 localhost:5000/my-ubuntu\ndocker push localhost:5000/my-ubuntu\ndocker image remove ubuntu:16.04\ndocker image remove localhost:5000/my-ubuntu\ndocker pull localhost:5000/my-ubuntu\ndocker container stop registry\ndocker container stop registry \n docker container rm -v registry\ncurl http://localhost:5000/v2/_catalog # to see list of docker images", 
            "title": "Registry"
        }, 
        {
            "location": "/software/docker/registry/#installation", 
            "text": "Without an SSL certifcate, this registry is only accessible from the Docker host. For our purposes this is okay. https://docs.docker.com/registry/deploying/#copy-an-image-from-docker-hub-to-your-registry  docker run -d \\\n  -p 5000:5000 \\\n  --restart=always \\\n  --name registry \\\n  registry:2\n\ndocker pull ubuntu:16.04\ndocker tag ubuntu:16.04 localhost:5000/my-ubuntu\ndocker push localhost:5000/my-ubuntu\ndocker image remove ubuntu:16.04\ndocker image remove localhost:5000/my-ubuntu\ndocker pull localhost:5000/my-ubuntu\ndocker container stop registry\ndocker container stop registry   docker container rm -v registry\ncurl http://localhost:5000/v2/_catalog # to see list of docker images", 
            "title": "Installation"
        }, 
        {
            "location": "/statistics/scratch/", 
            "text": "GLM\n\n\nRegression\n\n\nExperimental Design\n\n\nSampling\n\n\nStatistics tests like paired t tests, etc\n\n\nconfidence intervals\n\n\nbootstrapping\n\n\nbayesian inference\n\n\ngibbs sampling", 
            "title": "Scratch"
        }
    ]
}