{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome!\n\n\nWhat's this and what's it for?\n\n\nThis is my personal wiki. It will contain relevant information on software installations \n configurations, statistical and machine learning techniques and descriptions, and other related topics.\n\n\nWhat's it going to look like?\n\n\nThis wiki will attempt to be organized by broad topic like installation guide, statistics, etc. Longer pages will have a table of contents. Relevant links to external sources will also be provided in the pages. Code snippets and images will also be included.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome", 
            "text": "", 
            "title": "Welcome!"
        }, 
        {
            "location": "/#whats-this-and-whats-it-for", 
            "text": "This is my personal wiki. It will contain relevant information on software installations   configurations, statistical and machine learning techniques and descriptions, and other related topics.", 
            "title": "What's this and what's it for?"
        }, 
        {
            "location": "/#whats-it-going-to-look-like", 
            "text": "This wiki will attempt to be organized by broad topic like installation guide, statistics, etc. Longer pages will have a table of contents. Relevant links to external sources will also be provided in the pages. Code snippets and images will also be included.", 
            "title": "What's it going to look like?"
        }, 
        {
            "location": "/books/", 
            "text": "textbooks\n\n\n\n\nIntroduction To Machine Learning With Python\n\n\nIntroduction To Statistical Learning\n\n\nElements Of Statistical Learning\n\n\nDeep Learning With Python\n\n\nMastering Regular Expressions\n\n\nThe Linux Command Line\n\n\nintroduction to information retrieval \nhttps://nlp.stanford.edu/IR-book/\n\n\nnltk \nhttps://www.nltk.org/book/\n\n\ngit book \nhttps://git-scm.com/book/en/v2\n\n\nstructure and interpretation of computer programs \nhttps://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-4.html\n\n\nstats done wrong \nhttps://www.statisticsdonewrong.com/\n\n\ngang of four design patterns \nhttp://www.blackwasp.co.uk/gofpatterns.aspx\n\n\nr for data science \nhttp://r4ds.had.co.nz/", 
            "title": "Books"
        }, 
        {
            "location": "/books/#textbooks", 
            "text": "Introduction To Machine Learning With Python  Introduction To Statistical Learning  Elements Of Statistical Learning  Deep Learning With Python  Mastering Regular Expressions  The Linux Command Line  introduction to information retrieval  https://nlp.stanford.edu/IR-book/  nltk  https://www.nltk.org/book/  git book  https://git-scm.com/book/en/v2  structure and interpretation of computer programs  https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-4.html  stats done wrong  https://www.statisticsdonewrong.com/  gang of four design patterns  http://www.blackwasp.co.uk/gofpatterns.aspx  r for data science  http://r4ds.had.co.nz/", 
            "title": "textbooks"
        }, 
        {
            "location": "/games/", 
            "text": "Fallout\n\n\nElder Scrolls\n\n\nDark Souls\n\n\nCivilization\n\n\nWitcher\n\n\nInsurgency/Day of Infamy\n\n\nMetro\n\n\nBastion/Transistor/(Pyre)\n\n\nHalf Life\n\n\nleft 4 dead\n\n\nportal\n\n\nmount and blade\n\n\nshovel knight\n\n\nfactorio\n\n\nbesiege\n\n\nminecraft\n\n\nkerbel space program", 
            "title": "Games"
        }, 
        {
            "location": "/goals/", 
            "text": "jrnl for personal journal with backup to version control along with pictures everyday of self.\n\n\nmkdocs for wiki instead of dokuwiki. Editing may be more annoying innitialy, but deploying to a static site is easier and current conversion script doesn't work properly. add links to wiki for my bookmarks\n\n\nsnippets repository to source control instead of in wiki\n\n\nbookmark link/read it later list, kanban task board\n\n\ndeep learning - as much as possible\n\n\ngeneral statistics - which tests to use for what problem, more general here's a dataset and do something with it, just basic here's data with no goal find something interesting, etc\n\n\nweb development with flask, using authentication, templates, forms, data tables, etc. Deployment with horizontal scaling, load balancing, redis caching, api\n\n\nkerberos authentication\n\n\ndocker: compose, dockerfiles, swarm, before moving to other services like traefik, kubernetes\n\n\nci/cd deployments using drone + local docker host/travis-ci + heroku since jenkins fucking sucks\n\n\nredhat vm for practicing unix certifications, knowledge of bashrc, exporting variables, general configuration files, not dumping everything that needs to run on boot into rc.local\n\n\nmore shiny apps\n\n\nreverse proxy for web applications not on host\n\n\ngrafana with relational + nosql + graph databases\n\n\nredo existing unix containers with better organization - like lxc on 100ids, vm on 200ids, get rid of ones not going to always use. Start with plex, calibre, torrent, git, essential services, etc. Remove practice container since being replaced with redhat vm.", 
            "title": "Goals"
        }, 
        {
            "location": "/websites/", 
            "text": "Digital Ocean Tutorials\n\n\ngithub awesome pages\n\n\nhttps://github.com/heyalexej/awesome-images\n\n\nhttps://librivox.org/\n\n\nhttp://www.gutenberg.org/\n\n\nhttps://zooqle.com/\n\n\ngoldage.org\n\n\narchive.org\n\n\nenglish bookland\n\n\nstandardebooks\n\n\naudiobookbay\n\n\nhttps://projects.raspberrypi.org/en/projects?page%5Bnumber%5D=3\n\n\nhttps://github.com/Kickball/awesome-selfhosted#task-managementto-do-lists\n\n\nadafruit\n\n\noceanofpdf\n\n\nthe eye\n\n\nhttps://rpg.rem.uz/\n\n\nhttps://www.reddit.com/r/opendirectories/top/\n\n\nhttps://www.reddit.com/r/FreeEBOOKS/top/\n\n\nlibgen\n\n\nhttps://www.w3schools.com/bootstrap/bootstrap_typography.asp\n\n\nfreecodecamp.org\n\n\ngoalkicker.com\n\n\nreddit.com/r/learnprogramming top posts lots of great resources\n\n\nhttps://www.reddit.com/r/AskReddit/comments/7x639l/what_free_software_is_so_good_you_cant_believe/\n lots of great software\n\n\ngravatar\n\n\nrealpython", 
            "title": "Sites"
        }, 
        {
            "location": "/hosts/proxmox/", 
            "text": "Installation\n\n\n\n\nGrab latest Proxmox iso and boot from USB\n\n\nChoose install\n\n\nSelect ZFS RAID 0 and choose hard drives, be sure to not include USB drive here\n\n\nChange timezone to America/Detroit\n\n\nEnter hardware password and email address\n\n\nChange FQDN to \nsuperior.localdomain\n\n\n\n\nConfiguration\n\n\n\n\nComment out \ndeb \nhttps://enterprise.proxmox.com/debian/pve\n stretch pve-enterprise\n in \n/etc/apt/sources.list.d/pve-enterprise.list\n\n\nAdd \ndeb \nhttp://download.proxmox.com/debian/pve\n stretch pve-no-subscription\n to \n/etc/apt/sources.list\n\n\napt-get update \n apt-get dist-upgrade\n\n\nAdd LXC templates at \nDatacenter -\n Superior -\n local (superior) -\n Content -\n Templates\n\n\nIf TurnKey templates are missing, do \npveam update\n\n\n\n\nMount FreeNAS NFS Shares On Boot\n\n\n\n\nMake subdirectories in \n/mnt\n named after datasets to be mounted, ie, \nmkdir /mnt/torrents\n\n\nEdit \n/etc/fstab\n with \nfreenasip:/mnt/volume/datasetname /mnt/datasetname nfs _netdev,auto 0 0\n\n\nReboot and check file systems mounted properly\n\n\n\n\n192.168.1.124:/mnt/tank/test /mnt/test nfs _netdev,auto 0 0\n192.168.1.124:/mnt/tank/torrent /mnt/torrent nfs _netdev,auto 0 0\n192.168.1.124:/mnt/tank/tv /mnt/tv nfs _netdev,auto 0 0\n192.168.1.124:/mnt/tank/movies /mnt/movies nfs _netdev,auto 0 0\n192.168.1.124:/mnt/tank/music /mnt/music nfs _netdev,auto 0 0\n\n\n\n\nContainer Bind Mounts\n\n\n\n\nEdit \n/etc/pve/lxc/\ncontainerid\n.conf\n with \nmp0: /proxmox/mount/path,mp=/container/mount/path\n\n\nRestart container and check \ndu -h\n to see available storage\n\n\nFrom \nhttps://pve.proxmox.com/wiki/Linux_Container#_bind_mount_points\n and \nhttps://www.jamescoyle.net/how-to/2019-proxmox-4-x-bind-mount-mount-storage-in-an-lxc-container\n\n\n\n\n# /etc/pve/lxc/101.conf (torrent server)\nmp0: /mnt/torrent,mp=/srv/storage/download\n# /etc/pve/lxc/102.conf (rclone)\nmp0: /mnt/torrent,mp=/mnt/torrent\nmp1: /mnt/tv,mp=/mnt/tv\nmp2: /mnt/movies,mp=/mnt/movies\n# /etc/pve/lxc/103.conf (plex)\nmp0: /mnt/tv,mp=/mnt/tv\nmp1: /mnt/movies,mp=/mnt/movies\nmp2: /mnt/music,mp=/mnt/music\n\n\n\n\nContainer SSH\n\n\nSSH isn't enabled in containers by default. For using a 'native' terminal instead of that included in the Proxmox GUI to SSH into a container, SSH into Proxmox then \nlxc-attach --name \ncontainerid\n. From \nhttps://ping.flenny.net/2016/ssh-into-a-proxmox-lxc-container/.\n\n\nLocale errors in lxc containers\n\n\nlocale-gen en_US.UTF-8\n\n\nLXC templates\n\n\nOption there to make a template from an existing container. Good idea to have sudo user alex with home folder created, vim and other packages installed, etc.\n\n\nupload iso\n\n\nGo to superior -\n content and then upload\n\n\nadd memory to container\n\n\n\n\nDatacenter -\n hostname -\n container -\n resources -\n memory -\n edit\n\n\n\n\nadd memory to vm\n\n\n\n\nDatacenter -\n hostname -\n vm -\n hardware -\n memory -\n edit\n\n\n\n\norganization\n\n\n\n\nName and number services according to product and version, eg, nginx01, gitea01,etc. Additionally separate services by vm id groups, eg, containers have id 100-199, vms 200-299\n\n\n\n\nresize disk - not tested yet\n\n\n\n\npct resize \nvmid\n \ndisk\n \nsize\n [OPTIONS]\n\n\nhttps://pve.proxmox.com/wiki/Resize_disks", 
            "title": "Proxmox"
        }, 
        {
            "location": "/hosts/proxmox/#installation", 
            "text": "Grab latest Proxmox iso and boot from USB  Choose install  Select ZFS RAID 0 and choose hard drives, be sure to not include USB drive here  Change timezone to America/Detroit  Enter hardware password and email address  Change FQDN to  superior.localdomain", 
            "title": "Installation"
        }, 
        {
            "location": "/hosts/proxmox/#configuration", 
            "text": "Comment out  deb  https://enterprise.proxmox.com/debian/pve  stretch pve-enterprise  in  /etc/apt/sources.list.d/pve-enterprise.list  Add  deb  http://download.proxmox.com/debian/pve  stretch pve-no-subscription  to  /etc/apt/sources.list  apt-get update   apt-get dist-upgrade  Add LXC templates at  Datacenter -  Superior -  local (superior) -  Content -  Templates  If TurnKey templates are missing, do  pveam update", 
            "title": "Configuration"
        }, 
        {
            "location": "/hosts/proxmox/#mount-freenas-nfs-shares-on-boot", 
            "text": "Make subdirectories in  /mnt  named after datasets to be mounted, ie,  mkdir /mnt/torrents  Edit  /etc/fstab  with  freenasip:/mnt/volume/datasetname /mnt/datasetname nfs _netdev,auto 0 0  Reboot and check file systems mounted properly   192.168.1.124:/mnt/tank/test /mnt/test nfs _netdev,auto 0 0\n192.168.1.124:/mnt/tank/torrent /mnt/torrent nfs _netdev,auto 0 0\n192.168.1.124:/mnt/tank/tv /mnt/tv nfs _netdev,auto 0 0\n192.168.1.124:/mnt/tank/movies /mnt/movies nfs _netdev,auto 0 0\n192.168.1.124:/mnt/tank/music /mnt/music nfs _netdev,auto 0 0", 
            "title": "Mount FreeNAS NFS Shares On Boot"
        }, 
        {
            "location": "/hosts/proxmox/#container-bind-mounts", 
            "text": "Edit  /etc/pve/lxc/ containerid .conf  with  mp0: /proxmox/mount/path,mp=/container/mount/path  Restart container and check  du -h  to see available storage  From  https://pve.proxmox.com/wiki/Linux_Container#_bind_mount_points  and  https://www.jamescoyle.net/how-to/2019-proxmox-4-x-bind-mount-mount-storage-in-an-lxc-container   # /etc/pve/lxc/101.conf (torrent server)\nmp0: /mnt/torrent,mp=/srv/storage/download\n# /etc/pve/lxc/102.conf (rclone)\nmp0: /mnt/torrent,mp=/mnt/torrent\nmp1: /mnt/tv,mp=/mnt/tv\nmp2: /mnt/movies,mp=/mnt/movies\n# /etc/pve/lxc/103.conf (plex)\nmp0: /mnt/tv,mp=/mnt/tv\nmp1: /mnt/movies,mp=/mnt/movies\nmp2: /mnt/music,mp=/mnt/music", 
            "title": "Container Bind Mounts"
        }, 
        {
            "location": "/hosts/proxmox/#container-ssh", 
            "text": "SSH isn't enabled in containers by default. For using a 'native' terminal instead of that included in the Proxmox GUI to SSH into a container, SSH into Proxmox then  lxc-attach --name  containerid . From  https://ping.flenny.net/2016/ssh-into-a-proxmox-lxc-container/.", 
            "title": "Container SSH"
        }, 
        {
            "location": "/hosts/proxmox/#locale-errors-in-lxc-containers", 
            "text": "locale-gen en_US.UTF-8", 
            "title": "Locale errors in lxc containers"
        }, 
        {
            "location": "/hosts/proxmox/#lxc-templates", 
            "text": "Option there to make a template from an existing container. Good idea to have sudo user alex with home folder created, vim and other packages installed, etc.", 
            "title": "LXC templates"
        }, 
        {
            "location": "/hosts/proxmox/#upload-iso", 
            "text": "Go to superior -  content and then upload", 
            "title": "upload iso"
        }, 
        {
            "location": "/hosts/proxmox/#add-memory-to-container", 
            "text": "Datacenter -  hostname -  container -  resources -  memory -  edit", 
            "title": "add memory to container"
        }, 
        {
            "location": "/hosts/proxmox/#add-memory-to-vm", 
            "text": "Datacenter -  hostname -  vm -  hardware -  memory -  edit", 
            "title": "add memory to vm"
        }, 
        {
            "location": "/hosts/proxmox/#organization", 
            "text": "Name and number services according to product and version, eg, nginx01, gitea01,etc. Additionally separate services by vm id groups, eg, containers have id 100-199, vms 200-299", 
            "title": "organization"
        }, 
        {
            "location": "/hosts/proxmox/#resize-disk-not-tested-yet", 
            "text": "pct resize  vmid   disk   size  [OPTIONS]  https://pve.proxmox.com/wiki/Resize_disks", 
            "title": "resize disk - not tested yet"
        }, 
        {
            "location": "/hosts/freenas/", 
            "text": "Installation\n\n\n\n\nRequires two USB drives, for installation image and installation target\n\n\nChoose FreeNAS installer, select USB drive for installation target\n\n\nFresh install, format boot device\n\n\nHardware password\n\n\nBIOS booting\n\n\nReboot and remove installation media\n\n\nDisplays menu with IP for web GUI\n\n\nOpen web GUI, sign in with root, hardware password\n\n\nExit initial wizard\n\n\nChange hostname to \nhuron.localdomain\n, timezone to America/Detroit\n\n\nStorage -\n Volumes -\n Volume Manager\n to create volume using all disks and ZFS RAID 0 or ZFS Striped\n\n\nCreate datasets, change user and group permissions to nobody, add write privileges to user/owner/group\n\n\nGo to Sharing, select dataset, NSF share, change MapAll user and MapAll groups to nobody so it doesn't matter what user connects to the shares\n\n\nTo mount in Unix, \nmount -t nfs freenasip:/mnt/volumenname/datasetname /path/to/mount\n\n\nThen unmount with \numount /path/to/mount\n\n\n\n\nPlex Plugin\n\n\n\n\nInstall the Plex plugin\n\n\nGo to Jails -\n plexmediaserver -\n Add storage and add the necessary directories, be sure to choose 'create directory'\n\n\nGo to Plugins -\n Installed -\n plexmediaserver and turn on\n\n\nThen go to \nhttp://plexpluginip:32400/web/\n to access server\n\n\nNote that first install attempt did not initiate a plex 'server' only a plex client and reinstallation was necessary\n\n\n\n\nResilio Sync Plugin\n\n\n\n\nInstall BTsync plugin\n\n\nCreate new dataset with write permissions for group, other with nobody:nobody the owner:group users\n\n\nAdd the new dataset under \n/mnt\n\n\nOpen the GUI, username alex with software password\n\n\nAdd a read-only key from another host\n\n\n\n\nExisting Directories\n\n\n\n\nbooks: for Calibre server ebooks\n\n\njails: automatically created after using plugins like plex, btsync\n\n\nmovies: plex\n\n\ntv: plex\n\n\nsync: created own btsync dataset in case plugin stops working\n\n\ntest\n\n\ntorrent", 
            "title": "FreeNAS"
        }, 
        {
            "location": "/hosts/freenas/#installation", 
            "text": "Requires two USB drives, for installation image and installation target  Choose FreeNAS installer, select USB drive for installation target  Fresh install, format boot device  Hardware password  BIOS booting  Reboot and remove installation media  Displays menu with IP for web GUI  Open web GUI, sign in with root, hardware password  Exit initial wizard  Change hostname to  huron.localdomain , timezone to America/Detroit  Storage -  Volumes -  Volume Manager  to create volume using all disks and ZFS RAID 0 or ZFS Striped  Create datasets, change user and group permissions to nobody, add write privileges to user/owner/group  Go to Sharing, select dataset, NSF share, change MapAll user and MapAll groups to nobody so it doesn't matter what user connects to the shares  To mount in Unix,  mount -t nfs freenasip:/mnt/volumenname/datasetname /path/to/mount  Then unmount with  umount /path/to/mount", 
            "title": "Installation"
        }, 
        {
            "location": "/hosts/freenas/#plex-plugin", 
            "text": "Install the Plex plugin  Go to Jails -  plexmediaserver -  Add storage and add the necessary directories, be sure to choose 'create directory'  Go to Plugins -  Installed -  plexmediaserver and turn on  Then go to  http://plexpluginip:32400/web/  to access server  Note that first install attempt did not initiate a plex 'server' only a plex client and reinstallation was necessary", 
            "title": "Plex Plugin"
        }, 
        {
            "location": "/hosts/freenas/#resilio-sync-plugin", 
            "text": "Install BTsync plugin  Create new dataset with write permissions for group, other with nobody:nobody the owner:group users  Add the new dataset under  /mnt  Open the GUI, username alex with software password  Add a read-only key from another host", 
            "title": "Resilio Sync Plugin"
        }, 
        {
            "location": "/hosts/freenas/#existing-directories", 
            "text": "books: for Calibre server ebooks  jails: automatically created after using plugins like plex, btsync  movies: plex  tv: plex  sync: created own btsync dataset in case plugin stops working  test  torrent", 
            "title": "Existing Directories"
        }, 
        {
            "location": "/hosts/pfsense/", 
            "text": "Installation\n\n\n\n\nChoose 64-bit memstick version\n\n\nusername admin, hardware password\n\n\nhostname: \nerie\n, domain: \nlocaldomain\n\n\nAdd \nOPT1, OPT2\n interfaces and copy firewall rules\n\n\nCopy \nLAN\n interface configuration to \nOPT\n interfaces\n\n\nChange DNS servers to Google, OpenDNS\n\n\nChange DHCP server available range\n\n\nDo not allow DNS server list to be overridden\n\n\n\n\nPFBlockerNG\n\n\n\n\nFirewall -\n PFBlockerNG\n\n\nControl/Command click all outbound firewall rules except for lan and save\n\n\nGo to \nDNSBL\n\n\nAdd new \nDNSBL\n feeds, add DNS group name, eg pi-hole, description uses default pi-hole lists, add each pi-hole list to \nDNSBL\n and add 'test' for header/description\n\n\nList actio switched to unbound and update every hour\n\n\nSave, update, force update and watch lists download\n\n\nEnable and save\n\n\nDo same with EasyList \nDNSBL\n, check all, label as test and force update again\n\n\n\n\nUse hostnames for SSH\n\n\nFrom \nhttps://www.bytesizedalex.com/pfsense-dns-resolution-for-dhcp-leases/.\n Go to DNS Resolver and check Register DHCP leases and Register DHCP static mappings.", 
            "title": "pfSense"
        }, 
        {
            "location": "/hosts/pfsense/#installation", 
            "text": "Choose 64-bit memstick version  username admin, hardware password  hostname:  erie , domain:  localdomain  Add  OPT1, OPT2  interfaces and copy firewall rules  Copy  LAN  interface configuration to  OPT  interfaces  Change DNS servers to Google, OpenDNS  Change DHCP server available range  Do not allow DNS server list to be overridden", 
            "title": "Installation"
        }, 
        {
            "location": "/hosts/pfsense/#pfblockerng", 
            "text": "Firewall -  PFBlockerNG  Control/Command click all outbound firewall rules except for lan and save  Go to  DNSBL  Add new  DNSBL  feeds, add DNS group name, eg pi-hole, description uses default pi-hole lists, add each pi-hole list to  DNSBL  and add 'test' for header/description  List actio switched to unbound and update every hour  Save, update, force update and watch lists download  Enable and save  Do same with EasyList  DNSBL , check all, label as test and force update again", 
            "title": "PFBlockerNG"
        }, 
        {
            "location": "/hosts/pfsense/#use-hostnames-for-ssh", 
            "text": "From  https://www.bytesizedalex.com/pfsense-dns-resolution-for-dhcp-leases/.  Go to DNS Resolver and check Register DHCP leases and Register DHCP static mappings.", 
            "title": "Use hostnames for SSH"
        }, 
        {
            "location": "/hosts/raspberry_pi/", 
            "text": "Installation\n\n\n\n\nInstall latest Raspbian Lite image\n\n\nsudo raspi-config\n to change username and enable ssh\n\n\n\n\nDokuWiki Configuration\n\n\n\n\nsudo apt-get update \n apt-get install vim -y\n\n\nsudo apt-get install dokuwiki -y\n. Will need to enter admin password and purge pages on removal.\n\n\nsudo dpkg-reconfigure dokuwiki\n. Here will be asked which webserer to use, document root address, local network or global network, purge packages on removal, make configuration/plugin directories web writeable, wiki title, license, and acl. Found at \nhttp://techblog.danielpellarini.com/sysadmin/how-to-quickly-install-dokuwiki-on-debian/.\n\n\nAfterwards edit \napache.conf\n with \nAllow from all\n\n\nRestart Raspberry Pi\n\n\nGo to \nhttp://raspberrypiip/dokuwiki/\n and install MathJax plugin\n\n\n\n\nDokuWiki Backup\n\n\nCreate \nwikibackup.sh\n with the following contents:\n\n\n#!/bin/bash\n# add namespaces as created\nsudo sh -c \ncp -r /var/lib/dokuwiki/data/pages/*.txt /home/pi/wiki/\n\nsudo sh -c \ncp -r /var/lib/dokuwiki/data/pages/installation /home/pi/wiki/\n\ncd /home/pi/wiki\ngit add .\ngit commit -m 'wiki commit'\ngit push origin master\n\n\n\n\nThen SSH into Raspberry Pi and backup with \n/home/pi/wiki/wikibackup.sh\n. Making this a daily cron job is difficult because of the \nsudo sh -c\n commands. Will continue to look for solutions for auto-commiting wiki updates.", 
            "title": "Raspberry Pi"
        }, 
        {
            "location": "/hosts/raspberry_pi/#installation", 
            "text": "Install latest Raspbian Lite image  sudo raspi-config  to change username and enable ssh", 
            "title": "Installation"
        }, 
        {
            "location": "/hosts/raspberry_pi/#dokuwiki-configuration", 
            "text": "sudo apt-get update   apt-get install vim -y  sudo apt-get install dokuwiki -y . Will need to enter admin password and purge pages on removal.  sudo dpkg-reconfigure dokuwiki . Here will be asked which webserer to use, document root address, local network or global network, purge packages on removal, make configuration/plugin directories web writeable, wiki title, license, and acl. Found at  http://techblog.danielpellarini.com/sysadmin/how-to-quickly-install-dokuwiki-on-debian/.  Afterwards edit  apache.conf  with  Allow from all  Restart Raspberry Pi  Go to  http://raspberrypiip/dokuwiki/  and install MathJax plugin", 
            "title": "DokuWiki Configuration"
        }, 
        {
            "location": "/hosts/raspberry_pi/#dokuwiki-backup", 
            "text": "Create  wikibackup.sh  with the following contents:  #!/bin/bash\n# add namespaces as created\nsudo sh -c  cp -r /var/lib/dokuwiki/data/pages/*.txt /home/pi/wiki/ \nsudo sh -c  cp -r /var/lib/dokuwiki/data/pages/installation /home/pi/wiki/ \ncd /home/pi/wiki\ngit add .\ngit commit -m 'wiki commit'\ngit push origin master  Then SSH into Raspberry Pi and backup with  /home/pi/wiki/wikibackup.sh . Making this a daily cron job is difficult because of the  sudo sh -c  commands. Will continue to look for solutions for auto-commiting wiki updates.", 
            "title": "DokuWiki Backup"
        }, 
        {
            "location": "/hosts/ubuntu/", 
            "text": "Installation\n\n\n\n\nDownload latest Ubuntu LTS image and burn to USB\n\n\nBoot PC into BIOS and under Boot Options choose UEFI: USB_NAME to boot Ubuntu live image, then choose try Ubuntu without installing\n\n\nOpen GParted and ignore the Libparted warning about physical block size\n\n\nCarefully choose /dev/sd* corresponding to the SSD boot drive\n\n\nChoose the linux-swap partition and right-click and select Swapoff, this will redo the Libparted warning about physical block size after rescanning the partitions\n\n\nRight-click and delete both partitions separately, then hit the green check mark in GUI to apply these pending operations\n\n\nIt deletes partitions and again raises the Libparted warning\n\n\nExit GParted\n\n\nDouble click on install ubuntu icon\n\n\nEnglish -\n Continue\n\n\nDownload updates or proprietary drivers during installation (not doing so gives broken screen on login)\n\n\nChoose install ubuntu alongside windows boot manager\n\n\nInstall now, continue to write changes to disk\n\n\nSet Detroit, MI\n\n\nGive name, pc name, username, hardware password, require password to login\n\n\nRestart when completed, it will prompt for removal of installation media before rebooting\n\n\nWindows 10 is accidentally prioritized on boot, so need to boot back into BIOS and choose the GRUB partition as highest priority.\n\n\nWill possibly get a black or broken screen on restarting\n\n\nPer \nhttps://askubuntu.com/questions/760934/graphics-issues-after-while-installing-ubuntu-16-04-16-10-with-nvidia-graphics\n follow option 3\n\n\nReboot into GRUB, highlight Ubuntu option and press \ne\n\n\nAdd \nnouveau.modeset=0\n to the end of the line beginning with \nlinux\n\n\nPress \nF10\n to boot\n\n\nIf still black or broken screen after rebooting\n\n\nCtrl Alt F1\n opens a new console\n\n\nsudo apt-get purge nvidia-*\n\n\nsudo add-apt-repository ppa:graphics-drivers/ppa\n\n\nsudo apt-get update\n\n\nsudo apt-get install nvidia-384\n (or the latest Nvidia driver)\n\n\nReboot and graphics issues should be solved\n\n\nFix miscellaneous settings\n\n\nInstall Google Chrome \n.deb\n with \nsudo dpkg -i google-chrome...\n with an intermediate install fix with \nsudo apt-get install -f\n after first failing\n\n\nInstall Vim, Git, KeePass, HTop, Tmux with \nsudo apt-get install -y vim htop tmux keepass2 git\n\n\nSound may not work immediately, open All Settings -\n Sound and 'Test Sound' usually fixes this\n\n\nChoosing Detroit as location at installation results in package updates and installation from Ubuntu's Cananda mirrors, going to Software \n Updates and choosing 'Download from: Main server' fixes this\n\n\n\n\nCUDA 9.0 Installation\n\n\n\n\nDownload Cuda 9.0 network \n.deb\n file\n\n\nFollow installation guide at \nhttp://developer.download.nvidia.com/compute/cuda/9.0/Prod/docs/sidebar/CUDA_Installation_Guide_Linux.pdf\n\n\nVerify CUDA-capable GPU with \nlspci | grep -i nvidia\n\n\nVerify support Linux version with \nuname -m \n cat /etc/*release\n\n\nVerify gcc is installed \ngcc --version\n\n\nVerify correct kernel headers and development packages with \nuname -r\n then do \nsudo apt-get install linux-headers-$(uname -r)\n\n\nInstall repository meda-data with \nsudo dpkg -i cuda-repo \ndistro\n_\nversion\n_\narchitecture\n.deb\n\n\nInstall CUDA public GPG key with `sudo apt-key adv --fetch-keys\n\nhttp://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\n\n\nsudo apt-get update \n sudo apt-get install cuda-9-0\n (or whatever version TensorFlow requires)\n\n\ncuda-command-line-tools\n will be installed\n\n\nOlder \nnvidia- libcuda1 nvidia-opencl-icd\n will be removed and replaced with newer versions\n\n\nAdd path to PATH variable \nexport PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\n\n\nSince we used \n.deb\n installation, the following doesn't modify the path, but to be safe do \nexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\\\n${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\n\nInstall writable samples with \ncuda-install-samples-9.0.sh \ndir\n, change into the directory, and \nmake\n\n\nCompiling will take a file and some warnings or errors will be present, most are the result of the wrong CUDA version or environment variable in the code\n\n\nThe old \nnvidia-\n graphics driver will still be present, so running \ndeviceQuery\n will fail, restarting will show the newly installed \nnvidia-\n driver and \ndeviceQuery\n will pass\n\n\nAdd the PATH export statemnts to \n.bashrc\n\n\nSkip the Nsight plugin step\n\n\nThird-party libraries should already be installed\n\n\nSkip cuda-gdb installation\n\n\nIn the 9.0 guide, ther Persistence Daemon installation is not included, but is in version 9.1, if necessary add \n/usr/bin/nvidia-persistenced --verbose\n\n\n\n\ncuDNN 7.0 Installation\n\n\n\n\nDownload all \n.deb\n files, ie, runtime, developer, and example files\n\n\nDo \nsudo dpkg-i libcudnn7...\n for each starting with the runtime library, then developer library, then documentation and examples\n\n\nVerify installation with example\n\n\ncp -r /usr/src/cudnn_samples_v7/ $HOME\n\n\ncd $HOME/cudnn_samples_v7/mnistCUDNN\n\n\nmake clean \n make\n\n\n./mnistCUDNN\n\n\nIf properly running, should see a \"Test passed!\" or similar message\n\n\nAdd \nexport CUDA_HOME=/usr/local/cuda\n to \n.bashrc\n\n\n\n\nTensorFlow With GPU Support Installation\n\n\n\n\nFollow \nhttps://www.tensorflow.org/install/install_linux\n\n\ncuda-command-line-tools\n were already installed, so need only add its path to environment, \nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/extras/CUPTI/lib64\n in \n.bashrc\n\n\nUse \"Virtualenv\" installation mechanism\n\n\nInstall pip, virtualenv, and python-dev with \nsudo apt-get install python3-pip python3-dev python-virtualenv\n\n\nCreate a virtual environment \nvirtualenv --system-site-packages -p python3 targetdirectory\n\n\nActivate the environement \nsource ~/tensorflow/bin/activate\n\n\nInstall with \npip3 install --upgrade tensorflow-gpu\n\n\nValidate the installation with ```import tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\nprint(sess.run(hello))\n\n\n\n\n- Deactivate environment `deactivate`\n- To uninstall TensorFlow, `rm -r targetdirectory`\n\n# HDF5 #\nFor Keras features such as model saving to disk and using pre-trained models, this file system is necessary. Install it with `sudo apt-get install libhdf5-serial-dev`, which should install all the different HDF5 packages necessary. Then do `pip install hd5py` in your virtual environment. A warning will appear when using TensorFlow and models can be saved, however doing `hd5py.run_tests()` will fail. So this appears to be a non-issue for our uses.\n\n# cv2 #\nAlso needed for deep learning is the `OpenCV` package. In source directory install with `pip install opencv-python`.\n\n\n# R installation #\n* follow \nhttps://mran.microsoft.com/documents/rro/installation\n\n* download the gzip from Microsoft website\n* `tar -xf microsoft-r-open-3.4.4.tar.gz`\n* `sudo microsft-r-open/install.sh` and follow installation prompts, be sure to say yes to the MKL libraries\n* Download the Rstudio .deb file and install with `sudo dpkg -i ...`, there will be an intermediate install step before working the second time\n\n# Docker installation #\n\nFollow \nhttps://docs.docker.com/install/linux/docker-ce/ubuntu/\n\n\n\n\n\nsudo apt-get update\nsudo apt-get install \\\n\n \\\nca-certificates \\\ncurl \\\nsoftware-properties-common\ncurl -fsSL \nhttps://download.docker.com/linux/ubuntu/gpg\n | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nsudo add-apt-repository \\\n\"deb [arch=amd64] \nhttps://download.docker.com/linux/ubuntu\n \\\n$(lsb_release -cs) \\\nstable\"\nsudo apt-get update\nsudo apt-get install docker-ce\nsudo groupadd docker\nsudo usermod -aG docker $USER\nsudo systemctl enable docker\nsudo service docker start\n\n\nlogout/login\n\n\ndocker run hello-world\n\n\n\n# Heroku Installation #\n`curl \nhttps://cli-assets.heroku.com/install-ubuntu.sh\n | sh` and type password when prompted\n\n# Fix dual boot time difference #\nAn annoying thing when dual booting with Windows 10 is that after using Ubuntu and then booting Windows, the time will be off. Easiest solution is to set Ubuntu to use local time and rely on Windows for UTC.\n\n\n\n\ntimedatectl set-local-rtc 0 --adjust-system-clock # turn utc off\ntimedatectl set-local-rtc 1 --adjust-system-clock # turn utc on\n```\nFrom \nhttps://www.howtogeek.com/323390/how-to-fix-windows-and-linux-showing-different-times-when-dual-booting/", 
            "title": "Ubuntu"
        }, 
        {
            "location": "/hosts/ubuntu/#installation", 
            "text": "Download latest Ubuntu LTS image and burn to USB  Boot PC into BIOS and under Boot Options choose UEFI: USB_NAME to boot Ubuntu live image, then choose try Ubuntu without installing  Open GParted and ignore the Libparted warning about physical block size  Carefully choose /dev/sd* corresponding to the SSD boot drive  Choose the linux-swap partition and right-click and select Swapoff, this will redo the Libparted warning about physical block size after rescanning the partitions  Right-click and delete both partitions separately, then hit the green check mark in GUI to apply these pending operations  It deletes partitions and again raises the Libparted warning  Exit GParted  Double click on install ubuntu icon  English -  Continue  Download updates or proprietary drivers during installation (not doing so gives broken screen on login)  Choose install ubuntu alongside windows boot manager  Install now, continue to write changes to disk  Set Detroit, MI  Give name, pc name, username, hardware password, require password to login  Restart when completed, it will prompt for removal of installation media before rebooting  Windows 10 is accidentally prioritized on boot, so need to boot back into BIOS and choose the GRUB partition as highest priority.  Will possibly get a black or broken screen on restarting  Per  https://askubuntu.com/questions/760934/graphics-issues-after-while-installing-ubuntu-16-04-16-10-with-nvidia-graphics  follow option 3  Reboot into GRUB, highlight Ubuntu option and press  e  Add  nouveau.modeset=0  to the end of the line beginning with  linux  Press  F10  to boot  If still black or broken screen after rebooting  Ctrl Alt F1  opens a new console  sudo apt-get purge nvidia-*  sudo add-apt-repository ppa:graphics-drivers/ppa  sudo apt-get update  sudo apt-get install nvidia-384  (or the latest Nvidia driver)  Reboot and graphics issues should be solved  Fix miscellaneous settings  Install Google Chrome  .deb  with  sudo dpkg -i google-chrome...  with an intermediate install fix with  sudo apt-get install -f  after first failing  Install Vim, Git, KeePass, HTop, Tmux with  sudo apt-get install -y vim htop tmux keepass2 git  Sound may not work immediately, open All Settings -  Sound and 'Test Sound' usually fixes this  Choosing Detroit as location at installation results in package updates and installation from Ubuntu's Cananda mirrors, going to Software   Updates and choosing 'Download from: Main server' fixes this", 
            "title": "Installation"
        }, 
        {
            "location": "/hosts/ubuntu/#cuda-90-installation", 
            "text": "Download Cuda 9.0 network  .deb  file  Follow installation guide at  http://developer.download.nvidia.com/compute/cuda/9.0/Prod/docs/sidebar/CUDA_Installation_Guide_Linux.pdf  Verify CUDA-capable GPU with  lspci | grep -i nvidia  Verify support Linux version with  uname -m   cat /etc/*release  Verify gcc is installed  gcc --version  Verify correct kernel headers and development packages with  uname -r  then do  sudo apt-get install linux-headers-$(uname -r)  Install repository meda-data with  sudo dpkg -i cuda-repo  distro _ version _ architecture .deb  Install CUDA public GPG key with `sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub  sudo apt-get update   sudo apt-get install cuda-9-0  (or whatever version TensorFlow requires)  cuda-command-line-tools  will be installed  Older  nvidia- libcuda1 nvidia-opencl-icd  will be removed and replaced with newer versions  Add path to PATH variable  export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}  Since we used  .deb  installation, the following doesn't modify the path, but to be safe do  export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\\\n${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}  Install writable samples with  cuda-install-samples-9.0.sh  dir , change into the directory, and  make  Compiling will take a file and some warnings or errors will be present, most are the result of the wrong CUDA version or environment variable in the code  The old  nvidia-  graphics driver will still be present, so running  deviceQuery  will fail, restarting will show the newly installed  nvidia-  driver and  deviceQuery  will pass  Add the PATH export statemnts to  .bashrc  Skip the Nsight plugin step  Third-party libraries should already be installed  Skip cuda-gdb installation  In the 9.0 guide, ther Persistence Daemon installation is not included, but is in version 9.1, if necessary add  /usr/bin/nvidia-persistenced --verbose", 
            "title": "CUDA 9.0 Installation"
        }, 
        {
            "location": "/hosts/ubuntu/#cudnn-70-installation", 
            "text": "Download all  .deb  files, ie, runtime, developer, and example files  Do  sudo dpkg-i libcudnn7...  for each starting with the runtime library, then developer library, then documentation and examples  Verify installation with example  cp -r /usr/src/cudnn_samples_v7/ $HOME  cd $HOME/cudnn_samples_v7/mnistCUDNN  make clean   make  ./mnistCUDNN  If properly running, should see a \"Test passed!\" or similar message  Add  export CUDA_HOME=/usr/local/cuda  to  .bashrc", 
            "title": "cuDNN 7.0 Installation"
        }, 
        {
            "location": "/hosts/ubuntu/#tensorflow-with-gpu-support-installation", 
            "text": "Follow  https://www.tensorflow.org/install/install_linux  cuda-command-line-tools  were already installed, so need only add its path to environment,  export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/extras/CUPTI/lib64  in  .bashrc  Use \"Virtualenv\" installation mechanism  Install pip, virtualenv, and python-dev with  sudo apt-get install python3-pip python3-dev python-virtualenv  Create a virtual environment  virtualenv --system-site-packages -p python3 targetdirectory  Activate the environement  source ~/tensorflow/bin/activate  Install with  pip3 install --upgrade tensorflow-gpu  Validate the installation with ```import tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\nprint(sess.run(hello))   - Deactivate environment `deactivate`\n- To uninstall TensorFlow, `rm -r targetdirectory`\n\n# HDF5 #\nFor Keras features such as model saving to disk and using pre-trained models, this file system is necessary. Install it with `sudo apt-get install libhdf5-serial-dev`, which should install all the different HDF5 packages necessary. Then do `pip install hd5py` in your virtual environment. A warning will appear when using TensorFlow and models can be saved, however doing `hd5py.run_tests()` will fail. So this appears to be a non-issue for our uses.\n\n# cv2 #\nAlso needed for deep learning is the `OpenCV` package. In source directory install with `pip install opencv-python`.\n\n\n# R installation #\n* follow  https://mran.microsoft.com/documents/rro/installation \n* download the gzip from Microsoft website\n* `tar -xf microsoft-r-open-3.4.4.tar.gz`\n* `sudo microsft-r-open/install.sh` and follow installation prompts, be sure to say yes to the MKL libraries\n* Download the Rstudio .deb file and install with `sudo dpkg -i ...`, there will be an intermediate install step before working the second time\n\n# Docker installation #\n\nFollow  https://docs.docker.com/install/linux/docker-ce/ubuntu/   sudo apt-get update\nsudo apt-get install \\  \\\nca-certificates \\\ncurl \\\nsoftware-properties-common\ncurl -fsSL  https://download.docker.com/linux/ubuntu/gpg  | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nsudo add-apt-repository \\\n\"deb [arch=amd64]  https://download.docker.com/linux/ubuntu  \\\n$(lsb_release -cs) \\\nstable\"\nsudo apt-get update\nsudo apt-get install docker-ce\nsudo groupadd docker\nsudo usermod -aG docker $USER\nsudo systemctl enable docker\nsudo service docker start", 
            "title": "TensorFlow With GPU Support Installation"
        }, 
        {
            "location": "/hosts/ubuntu/#logoutlogin", 
            "text": "docker run hello-world  \n# Heroku Installation #\n`curl  https://cli-assets.heroku.com/install-ubuntu.sh  | sh` and type password when prompted\n\n# Fix dual boot time difference #\nAn annoying thing when dual booting with Windows 10 is that after using Ubuntu and then booting Windows, the time will be off. Easiest solution is to set Ubuntu to use local time and rely on Windows for UTC.  timedatectl set-local-rtc 0 --adjust-system-clock # turn utc off\ntimedatectl set-local-rtc 1 --adjust-system-clock # turn utc on\n```\nFrom  https://www.howtogeek.com/323390/how-to-fix-windows-and-linux-showing-different-times-when-dual-booting/", 
            "title": "logout/login"
        }, 
        {
            "location": "/hosts/windows_10/", 
            "text": "Installation\n\n\n\n\nubuntu live usb and use gparted to remove partitions and format as ntfs\n\n\nboot windows 10 usb and select education edition - full name and select basic security question\n\n\nreconnect hard drives and reboot, if extra drives formatted as ntsf, they\n\n\nshould automatically be added as the remaining drive letters, D,E,F,G, etc\n\n\ngo to disk management and name them if necessary\n\n\ngo to settings - notifications \n actions, turn off notifications frm apps and other senders\n\n\nsettings - storage: change where new content is saved and move everything to\n\n\nother drive (pictures, documents, etc)\n\n\nsettings - remote desktop enabled\n\n\nsettings - shared experiences - disable share across devices\n\n\nsettings - printers \n scanners: disable windows manage default printer\n\n\nsettings - typing: turn off everything basically, same with pen \n windows ink, autoplay\n\n\nsettings - personalization: turn everything off basically unpin all junk from taskbar\n\n\nsettings - personalization - taskbar: turn off contacts settings\n\n\nsettings - privacy - basically turn everything off\n\n\nsettings - cortanna: basically turn everything off\n\n\nsettings - gaming: turn everything off\n\n\ndownloads: chrome, keepass, btsync, notepad++, steam, gog, flux, rclone, geforce experience, google drive\n\n\nubuntu dual boot: go to diskmgmt.msc and select C: partition and shrink volume, then boot ubuntu and install, should only select the C: drive and give no options unlike before. followed guide from: \nhttps://www.tecmint.com/install-ubuntu-16-04-alongside-with-windows-10-or-8-in-dual-boot/\n\n\nserver naming scheme: great lakes, main pc is michigan", 
            "title": "Windows 10"
        }, 
        {
            "location": "/hosts/windows_10/#installation", 
            "text": "ubuntu live usb and use gparted to remove partitions and format as ntfs  boot windows 10 usb and select education edition - full name and select basic security question  reconnect hard drives and reboot, if extra drives formatted as ntsf, they  should automatically be added as the remaining drive letters, D,E,F,G, etc  go to disk management and name them if necessary  go to settings - notifications   actions, turn off notifications frm apps and other senders  settings - storage: change where new content is saved and move everything to  other drive (pictures, documents, etc)  settings - remote desktop enabled  settings - shared experiences - disable share across devices  settings - printers   scanners: disable windows manage default printer  settings - typing: turn off everything basically, same with pen   windows ink, autoplay  settings - personalization: turn everything off basically unpin all junk from taskbar  settings - personalization - taskbar: turn off contacts settings  settings - privacy - basically turn everything off  settings - cortanna: basically turn everything off  settings - gaming: turn everything off  downloads: chrome, keepass, btsync, notepad++, steam, gog, flux, rclone, geforce experience, google drive  ubuntu dual boot: go to diskmgmt.msc and select C: partition and shrink volume, then boot ubuntu and install, should only select the C: drive and give no options unlike before. followed guide from:  https://www.tecmint.com/install-ubuntu-16-04-alongside-with-windows-10-or-8-in-dual-boot/  server naming scheme: great lakes, main pc is michigan", 
            "title": "Installation"
        }, 
        {
            "location": "/unix/git/", 
            "text": "Merge two different repositories\n\n\nBe sure to remove merge conflicts from repositories, like multiple \nREADME.md\n, \n.gitignore\n\n\ncd /projectb/path\ngit remote add projecta /projecta/path\ngit fetch projecta\ngit merge projecta/master\ngit remote remove projecta\n\n\n\n\nMultiple Remotes\n\n\nChange origin remote URL: \nhttps://help.github.com/articles/changing-a-remote-s-url/\n\n\nRemove remote: \nhttps://help.github.com/articles/removing-a-remote/\n\n\nOrigin should be set to internal Gogs server\n\n\nAdd BitBucket and Github remotes with\n\n\ngit remote add bitbucket \nhttps://bitbucket.com/ajd2/repo.git\n # or use SSH connection\ngit push bitbucket master\n\n\n\n\nList large files in git repositry\n\n\nhttps://confluence.atlassian.com/bitbucket/reduce-repository-size-321848262.html\n\n\nhttps://rtyley.github.io/bfg-repo-cleaner/\n\n\nAdd this shell script to repository\n\n\n#!/bin/bash\n#set -x\n\n# Shows you the largest objects in your repo's pack file.\n# Written for osx.\n#\n# @see \nhttp://stubbisms.wordpress.com/2009/07/10/git-script-to-show-largest-pack-objects-and-trim-your-waist-line/\n\n# @author Antony Stubbs\n\n# set the internal field spereator to line break, so that we can iterate easily over the verify-pack output\nIFS=$'\\n';\n\n# list all objects including their size, sort by size, take top 10\nobjects=`git verify-pack -v .git/objects/pack/pack-*.idx | grep -v chain | sort -k3nr | head`\n\necho \nAll sizes are in kB's. The pack column is the size of the object, compressed, inside the pack file.\n\n\noutput=\nsize,pack,SHA,location\n\nfor y in $objects\ndo\n# extract the size in bytes\nsize=$((`echo $y | cut -f 5 -d ' '`/1024))\n# extract the compressed size in bytes\ncompressedSize=$((`echo $y | cut -f 6 -d ' '`/1024))\n# extract the SHA\nsha=`echo $y | cut -f 1 -d ' '`\n# find the objects location in the repository tree\nother=`git rev-list --all --objects | grep $sha`\n#lineBreak=`echo -e \n\\n\n`\noutput=\n${output}\\n${size},${compressedSize},${other}\n\ndone\n\necho -e $output | column -t -s ', '\n\n\n\n\nMake it executable with \nchmod +x git_find_big.sh\n, do garbage collection \ngit gc --auto\n, \ndu -hs .git/objects\n, and then list large files with \n./git_find_big.sh\n\n\nRemoving Large Files from repository history\n\n\nFor all those pesky \nRData\n and \ncsv\n files accidentally committed.\n\n\nBFG Repo Cleaner, \napt install default-jre\n and then \nwget\n the \n.jar\n from the documentation.\n\n\nMake a copy of your bare repo, `git clone --mirror \nhttps://myrepo/user/repo.git\n\n\nClean repo with \njava -jar /path/to/bfg.jar --strip-blobs-bigger-than 100M repo.git\n or \njava -jar /path/to/bfg.jar --delete-files *.ext repo.git\n\n\nWhen done making changes\n\n\ncd repo.git\ngit reflog expire --expire=now --all \n git gc --prune=now --aggressive\ngit push", 
            "title": "Git"
        }, 
        {
            "location": "/unix/git/#merge-two-different-repositories", 
            "text": "Be sure to remove merge conflicts from repositories, like multiple  README.md ,  .gitignore  cd /projectb/path\ngit remote add projecta /projecta/path\ngit fetch projecta\ngit merge projecta/master\ngit remote remove projecta", 
            "title": "Merge two different repositories"
        }, 
        {
            "location": "/unix/git/#multiple-remotes", 
            "text": "Change origin remote URL:  https://help.github.com/articles/changing-a-remote-s-url/  Remove remote:  https://help.github.com/articles/removing-a-remote/  Origin should be set to internal Gogs server  Add BitBucket and Github remotes with  git remote add bitbucket  https://bitbucket.com/ajd2/repo.git  # or use SSH connection\ngit push bitbucket master", 
            "title": "Multiple Remotes"
        }, 
        {
            "location": "/unix/git/#list-large-files-in-git-repositry", 
            "text": "https://confluence.atlassian.com/bitbucket/reduce-repository-size-321848262.html  https://rtyley.github.io/bfg-repo-cleaner/  Add this shell script to repository  #!/bin/bash\n#set -x\n\n# Shows you the largest objects in your repo's pack file.\n# Written for osx.\n#\n# @see  http://stubbisms.wordpress.com/2009/07/10/git-script-to-show-largest-pack-objects-and-trim-your-waist-line/ \n# @author Antony Stubbs\n\n# set the internal field spereator to line break, so that we can iterate easily over the verify-pack output\nIFS=$'\\n';\n\n# list all objects including their size, sort by size, take top 10\nobjects=`git verify-pack -v .git/objects/pack/pack-*.idx | grep -v chain | sort -k3nr | head`\n\necho  All sizes are in kB's. The pack column is the size of the object, compressed, inside the pack file. \n\noutput= size,pack,SHA,location \nfor y in $objects\ndo\n# extract the size in bytes\nsize=$((`echo $y | cut -f 5 -d ' '`/1024))\n# extract the compressed size in bytes\ncompressedSize=$((`echo $y | cut -f 6 -d ' '`/1024))\n# extract the SHA\nsha=`echo $y | cut -f 1 -d ' '`\n# find the objects location in the repository tree\nother=`git rev-list --all --objects | grep $sha`\n#lineBreak=`echo -e  \\n `\noutput= ${output}\\n${size},${compressedSize},${other} \ndone\n\necho -e $output | column -t -s ', '  Make it executable with  chmod +x git_find_big.sh , do garbage collection  git gc --auto ,  du -hs .git/objects , and then list large files with  ./git_find_big.sh", 
            "title": "List large files in git repositry"
        }, 
        {
            "location": "/unix/git/#removing-large-files-from-repository-history", 
            "text": "For all those pesky  RData  and  csv  files accidentally committed.  BFG Repo Cleaner,  apt install default-jre  and then  wget  the  .jar  from the documentation.  Make a copy of your bare repo, `git clone --mirror  https://myrepo/user/repo.git  Clean repo with  java -jar /path/to/bfg.jar --strip-blobs-bigger-than 100M repo.git  or  java -jar /path/to/bfg.jar --delete-files *.ext repo.git  When done making changes  cd repo.git\ngit reflog expire --expire=now --all   git gc --prune=now --aggressive\ngit push", 
            "title": "Removing Large Files from repository history"
        }, 
        {
            "location": "/unix/regex/", 
            "text": "Usage\n\n\n\n\ngrep regex file\n\n\ngrep -i regex file\n is case insensitive matching\n\n\nPython \nre\n module\n\n\n\n\nMetacharacter Reference\n\n\n\n\n*\n: Match anything, eg, \n*.txt$\n matches anything that ends in \n.txt\n.\n\n\n?\n: Optional match a character, eg, \n?at\n matches all of 'cat', 'rat', 'sat', \nat\n, etc.\n\n\n^\n: Start of a line. Or negation inside \n[^abc]\n\n\n$\n: End of a line.\n\n\n[..]\n: Character class. Explicitly list the characters we want to match, eg, \n[aeiou]\n matches any vowels, \n[A-Za-z]\n matches any alphabetical character, \n[0-9]\n matches any digit, \n[^ae]\n matches everything but 'a' or 'e'. Note that \n-\n inside a character class is only considered a character if it's first.\n\n\n.\n matches any single character.\n\n\n|\n means 'or' and is often used with parentheses to constrain the alternation, eg, \ngr(a|e)y\n. Note that \ngr[a|e]y\n is not valid because \n|\n is interpreted as a character and not a metacharacter inside brackets, also \ngr[ae]y\n works.\n\n\n\\\nword\\\n are word boundary metacharacters, this example matches 'word' and not 'stopwords'.\n\n\n+\n matches one or more of the immediately preceding item.\n\n\n\n\nAlternation Versus Character Class\n\n\nWhile \ngr[ae]y\n and \ngr(a|e)y\n are equivalent, character classes only match single characters. Using alternation, we can match full words, eg, \n^(From|Subject):\n matches either of those words.", 
            "title": "Regex"
        }, 
        {
            "location": "/unix/regex/#usage", 
            "text": "grep regex file  grep -i regex file  is case insensitive matching  Python  re  module", 
            "title": "Usage"
        }, 
        {
            "location": "/unix/regex/#metacharacter-reference", 
            "text": "* : Match anything, eg,  *.txt$  matches anything that ends in  .txt .  ? : Optional match a character, eg,  ?at  matches all of 'cat', 'rat', 'sat',  at , etc.  ^ : Start of a line. Or negation inside  [^abc]  $ : End of a line.  [..] : Character class. Explicitly list the characters we want to match, eg,  [aeiou]  matches any vowels,  [A-Za-z]  matches any alphabetical character,  [0-9]  matches any digit,  [^ae]  matches everything but 'a' or 'e'. Note that  -  inside a character class is only considered a character if it's first.  .  matches any single character.  |  means 'or' and is often used with parentheses to constrain the alternation, eg,  gr(a|e)y . Note that  gr[a|e]y  is not valid because  |  is interpreted as a character and not a metacharacter inside brackets, also  gr[ae]y  works.  \\ word\\  are word boundary metacharacters, this example matches 'word' and not 'stopwords'.  +  matches one or more of the immediately preceding item.", 
            "title": "Metacharacter Reference"
        }, 
        {
            "location": "/unix/regex/#alternation-versus-character-class", 
            "text": "While  gr[ae]y  and  gr(a|e)y  are equivalent, character classes only match single characters. Using alternation, we can match full words, eg,  ^(From|Subject):  matches either of those words.", 
            "title": "Alternation Versus Character Class"
        }, 
        {
            "location": "/unix/tools/", 
            "text": "top/htop\n\n\ngrep\n\n\nsed\n\n\nawk\n\n\nvim\n\n\npipe operator\n\n\nless/more\n\n\ngit\n\n\npushd/popd\n\n\n!! run last command\n\n\n!* run command with arguments passed to previous command\n\n\n!^ first argument in bash history\n\n\n!$ last arugument in bash history\n\n\n!?keyword? run last command from bash history beginning with keyword\n\n\nrsync\n\n\nps\n\n\ntee for storing/viewing output of another command\n\n\nfind\n\n\ntree\n\n\nkaggle has command line client for downloading datasets\n\n\nslurm\n\n\nenvironment modules\n\n\nogr2ogr for shapefile/geojson conversions\n\n\ntmux", 
            "title": "Tools"
        }, 
        {
            "location": "/datasets/datasets/", 
            "text": "Driving video database: \nhttp://bair.berkeley.edu/blog/2018/05/30/bdd/\n\n\nhttps://www.microsoft.com/en-us/research/blog/announcing-microsoft-research-open-data-datasets-by-microsoft-research-now-available-in-the-cloud/\n\n\ngoogle bigquery\n\n\ngoogle open images 9million images in 6000 categories\n\n\ngoogle vector drawings quickdraw.withgoogle.com\n\n\nmsropen data", 
            "title": "Datasets"
        }, 
        {
            "location": "/deep_learning/links/", 
            "text": "https://www.azavea.com/blog/2017/05/30/deep-learning-on-aerial-imagery/\n # cool blog\n\n\nmaking infrared color masks on images based on what density we want\n\n\ncv2.Canny, cv2.GaussainBlur\n\n\n\n\naerieal imagery dataset and tutorial\n\n\n\n\nhttps://en.wikipedia.org/wiki/National_Lidar_Dataset_(United_States)\n\n\nhttps://coast.noaa.gov/inventory/\n\n\nhttps://viewer.nationalmap.gov/basic/#productSearch\n\n\nftp://rockyftp.cr.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/\n\n\nhttp://pythonhosted.org/laspy/tut_part_1.html\n\n\nhttps://www.youtube.com/watch?v=SIctLjCScnk\n\n\nhttps://handong1587.github.io/deep_learning/2015/10/09/object-detection.html\n\n\nhttps://www.azavea.com/blog/2017/05/30/deep-learning-on-aerial-imagery/\n\n\nhttps://www.robots.ox.ac.uk/~vgg/rg/slides/vgg_rg_16_feb_2017_rfcn.pdf\n\n\nhttps://github.com/xdever/RFCN-tensorflow\n\n\nhttps://github.com/bharatsingh430/Deformable-ConvNets/\n\n\nhttps://www.slideshare.net/WenjingChen7/deep-learning-for-object-detection\n\n\nhttps://arxiv.org/pdf/1605.06409.pdf\n\n\nhttps://github.com/msracver/Deformable-ConvNets\n\n\nhttps://github.com/xslittlegrass/CarND-Vehicle-Detection\n\n\nhttps://github.com/experiencor/basic-yolo-keras/blob/master/Basic%20Yolo%20Keras.ipynb\n\n\nhttps://experiencor.github.io/yolo_keras.html\n\n\nhttps://github.com/sadeepj/crfasrnn_keras\n\n\nhttps://pdfs.semanticscholar.org/2612/a977affa97090aa6b7c740b262bf9ce71458.pdf\n\n\nhttps://lmb.informatik.uni-freiburg.de/lectures/seminar_brox/seminar_ss17/maskrcnn_slides.pdf\n\n\nhttp://classes.engr.oregonstate.edu/eecs/spring2017/cs637/Slides/CS_637_Fast_Faster_Mask_RCNN.pdf\n\n\nhttp://blog.qure.ai/notes/semantic-segmentation-deep-learning-review\n\n\nhttp://forums.fast.ai/t/implementing-mask-r-cnn/2234\n\n\nhttps://arxiv.org/pdf/1705.05922.pdf\n\n\nhttps://arxiv.org/pdf/1612.01051.pdf\n\n\nhttps://www.ir.com/blog/visualizing-outputs-cnn-model-training-phase\n\n\nhttps://blog.deepsense.ai/region-of-interest-pooling-explained/\n\n\nhttps://logz.io/blog/machine-learning-log-analytics/\n\n\nhttps://www.druva.com/blog/machine-learning-detect-anomalies-application-logs/", 
            "title": "Links"
        }, 
        {
            "location": "/deep_learning/links/#aerieal-imagery-dataset-and-tutorial", 
            "text": "https://en.wikipedia.org/wiki/National_Lidar_Dataset_(United_States)  https://coast.noaa.gov/inventory/  https://viewer.nationalmap.gov/basic/#productSearch  ftp://rockyftp.cr.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/  http://pythonhosted.org/laspy/tut_part_1.html  https://www.youtube.com/watch?v=SIctLjCScnk  https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html  https://www.azavea.com/blog/2017/05/30/deep-learning-on-aerial-imagery/  https://www.robots.ox.ac.uk/~vgg/rg/slides/vgg_rg_16_feb_2017_rfcn.pdf  https://github.com/xdever/RFCN-tensorflow  https://github.com/bharatsingh430/Deformable-ConvNets/  https://www.slideshare.net/WenjingChen7/deep-learning-for-object-detection  https://arxiv.org/pdf/1605.06409.pdf  https://github.com/msracver/Deformable-ConvNets  https://github.com/xslittlegrass/CarND-Vehicle-Detection  https://github.com/experiencor/basic-yolo-keras/blob/master/Basic%20Yolo%20Keras.ipynb  https://experiencor.github.io/yolo_keras.html  https://github.com/sadeepj/crfasrnn_keras  https://pdfs.semanticscholar.org/2612/a977affa97090aa6b7c740b262bf9ce71458.pdf  https://lmb.informatik.uni-freiburg.de/lectures/seminar_brox/seminar_ss17/maskrcnn_slides.pdf  http://classes.engr.oregonstate.edu/eecs/spring2017/cs637/Slides/CS_637_Fast_Faster_Mask_RCNN.pdf  http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review  http://forums.fast.ai/t/implementing-mask-r-cnn/2234  https://arxiv.org/pdf/1705.05922.pdf  https://arxiv.org/pdf/1612.01051.pdf  https://www.ir.com/blog/visualizing-outputs-cnn-model-training-phase  https://blog.deepsense.ai/region-of-interest-pooling-explained/  https://logz.io/blog/machine-learning-log-analytics/  https://www.druva.com/blog/machine-learning-detect-anomalies-application-logs/", 
            "title": "aerieal imagery dataset and tutorial"
        }, 
        {
            "location": "/deep_learning/scratch/", 
            "text": "About\n\n\nThis page is for miscellaneous notes on deep learning concepts, ideas, and problems. In time these will be expanded into their own pages, but for now these serve as bookmarks.\n\n\nTypes\n\n\n\n\nNeural Network\n\n\nConvolutional Neural Network\n\n\nRecurrent Neural Network (GUR, LSTM)\n\n\nGAN\n\n\nrepresentation learning\n\n\nautoencoding\n\n\nreinforcement learning\n\n\ntransfer learning\n\n\n\n\nProblems\n\n\n\n\nImage classification\n\n\nRepresentation Learning\n\n\nAutoEncoder\n\n\nImage to text/caption generation\n\n\nStarry night + photo = Starry night photo\n\n\nImage detection or outline of objects\n\n\nBounding box of faces\n\n\nHeatmap of images for classifier predictions\n\n\nGAN image/text generation\n\n\nTensorBoard\n\n\nAudio transcription, detection, etc\n\n\n\n\nConcepts\n\n\n\n\nNumber of filters, stride lengths, padding\n\n\nData augmentation, ie, rotating images, flipping, shifting, etc\n\n\nGlobal vs local pooling and min/max/average pooling\n\n\nNo pooling in between convolutional layers but at end\n\n\nbatch normalization\n\n\ndilation\n\n\nDropout versus explicit regularization\n\n\nAlternating convolutional layers beginning with descending 128 hidden layers to 64, 32, 16 before pooling with two sets of convolutional layers with stride length two first and one second\n\n\n\n\nconditional random fields\n\n\n\n\n\n\nkaggle datasets\n\n\n\n\ncv datasets on the web for image datasets in computer vision\n\n\nnot hotdog\n\n\nnetflix movies\n\n\noriginalsnapchat filter\n\n\ntwitter stream\n\n\nsignal processing\n\n\ngoogle cloud big data platform\n\n\nvci machine learning repository\n\n\ngithub awesome public datasets\n\n\nautoencoders\n\n\nplaces 365 dataset\n\n\nscikit-image bounding box segmentation\n\n\nsegnet\n\n\ntsosu technique\n\n\nvisualize outputs in CNN model network layers (ir.com blog post)\n\n\nMNIST GAN example from \nhttps://towardsdatascience.com/demystifying-generative-adversarial-networks-c076d8db8f44\n\n\ngoogle cloud ml examples, they tag stack overflow posts\n\n\ntwitter/stack-overflow/etc tagging\n\n\nreal-time human pose estimation \nhttps://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n\n\nclassify image/descirbe image/annotate image\n\n\nencoder-decoder\n\n\ngenetic algorithms to get neural network architecture\n\n\nflatten with hidden vs global max/average pooling\n\n\nunderstand what the layers are doing conceptually, ie, local vs global features\n\n\ndilation, stride, hidden units\n\n\ngenetic algorithm: \nhttps://github.com/joeddav/devol/blob/master/genome_handler.py\n\n\nface averaging like on reddit\n\n\nplaces365\n\n\nimage transformations\n\n\nbounding boxes/polygons/edge detection/brush\n\n\nlabeling application to easily label photos for classification tasks\n\n\nstereo depth algorithms\n\n\nreal-time video processing\n\n\nobject detection/bounding box/segmentation\n\n\nheat map over picture of what algorithm picked up on most\n\n\nlocalization\n\n\nsqueezenet\n\n\ndarkflow\n\n\naudio transcription/music tagging\n\n\nvideo lapse\n\n\ngenerate video from images\n\n\nface swap\n\n\nchat bot\n\n\ncrossover: combine models to form child, not obvious to do if they have different layers\n\n\nput accuracy, fscore, precision, accuracy on same figure\n\n\nnot hotdog using keras, tf, tf mobile\n\n\ntf mobile\n\n\nsemantic segmentation, localization, object detection, instance segmentation\n\n\nleonardo gitbooks for good tutorials on cnn\n\n\nopen source satellite imagery\n\n\nkitte road dataset\n\n\nbatch normalization, genetic algorithms, regularization, under/oversampling,\n\n\nauto encoding\n\n\ndense layer in between output and loss function\n\n\nimage augmentation (rotation etc)\n\n\nconnectionist temporal classification - train for speech recognition, handwriting recognition, other sequence analysis\n\n\nglcm, gabor, local binary patterns, smoothing, contours, circle mask, tree mask, stitching images together, simultaneous location and mapping, semantic segmentation, segnet, otsu technique, regionprops\n\n\nconditional random fields\n\n\ndeep learning chat bot\n\n\nstargan model\n\n\nneural color transfer between images\n\n\ndeep image analogy\n\n\nneural style\n\n\nfast.ai\n\n\ndensepose from facebook\n\n\nhttp://botnik.org/content/harry-potter.html\n gan to make some harry potter chapters\n\n\nhttps://licensed.storyful.com/videos/189917\n gan to make a recipe\n\n\nhttps://twitter.com/headlinertron/\n gan powered commedian\n\n\nluncene vector space model\n\n\nexplosion.ai\n\n\ntensorflow, keras resources on github\n\n\ngenetic algorithms and hyperparameter optimization for deep learning models\n\n\nsingle class, single-multi class, multi-multi class classification\n\n\nhttp://theorangeduck.com/page/neural-network-not-working\n\n\nhttps://deepmind.com/\n wavenet generative model for raw audio\n\n\nhttps://distill.pub/\n\n\ngan trained on olive garden commercials, harry potter similar\n\n\nhttps://medium.com/retail-vuepoints/your-shoppers-ai-powered-stylist-vue-ai-s-ensemble-generator-f9dced5c66f7,\n \nhttps://medium.com/@fadeurlife/fadeur-your-ai-powered-fashion-stylist-for-every-moment-bb3f7ae8ccac,\n \nhttps://hackernoon.com/does-this-blazer-go-with-this-shirt-ai-stylist-54ec79a47215?gi=85738099fe22\n fashion powered stylists\n\n\nhttps://medium.com/data-from-the-trenches/object-detection-with-deep-learning-on-aerial-imagery-2465078db8a9\n\n\nhttps://able.bio/devforfu/identifying-dog-breeds-using-keras--767qpxs\n\n\nhttps://medium.com/@animeshsk3/t2f-text-to-face-generation-using-deep-learning-b3b6ba5a5a93\n\n\ntext to face generation using deep learning\n\n\ngenerate music using a lstm neural network\n\n\nsnapchat-like filters with python openvc/dlib and tkinter\n\n\nmuvilib labeller\n\n\nlabel/annotator r shiny app\n\n\nhttps://github.com/shinseung428/GlobalLocalImageCompletion_TF\n\n\nhttp://dcgi.fel.cvut.cz/home/sykorad/facestyle.html\n\n\nstargan takes input picture and can change hair color gender age, very cool\n\n\nhttps://dmitryulyanov.github.io/deep_image_prior\n\n\nhttps://github.com/msracver/Deep-Image-Analogy\n\n\nhttps://www.reddit.com/r/MachineLearning/comments/748cco/r_neural_color_transfer_between_images/\n\n\ntext to speech/speech to text (much harder)\n\n\nhttps://www.reddit.com/r/MachineLearning/comments/8vbkti/p_progan_trained_on_rearthporn_images/\n\n\nsegmodel, digitreader", 
            "title": "Scratch"
        }, 
        {
            "location": "/deep_learning/scratch/#about", 
            "text": "This page is for miscellaneous notes on deep learning concepts, ideas, and problems. In time these will be expanded into their own pages, but for now these serve as bookmarks.", 
            "title": "About"
        }, 
        {
            "location": "/deep_learning/scratch/#types", 
            "text": "Neural Network  Convolutional Neural Network  Recurrent Neural Network (GUR, LSTM)  GAN  representation learning  autoencoding  reinforcement learning  transfer learning", 
            "title": "Types"
        }, 
        {
            "location": "/deep_learning/scratch/#problems", 
            "text": "Image classification  Representation Learning  AutoEncoder  Image to text/caption generation  Starry night + photo = Starry night photo  Image detection or outline of objects  Bounding box of faces  Heatmap of images for classifier predictions  GAN image/text generation  TensorBoard  Audio transcription, detection, etc", 
            "title": "Problems"
        }, 
        {
            "location": "/deep_learning/scratch/#concepts", 
            "text": "Number of filters, stride lengths, padding  Data augmentation, ie, rotating images, flipping, shifting, etc  Global vs local pooling and min/max/average pooling  No pooling in between convolutional layers but at end  batch normalization  dilation  Dropout versus explicit regularization  Alternating convolutional layers beginning with descending 128 hidden layers to 64, 32, 16 before pooling with two sets of convolutional layers with stride length two first and one second   conditional random fields    kaggle datasets   cv datasets on the web for image datasets in computer vision  not hotdog  netflix movies  originalsnapchat filter  twitter stream  signal processing  google cloud big data platform  vci machine learning repository  github awesome public datasets  autoencoders  places 365 dataset  scikit-image bounding box segmentation  segnet  tsosu technique  visualize outputs in CNN model network layers (ir.com blog post)  MNIST GAN example from  https://towardsdatascience.com/demystifying-generative-adversarial-networks-c076d8db8f44  google cloud ml examples, they tag stack overflow posts  twitter/stack-overflow/etc tagging  real-time human pose estimation  https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5  classify image/descirbe image/annotate image  encoder-decoder  genetic algorithms to get neural network architecture  flatten with hidden vs global max/average pooling  understand what the layers are doing conceptually, ie, local vs global features  dilation, stride, hidden units  genetic algorithm:  https://github.com/joeddav/devol/blob/master/genome_handler.py  face averaging like on reddit  places365  image transformations  bounding boxes/polygons/edge detection/brush  labeling application to easily label photos for classification tasks  stereo depth algorithms  real-time video processing  object detection/bounding box/segmentation  heat map over picture of what algorithm picked up on most  localization  squeezenet  darkflow  audio transcription/music tagging  video lapse  generate video from images  face swap  chat bot  crossover: combine models to form child, not obvious to do if they have different layers  put accuracy, fscore, precision, accuracy on same figure  not hotdog using keras, tf, tf mobile  tf mobile  semantic segmentation, localization, object detection, instance segmentation  leonardo gitbooks for good tutorials on cnn  open source satellite imagery  kitte road dataset  batch normalization, genetic algorithms, regularization, under/oversampling,  auto encoding  dense layer in between output and loss function  image augmentation (rotation etc)  connectionist temporal classification - train for speech recognition, handwriting recognition, other sequence analysis  glcm, gabor, local binary patterns, smoothing, contours, circle mask, tree mask, stitching images together, simultaneous location and mapping, semantic segmentation, segnet, otsu technique, regionprops  conditional random fields  deep learning chat bot  stargan model  neural color transfer between images  deep image analogy  neural style  fast.ai  densepose from facebook  http://botnik.org/content/harry-potter.html  gan to make some harry potter chapters  https://licensed.storyful.com/videos/189917  gan to make a recipe  https://twitter.com/headlinertron/  gan powered commedian  luncene vector space model  explosion.ai  tensorflow, keras resources on github  genetic algorithms and hyperparameter optimization for deep learning models  single class, single-multi class, multi-multi class classification  http://theorangeduck.com/page/neural-network-not-working  https://deepmind.com/  wavenet generative model for raw audio  https://distill.pub/  gan trained on olive garden commercials, harry potter similar  https://medium.com/retail-vuepoints/your-shoppers-ai-powered-stylist-vue-ai-s-ensemble-generator-f9dced5c66f7,   https://medium.com/@fadeurlife/fadeur-your-ai-powered-fashion-stylist-for-every-moment-bb3f7ae8ccac,   https://hackernoon.com/does-this-blazer-go-with-this-shirt-ai-stylist-54ec79a47215?gi=85738099fe22  fashion powered stylists  https://medium.com/data-from-the-trenches/object-detection-with-deep-learning-on-aerial-imagery-2465078db8a9  https://able.bio/devforfu/identifying-dog-breeds-using-keras--767qpxs  https://medium.com/@animeshsk3/t2f-text-to-face-generation-using-deep-learning-b3b6ba5a5a93  text to face generation using deep learning  generate music using a lstm neural network  snapchat-like filters with python openvc/dlib and tkinter  muvilib labeller  label/annotator r shiny app  https://github.com/shinseung428/GlobalLocalImageCompletion_TF  http://dcgi.fel.cvut.cz/home/sykorad/facestyle.html  stargan takes input picture and can change hair color gender age, very cool  https://dmitryulyanov.github.io/deep_image_prior  https://github.com/msracver/Deep-Image-Analogy  https://www.reddit.com/r/MachineLearning/comments/748cco/r_neural_color_transfer_between_images/  text to speech/speech to text (much harder)  https://www.reddit.com/r/MachineLearning/comments/8vbkti/p_progan_trained_on_rearthporn_images/  segmodel, digitreader", 
            "title": "Concepts"
        }, 
        {
            "location": "/machine_learning/methods/", 
            "text": "Need standard set of techniques to do when approaching new problems.\n\n\nMethods\n\n\n\n\nregression\n\n\nlogistic regression\n\n\nlasso\n\n\nridge\n\n\nelastic net\n\n\ncart\n\n\nrandom forest\n\n\nadaboost\n\n\nextratrees\n\n\ngcForest\n\n\nk means\n\n\ndbscan\n\n\npcr\n\n\npca\n\n\nsvm\n\n\nwls\n\n\nfeature importance plots\n\n\nroc/auc plots\n\n\nisolation forest\n\n\nmanifold learning/tsne\n\n\ndbscan\n\n\nhierarchical clustering\n\n\nboosted forest\n\n\nmultidimensional scaling\n\n\nagglomerative/hierarchical\n\n\n\n\nScoring Metrics\n\n\nDifferent ways to score if groups/unbalanced classes\n\n\nDifferent Shuffling Methods\n\n\nHow to split data properly for grid searching model to be used over before training and then testing?\n\n\nSalad\n\n\n\n\nsklearn face data examples\n\n\npca/kmeans/agglomerative clustering use scree plots\n\n\ndbscan isomap\n\n\nother metrics for clustering besides rand\n\n\nover vs undersampling with imbalanced classes\n\n\nDood: set of hyperparameters selected randomly, train N models, select M best, add jitter, add more randomly generated and iterate to automatically choose model architecture\n\n\nCrossover: because not obvious to combine models with different number of layers to form child\n\n\ncostline automation genetic algorithms\n\n\ndimension reduction then fit classifier/regression examples\n\n\nanomaly detection\n\n\nlstm\n\n\nisolation forest\n\n\nnmf\n\n\nreddit machine learning wiki\n\n\nhttps://distill.pub/2016/misread-tsne/\n how to use t-sne effectively", 
            "title": "Methods"
        }, 
        {
            "location": "/machine_learning/methods/#methods", 
            "text": "regression  logistic regression  lasso  ridge  elastic net  cart  random forest  adaboost  extratrees  gcForest  k means  dbscan  pcr  pca  svm  wls  feature importance plots  roc/auc plots  isolation forest  manifold learning/tsne  dbscan  hierarchical clustering  boosted forest  multidimensional scaling  agglomerative/hierarchical", 
            "title": "Methods"
        }, 
        {
            "location": "/machine_learning/methods/#scoring-metrics", 
            "text": "Different ways to score if groups/unbalanced classes", 
            "title": "Scoring Metrics"
        }, 
        {
            "location": "/machine_learning/methods/#different-shuffling-methods", 
            "text": "How to split data properly for grid searching model to be used over before training and then testing?", 
            "title": "Different Shuffling Methods"
        }, 
        {
            "location": "/machine_learning/methods/#salad", 
            "text": "sklearn face data examples  pca/kmeans/agglomerative clustering use scree plots  dbscan isomap  other metrics for clustering besides rand  over vs undersampling with imbalanced classes  Dood: set of hyperparameters selected randomly, train N models, select M best, add jitter, add more randomly generated and iterate to automatically choose model architecture  Crossover: because not obvious to combine models with different number of layers to form child  costline automation genetic algorithms  dimension reduction then fit classifier/regression examples  anomaly detection  lstm  isolation forest  nmf  reddit machine learning wiki  https://distill.pub/2016/misread-tsne/  how to use t-sne effectively", 
            "title": "Salad"
        }, 
        {
            "location": "/machine_learning/trees/", 
            "text": "CART\n\n\nCannot extrapolate outside of the range of the training data.", 
            "title": "Trees"
        }, 
        {
            "location": "/machine_learning/trees/#cart", 
            "text": "Cannot extrapolate outside of the range of the training data.", 
            "title": "CART"
        }, 
        {
            "location": "/natural_language_processing/concepts/", 
            "text": "https://github.com/keon/awesome-nlp\n\n\nbag of words model\n\n\nText is represented as the bag or multiset of its words. It disregards grammar and word order and only keeps the multiplicity of each word.\n\n\nSo \"John likes to watch movies. Mary likes movies too.\" is decomposed into \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"likes\",\"movies\",\"too\" and then the bag of words representation would look like \n{\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1}\n.\n\n\ntfidf\n\n\nTerm frequency-inverse document frequency. Statistical measure to evaluate how important a word is to a document in a corpus. Importance increase proportionally to the number of times a word appears in a document but is offset by the frequency of the word in the corpus.\n\n\ntfidf can be used in a search scheme in scoring and ranking a document's relevance to a query. The simplest ranking function is summing the tfidf for each query term.\n\n\nIt is successively used for stop word filtering.\n\n\nComposed of two terms. Term frequency is the number of times the word appears in a document, divided by the total number of words in that document. Inverse document frequency is the logarithm of the number of documents in the corpus divided by the number of documents where the term appears.\n\n\nlatent semantic indexing/analysis\n\n\nAnalyzes documents to find the underlying meaning or concepts of the documents. Basically SVD on tfidf/bow vectors.\n\n\nProblem setting: Comparing words to find relevant documents, because we really want to do is compare meanings or concepts behind the words. LSI maps the words and documents into a vector space to do the comparison there. Concepts are obscured with different word choices, introducing noise. LSA filters out some of the noise to find the smallest set of concepts that span all the documents.\n\n\nIdea:\n- Documents are represented as bags of words\n- Topics/concepts are represented as patterns of words that usually appear together in documents\n- Words are assumed to have a single meaning to make the problem tractable\n- Usually use tfidf on the bag of words befor performing LSI\n- Essentially performing SVD on the matrix of word counts\n\n\nApplication:\n- Cluster documents/words\n- Feature for classification\n\n\nlatent Dirichlet analysis\n\n\nRepresents documents as mixtures of topics that spit out words with certain probabilities. Assumes that documents are produced following:\n- N words in each document follow a distribution, maybe Poisson\n- There is a topic mixture in each document following a Dirichlet distribution over K topics\n- Each word in the document is generated by first picking a topic according to the multinomial distribution above. The topic then generates the word itself.\nLDA then tries to use the documents to infer the set of topics that generated them.\n\n\nWe have a corpus with K topics to discover a priori. Using collapsed Gibbs sampling, LDA:\n- goes through each document and randomly assigns each word to one of the K topics\n- for each word in each document and for each topic, compute the proportion of words in the document that are assigned to that topic and the proportion of assigned words to that topic over all documents that come from this word. We then reassign the word a different topic and compute the posterior. So we're updating the assignment of our current word using our model of how all the other words are assigned.\n- iterate until our assignments are stable.\n- Use these assignments to estimate the topic mixtures of each document using proportion of words assigned to each topic within that document and the words associated to each topic overall.\n\n\nEssential usage is dimension reduction and uncovering the underlying themes in your corpora.\n\n\nword2vec\n\n\nLearns relationships between words essentially. Uses the context in which words are used to infer their similarities. Uses a neural network with few layers to predict the current word based on the context. We use the weights of the hidden layers as the embeddings or actual word vectors. We can then use cosine similarity or other distance metrics to compute word similarities or use the word vectors for classification/clustering. Also useful for sentiment analysis, where we use the similarity scores. Uses skip gram or continuous bag of words for these probability calculations.\n\n\ndoc2vec\n\n\nSimilar idea to word2vec that extends the method to unsupervised learning of continuous representations for larger blocks of text like sentences, paragraphs, or entire documents. Again. clustering or similarity applications.\n\n\nWord Mover's Distance\n\n\nSubmit a query and return the most relevant documents. Assess the distance between documents in a meaningful way, even when they have no words in common. It does use word2vec word embedding. Helpful that word vectors are normalized. Also uses euclidean distance instead of cosine similarity.\n\n\nTopic Modelling Terminology\n\n\n\n\nIf document classification is assigning a single category to a text, topic modelling is assigning multiple tags to a text.\n\n\nCoherence: Compute sum of pairwise scores of top n words used to describe a topic. There are a few different measures of this. Good models generate coherent topics, ie, topics with high coherence scores.\n\n\nPerplexity: Measure of how well a probability distribution or model predicts a sample. In LDA, topics are described by a probability distribution over vocabulary words. So it can be used to evaluate the topic-term distribution output. Good models have low perplexity.\n\n\nTopic Difference: Calculates the distance between two topic models. It's calculated based on the topics either using their probability distributions over vocabulary words or using the common vocabulary words between the topics from both models. By increasing epochs in both models, the distance between identical topics should decrease.\n\n\nConvergence: Sum of the difference between all identical topics from consecutive epochs. Models have converged when the convergence stops decreasing with increasing epochs.\n\n\n\n\nTopic Modelling Uses\n\n\n\n\ntext classification by grouping similar words together\n\n\nrecommendation systems: Use similarity measures to build recommender systems that recommend text with a topic structure similar to what our current text\n\n\nUncovering themes in text\n\n\n\n\nDynamic Topic Modeling\n\n\nAuthor-Topic Modeling\n\n\nSentiment Analysis\n\n\nAnalyse the opinion or tone of text. Polarity analysis identifies tones as positive or negative. Categorisation can in addition identify confused or angry. There is also an emotion scale 'sad' to 'happy' and from 0-10.\n\n\nCan attempt to answer questions such as:\n- Is this review positive or negative?\n- What do people think about this product?\n- What do people think about this person or issue?\n- Analyse attitude trends over time\n\n\nNamed Entity Recognition\n\n\nAttempts to extract persons, organisations, locations, dates, monetary amounts, etc by looking at nouns and co-occurrence.\n\n\nEvent Extraction #=\n\n\nStep above NER. Tries to extract relational information like \"Company A is acquiring company B\".\n\n\nDocument Clustering\n\n\nApplications in automatic document organisation, topic extraction, and information retrieval or filtering. Use standard clustering techniques like hierarchical/agglomerative or KMeans. Dimensionality reduction techniques can be applied beforehand as well using lsi/truncated svd on tfidf/lda. The next problem is selecting descriptive human-readable labels for the clusters. There are a few different techniques that examine the contents of the documents per cluster to find a labeling that summarize the topic of each cluster to distinguish it from others. There is differential cluster labeling and internal cluster labeling.\n\n\nTo cluster:\n- tokenize\n- stem/lemmatize\n- stopwords/punctutation removal\n- tfidf or term frequencies\n- cluster\n- evaluate using metrics", 
            "title": "Concepts"
        }, 
        {
            "location": "/natural_language_processing/concepts/#bag-of-words-model", 
            "text": "Text is represented as the bag or multiset of its words. It disregards grammar and word order and only keeps the multiplicity of each word.  So \"John likes to watch movies. Mary likes movies too.\" is decomposed into \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"likes\",\"movies\",\"too\" and then the bag of words representation would look like  {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1} .", 
            "title": "bag of words model"
        }, 
        {
            "location": "/natural_language_processing/concepts/#tfidf", 
            "text": "Term frequency-inverse document frequency. Statistical measure to evaluate how important a word is to a document in a corpus. Importance increase proportionally to the number of times a word appears in a document but is offset by the frequency of the word in the corpus.  tfidf can be used in a search scheme in scoring and ranking a document's relevance to a query. The simplest ranking function is summing the tfidf for each query term.  It is successively used for stop word filtering.  Composed of two terms. Term frequency is the number of times the word appears in a document, divided by the total number of words in that document. Inverse document frequency is the logarithm of the number of documents in the corpus divided by the number of documents where the term appears.", 
            "title": "tfidf"
        }, 
        {
            "location": "/natural_language_processing/concepts/#latent-semantic-indexinganalysis", 
            "text": "Analyzes documents to find the underlying meaning or concepts of the documents. Basically SVD on tfidf/bow vectors.  Problem setting: Comparing words to find relevant documents, because we really want to do is compare meanings or concepts behind the words. LSI maps the words and documents into a vector space to do the comparison there. Concepts are obscured with different word choices, introducing noise. LSA filters out some of the noise to find the smallest set of concepts that span all the documents.  Idea:\n- Documents are represented as bags of words\n- Topics/concepts are represented as patterns of words that usually appear together in documents\n- Words are assumed to have a single meaning to make the problem tractable\n- Usually use tfidf on the bag of words befor performing LSI\n- Essentially performing SVD on the matrix of word counts  Application:\n- Cluster documents/words\n- Feature for classification", 
            "title": "latent semantic indexing/analysis"
        }, 
        {
            "location": "/natural_language_processing/concepts/#latent-dirichlet-analysis", 
            "text": "Represents documents as mixtures of topics that spit out words with certain probabilities. Assumes that documents are produced following:\n- N words in each document follow a distribution, maybe Poisson\n- There is a topic mixture in each document following a Dirichlet distribution over K topics\n- Each word in the document is generated by first picking a topic according to the multinomial distribution above. The topic then generates the word itself.\nLDA then tries to use the documents to infer the set of topics that generated them.  We have a corpus with K topics to discover a priori. Using collapsed Gibbs sampling, LDA:\n- goes through each document and randomly assigns each word to one of the K topics\n- for each word in each document and for each topic, compute the proportion of words in the document that are assigned to that topic and the proportion of assigned words to that topic over all documents that come from this word. We then reassign the word a different topic and compute the posterior. So we're updating the assignment of our current word using our model of how all the other words are assigned.\n- iterate until our assignments are stable.\n- Use these assignments to estimate the topic mixtures of each document using proportion of words assigned to each topic within that document and the words associated to each topic overall.  Essential usage is dimension reduction and uncovering the underlying themes in your corpora.", 
            "title": "latent Dirichlet analysis"
        }, 
        {
            "location": "/natural_language_processing/concepts/#word2vec", 
            "text": "Learns relationships between words essentially. Uses the context in which words are used to infer their similarities. Uses a neural network with few layers to predict the current word based on the context. We use the weights of the hidden layers as the embeddings or actual word vectors. We can then use cosine similarity or other distance metrics to compute word similarities or use the word vectors for classification/clustering. Also useful for sentiment analysis, where we use the similarity scores. Uses skip gram or continuous bag of words for these probability calculations.", 
            "title": "word2vec"
        }, 
        {
            "location": "/natural_language_processing/concepts/#doc2vec", 
            "text": "Similar idea to word2vec that extends the method to unsupervised learning of continuous representations for larger blocks of text like sentences, paragraphs, or entire documents. Again. clustering or similarity applications.", 
            "title": "doc2vec"
        }, 
        {
            "location": "/natural_language_processing/concepts/#word-movers-distance", 
            "text": "Submit a query and return the most relevant documents. Assess the distance between documents in a meaningful way, even when they have no words in common. It does use word2vec word embedding. Helpful that word vectors are normalized. Also uses euclidean distance instead of cosine similarity.", 
            "title": "Word Mover's Distance"
        }, 
        {
            "location": "/natural_language_processing/concepts/#topic-modelling-terminology", 
            "text": "If document classification is assigning a single category to a text, topic modelling is assigning multiple tags to a text.  Coherence: Compute sum of pairwise scores of top n words used to describe a topic. There are a few different measures of this. Good models generate coherent topics, ie, topics with high coherence scores.  Perplexity: Measure of how well a probability distribution or model predicts a sample. In LDA, topics are described by a probability distribution over vocabulary words. So it can be used to evaluate the topic-term distribution output. Good models have low perplexity.  Topic Difference: Calculates the distance between two topic models. It's calculated based on the topics either using their probability distributions over vocabulary words or using the common vocabulary words between the topics from both models. By increasing epochs in both models, the distance between identical topics should decrease.  Convergence: Sum of the difference between all identical topics from consecutive epochs. Models have converged when the convergence stops decreasing with increasing epochs.", 
            "title": "Topic Modelling Terminology"
        }, 
        {
            "location": "/natural_language_processing/concepts/#topic-modelling-uses", 
            "text": "text classification by grouping similar words together  recommendation systems: Use similarity measures to build recommender systems that recommend text with a topic structure similar to what our current text  Uncovering themes in text", 
            "title": "Topic Modelling Uses"
        }, 
        {
            "location": "/natural_language_processing/concepts/#dynamic-topic-modeling", 
            "text": "", 
            "title": "Dynamic Topic Modeling"
        }, 
        {
            "location": "/natural_language_processing/concepts/#author-topic-modeling", 
            "text": "", 
            "title": "Author-Topic Modeling"
        }, 
        {
            "location": "/natural_language_processing/concepts/#sentiment-analysis", 
            "text": "Analyse the opinion or tone of text. Polarity analysis identifies tones as positive or negative. Categorisation can in addition identify confused or angry. There is also an emotion scale 'sad' to 'happy' and from 0-10.  Can attempt to answer questions such as:\n- Is this review positive or negative?\n- What do people think about this product?\n- What do people think about this person or issue?\n- Analyse attitude trends over time", 
            "title": "Sentiment Analysis"
        }, 
        {
            "location": "/natural_language_processing/concepts/#named-entity-recognition", 
            "text": "Attempts to extract persons, organisations, locations, dates, monetary amounts, etc by looking at nouns and co-occurrence.", 
            "title": "Named Entity Recognition"
        }, 
        {
            "location": "/natural_language_processing/concepts/#event-extraction", 
            "text": "Step above NER. Tries to extract relational information like \"Company A is acquiring company B\".", 
            "title": "Event Extraction #="
        }, 
        {
            "location": "/natural_language_processing/concepts/#document-clustering", 
            "text": "Applications in automatic document organisation, topic extraction, and information retrieval or filtering. Use standard clustering techniques like hierarchical/agglomerative or KMeans. Dimensionality reduction techniques can be applied beforehand as well using lsi/truncated svd on tfidf/lda. The next problem is selecting descriptive human-readable labels for the clusters. There are a few different techniques that examine the contents of the documents per cluster to find a labeling that summarize the topic of each cluster to distinguish it from others. There is differential cluster labeling and internal cluster labeling.  To cluster:\n- tokenize\n- stem/lemmatize\n- stopwords/punctutation removal\n- tfidf or term frequencies\n- cluster\n- evaluate using metrics", 
            "title": "Document Clustering"
        }, 
        {
            "location": "/natural_language_processing/scratch/", 
            "text": "About\n\n\nThis wiki is for natural language processing methods and ideas.\n\n\nPackages\n\n\n\n\ngensim\n\n\nnltk\n\n\nspacy\n\n\ntextblob\n\n\n\n\nGeneral\n\n\n\n\ntext summarization (different algorithms than bag of words)\n\n\nbag of words\n\n\ntf-idf\n\n\nword2vec\n\n\ndoc2vec\n\n\nngram\n\n\ntopic modeling\n\n\ntopic segmentation\n\n\nlda/lsi\n\n\ntext tiling\n\n\nwordclouds\n\n\nkeyword/keyphrase extraction\n\n\nGensim for modeling and named entity stuff. First stream tokenized text and then construct phrases and then from there train a word2vec/doc2vec model\n\n\ngensim phrases model\n\n\nsvm text classification on tfidf vectors\n\n\nsklearn has a couple good text articles\n\n\nreally good tutorial on text svm classification at \nhttps://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n\n\ncan also do lds/lsi/neuralnetwork/naive bayes on these results from tfidf, clustering too for unsupervised problems (example on sklearn)\n\n\ngensim has common phrases utility to be included in phrases function, so if we are bigramming and want to to identify words in a common sequence like 'of' in 'secretary of state' without trigramming.\n\n\nGrid search best topic models from \nhttps://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n\n\nPossible method to be used in searching a large corpus of documents for keywords and returning the documents where the probability of certain topics is highest: \nhttps://stackoverflow.com/questions/15377290/unsupervised-automatic-tagging-algorithms\n\n\nLooking at most important words in SVM text classification \nhttps://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d\n\n\ndocument/word clustering\n\n\nsklearn text data examples\n\n\nlda with specific solvers is lsa/lsi\n\n\nsimilarly truncted svd with term matrices is lsa/lsi\n\n\nmachine learning plus has a couple really good tutorials on lda\n\n\ngensim has some tutorials too\n\n\nnlp for hackers text cluster example recipe\n\n\nnormalize/smooth tfidf before nmf/pca\n\n\nsklearn text examples/snippets\n\n\ngensim examples/snippets\n\n\ntfidftransformer vs tfidfvectorizer and their application on countvectorizer\n\n\ndocument clustering/classification\n\n\ngensim + keras text classification using lstm\n\n\ngensim has many github tutorials\n\n\nfasktext\n\n\nlegal dataset from lawinsider for document clustering/classification\n\n\nword counts/wordclouds/similarities/ngramming(phrases)/sentiment/clustering/classification/lda/lsa(lsi)\n\n\nstemming/tfidf\n\n\naveraging word vectors\n\n\ndeep inverse regression with yelp reviews\n\n\nuse pretrained word vectors like Google's, GloVe, stanford, etc.\n\n\njusText, NLTK, Pattern, TextBlob\n\n\nhttps://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks\n\n\ncorrelated topics models\n\n\nslda\n\n\ngensim streaming data pipeline\n\n\ncan add words to dictionaries after computation\n\n\ncan load model and retrain\n\n\nget example uses of word2vec/doc2vec/word/doc/vector usage in unsupervised problems\n\n\nunsupervised text analytics\n\n\nintroduction to information retrieval, modern information retrieval\n\n\nhttps://github.com/mattdennewitz/playlist-to-vec\n\n\ntext tiling\n\n\ntext rank/page rank\n\n\nextractive summarization\n\n\nnltk book\n\n\ndiscourse analysis - detecting topic shifts\n\n\ndepth scoring\n\n\nhttps://www-nlpir.nist.gov/projects/duc/data.html\n\n\nannoy indexer - search for points in space that are close to given query point\n\n\ndynamic topic modeling\n\n\nstemming, lemmitization, parts of speach tagging, ner, sentiment\n\n\npassage retrieval\n\n\nquestion answering\n\n\njrc-acquis dataset for information retrieval\n\n\ntrec9 qa corpus\n\n\nhttps://hackernoon.com/reading-100k-newspapers-in-20-vernacular-languages-in-india-4e859c468a57", 
            "title": "Scratch"
        }, 
        {
            "location": "/natural_language_processing/scratch/#about", 
            "text": "This wiki is for natural language processing methods and ideas.", 
            "title": "About"
        }, 
        {
            "location": "/natural_language_processing/scratch/#packages", 
            "text": "gensim  nltk  spacy  textblob", 
            "title": "Packages"
        }, 
        {
            "location": "/natural_language_processing/scratch/#general", 
            "text": "text summarization (different algorithms than bag of words)  bag of words  tf-idf  word2vec  doc2vec  ngram  topic modeling  topic segmentation  lda/lsi  text tiling  wordclouds  keyword/keyphrase extraction  Gensim for modeling and named entity stuff. First stream tokenized text and then construct phrases and then from there train a word2vec/doc2vec model  gensim phrases model  svm text classification on tfidf vectors  sklearn has a couple good text articles  really good tutorial on text svm classification at  https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a  can also do lds/lsi/neuralnetwork/naive bayes on these results from tfidf, clustering too for unsupervised problems (example on sklearn)  gensim has common phrases utility to be included in phrases function, so if we are bigramming and want to to identify words in a common sequence like 'of' in 'secretary of state' without trigramming.  Grid search best topic models from  https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/  Possible method to be used in searching a large corpus of documents for keywords and returning the documents where the probability of certain topics is highest:  https://stackoverflow.com/questions/15377290/unsupervised-automatic-tagging-algorithms  Looking at most important words in SVM text classification  https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d  document/word clustering  sklearn text data examples  lda with specific solvers is lsa/lsi  similarly truncted svd with term matrices is lsa/lsi  machine learning plus has a couple really good tutorials on lda  gensim has some tutorials too  nlp for hackers text cluster example recipe  normalize/smooth tfidf before nmf/pca  sklearn text examples/snippets  gensim examples/snippets  tfidftransformer vs tfidfvectorizer and their application on countvectorizer  document clustering/classification  gensim + keras text classification using lstm  gensim has many github tutorials  fasktext  legal dataset from lawinsider for document clustering/classification  word counts/wordclouds/similarities/ngramming(phrases)/sentiment/clustering/classification/lda/lsa(lsi)  stemming/tfidf  averaging word vectors  deep inverse regression with yelp reviews  use pretrained word vectors like Google's, GloVe, stanford, etc.  jusText, NLTK, Pattern, TextBlob  https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks  correlated topics models  slda  gensim streaming data pipeline  can add words to dictionaries after computation  can load model and retrain  get example uses of word2vec/doc2vec/word/doc/vector usage in unsupervised problems  unsupervised text analytics  introduction to information retrieval, modern information retrieval  https://github.com/mattdennewitz/playlist-to-vec  text tiling  text rank/page rank  extractive summarization  nltk book  discourse analysis - detecting topic shifts  depth scoring  https://www-nlpir.nist.gov/projects/duc/data.html  annoy indexer - search for points in space that are close to given query point  dynamic topic modeling  stemming, lemmitization, parts of speach tagging, ner, sentiment  passage retrieval  question answering  jrc-acquis dataset for information retrieval  trec9 qa corpus  https://hackernoon.com/reading-100k-newspapers-in-20-vernacular-languages-in-india-4e859c468a57", 
            "title": "General"
        }, 
        {
            "location": "/python/environments/", 
            "text": "Usage\n\n\nTo resolve package conflict issues, it's best to make a separate Python environment for each project. This way, we can explicitly specify our package versions and not worry about global package or package version conflicts.\n\n\nCreate a directory for your project and then create the virtual environment with \nvirtualenv -p python3 targetdirectory\n\n\nActivate the environment with \nsource targetdirectory/bin/activate\n\n\nFrom here, install packages using \npip3 install packagename\n\n\nTo then make a file with all installed packages and their exact version numbers do \npip3 freeze -r \n requirements.txt\n\n\nThen when creating a new environment and want to install the same exact packages, use \npip install -r requirements.txt\n\n\nLXC Containers\n\n\nLXC Ubuntu containers should have Python 3.5 installed, but PIP will also need to be installed by install locales with \nlocale-gen en_US.UTF-8\n and then \napt install python3-pip\n.\n\n\nInstalling from a git repository\n\n\n`pip install", 
            "title": "Environments"
        }, 
        {
            "location": "/python/environments/#usage", 
            "text": "To resolve package conflict issues, it's best to make a separate Python environment for each project. This way, we can explicitly specify our package versions and not worry about global package or package version conflicts.  Create a directory for your project and then create the virtual environment with  virtualenv -p python3 targetdirectory  Activate the environment with  source targetdirectory/bin/activate  From here, install packages using  pip3 install packagename  To then make a file with all installed packages and their exact version numbers do  pip3 freeze -r   requirements.txt  Then when creating a new environment and want to install the same exact packages, use  pip install -r requirements.txt", 
            "title": "Usage"
        }, 
        {
            "location": "/python/environments/#lxc-containers", 
            "text": "LXC Ubuntu containers should have Python 3.5 installed, but PIP will also need to be installed by install locales with  locale-gen en_US.UTF-8  and then  apt install python3-pip .", 
            "title": "LXC Containers"
        }, 
        {
            "location": "/python/environments/#installing-from-a-git-repository", 
            "text": "`pip install", 
            "title": "Installing from a git repository"
        }, 
        {
            "location": "/python/packages/", 
            "text": "NumPy\n\n\nSciPy\n\n\npandas\n\n\nmatplotlib\n\n\nSeaborn\n\n\nHistograms, density, plots, etc. Easier plotting of common statistical plots using dataframes.\n\n\nscikit-learn\n\n\nMachine learning toolkit. Consistent syntax for various classification and regression techniques.\n\n\nStatsModels\n\n\nCompared to scikit-learn, more statistically oriented with output of p-values, etc. Output similar to \nR\n statistical tests. Contains many statistical tests not found in scipy.stats or scikit-learn.\n\n\nTensorFlow\n\n\nDeep learning library, uses GPU for acceleration.\n\n\nKeras\n\n\nWrapper for TensorFlow with common utilities.\n\n\nXGBoost\n\n\nPopular library for implementing boosted trees. Faster than scikit-learn implementation. Can offload on GPU.\n\n\nPyODBC\n\n\nDatabase connection utility.\n\n\nsmtplib\n\n\nUsed for sending emails given a SMTP server.\n\n\nMultiprocessing\n\n\nRelatively straightforward to use parallel processing module. Most used components are \nPool, map, starmap\n.\n\n\nscikit-image\n\n\nImage processing module with various utilities for transforming images\n\n\nNumba\n\n\nJust-in-time compiler for array-oriented, math-heavy python code. Can offload calculations to GPU.\n\n\nPySpark\n\n\nSpark connector for python. Offers modelling modules as well.\n\n\nlocust\n\n\nHTTP server load testing tool, useful for testing APIs.\n\n\nflask/flask_redis/flask_api\n\n\nMicroservice for writing web applications and web APIs. Redis module is for server-side caching.\n\n\ntqdm\n\n\nUseful progress bar for looping operations\n\n\nhue\n\n\nPrettier terminal printing\n\n\nRequests\n\n\nHTTP post/get operations for web servers\n\n\nretrying\n\n\nUseful for retrying connections to web servers, database servers, etc.\n\n\nshapely\n\n\nPython geometry library for pip tests, overlapping polygons, etc.\n\n\nPickle\n\n\npython serialization module\n\n\nhashlib\n\n\nnltk\n\n\nNLP library\n\n\nGensim\n\n\nNLP library with Doc2Vec\n\n\nSpaCy\n\n\nHigh performance NLP library\n\n\nLogging\n\n\nlogging module\n\n\nre\n\n\nRegular expressions module\n\n\nshutil\n\n\nShell utilities, like copying files from one directory to another. Somewhat overlaps with os module\n\n\npytesseract\n\n\nOCR of images\n\n\ntextract\n\n\nAble to read 'text' PDFs, docx files, etc.\n\n\nopencv-python\n\n\ntox\n\n\ntest suite for programs\n\n\ntempfile\n\n\ncreate temporary files/directories\n\n\nfaker\n\n\ncreate fake data like names, addresses, phone numbers text for population of databases or testing\n\n\njupyter widgets\n\n\nnot quite R Shiny\n\n\ndateutil\n\n\nAdditional datetime functionality, like relative deltas\n\n\nlimeml\n\n\nexplain predictions of classifiers graphically - have not looked into more\n\n\npyowm\n\n\nPython wrapper for OpenWeatherMap API\n\n\nplotly\n\n\nInteractive graphics. Complicated syntax and need API\n\n\nBokeh\n\n\nSlightly interactive graphics\n\n\nnose\n\n\nsimilar testing to tox\n\n\nast\n\n\nabstract syntax trees. Turn a string of a command into the actual command.\n\n\naffinity\n\n\nsometimes os parallel processes are weird\n\n\nitertools\n\n\nlots of useful functions in here, repea and chain.from_iterable to unpack nested lists\n\n\njoblib\n\n\nmore advanced distributed computing\n\n\nscrapy\n\n\nweb scraper\n\n\ngensim sklearn api\n\n\njupyter notebook slides\n\n\npipenv\n\n\npip + venv\n\n\nopencv-python\n\n\ndash, plotly, bokeh\n\n\nrise is jupyter notebook slideshows\n\n\nreportlab\n\n\njinja2\n\n\noptparse\n\n\npyqt\n\n\nitertools dropwhile, takewhile\n\n\npprint\n\n\ndocker\n\n\nwarnings\n\n\nhugraphs\n\n\ntkinter\n\n\nlightgbm\n\n\nglob - good for listing files in nested subdirectories than os.walk\n\n\nsqlalchemy - another sql agent\n\n\nuswgi web server\n\n\ncassandra\n\n\nsqlalchemy\n\n\nkafka\n\n\npyproj - cartographic transformations and geo computation\n\n\npython dataclasses \nhttps://realpython.com/python-data-classes/#type-hints\n\n\ngeopandas\n\n\nSubprocess.Popen\n\n\nogr2ogr\n\n\nlxml\n\n\nmicropython\n\n\ncircuitpython\n\n\ndateutil.relativedelta\n\n\ntox\n\n\nsqlite3\n\n\nmechanical soup\n\n\npypiserver\n\n\nhttps://pypiserver.readthedocs.io/en/latest/\n\n\nhttps://github.com/pypiserver/pypiserver#quickstart-installation-and-usage\n\n\nhttps://github.com/dexterous/pypiserver-on-the-cloud\n\n\nautogui\n\n\nwxPython\n\n\nwerkzeug - flask dependency with password hashing utilities\n\n\nflask-login\n\n\nunittest\n\n\nflashtext\n\n\nHigh performance keyword searching library, scales linearly with number of terms, preferred method over regular expressions for exact word searches\n\n\npyprind like tqp progress bar\n\n\nperfplot performance plots for python code\n\n\ndash\n\n\npipenv alternative to both pip and virtualenv usage\n\n\npyflux time series\n\n\nfire generate cli for any project\n\n\nimbalanced learn\n\n\npsutil for montoring processes like command line ps tool\n\n\npomegranete - probalistic and graphical models, mixture models, hierarchical\n\n\nflashtext\n\n\nautocorrect\n\n\ntextblob\n\n\nspacy-universe\n\n\nlexnlp\n\n\nasyncio\n\n\nuuid for generating universally unique identifiers\n\n\ntraceback\n\n\npydoct for marking graphs, interface to graphviz in pure Python\n\n\neli5, visualize and debug models and track work of algorithm step by step, supports sklearn, xgboost, lightgbm, lightning, sklearn-crfsuite\n\n\ndist-keras for distributed keras, elphas, spark-deep-learning\n\n\nscrapy\n\n\nbokeh, plotly\n\n\nhug, bottle flask alternatives?\n\n\npraw, python reddit api wrapper\n\n\nMIME, combine with smtplib for email functionality\n\n\nclick\n\n\npython jenkins\n\n\nautograph\n\n\nyellobrick - machine learning visualization library\n\n\npyldavis - visualize lda", 
            "title": "Packages"
        }, 
        {
            "location": "/python/scratch/", 
            "text": "text tags\n\n\nsorting algorithms/techniques\n\n\nsetup tools\n\n\nmap, filter\n\n\nsorted(zip()) for sort one list byanother list\n\n\ncollections.Counter\n\n\nglobal vars\n\n\nnext\n\n\niter\n\n\nitertools\n\n\ncombinations\n\n\nmaybe put repeat in second argument during starmap calls, problem at work not parallelizing this\n\n\ncontinuously write to log files,temporary files for progress, grid search iterations completed, loop progress, etc.\n\n\nyaml, pyqt, pathlib\n\n\ndocstrings for auto documentation using sphinx\n\n\ntornado web server, gunicorn\n\n\nsetuptools for making setup.py files\n\n\nsmooth plots with exponential moving average\n\n\nset(list) to get unique list elements\n\n\nuse tox (integrate with testing in jenkins, travis-ci)\n\n\nuse threading within class (like Ryan's automodeler)\n\n\nflask + gunicorn(maybe tornado) for asynchronous application\n\n\nflask gui with api\n\n\nonly learn flask enough for api development after the two tutorials\n\n\njupyter notebook magic commands\n\n\npprint has stream to file option\n\n\nFlask application database storage, populate once at startup?\n\n\nflask test coverage page for unit tests\n\n\nlots of flask tutorials on digital ocean\n\n\ngunicorn/celery/tornado/pyramid web servers for flask/django\n\n\nreverse proxy flask applications\n\n\nsort one list by another\n\n\nfunction within function definitions\n\n\nf strings for better formatting instead of \n%\n or \n.format\n calls\n\n\nwarnings.filterwarnings(action=...,cat=...,module=...)\n\n\ncheckpointing long running programs\n\n\nsqlite3 databases\n\n\nnumba for offloading calculations on gpu\n\n\nmap/filter\n\n\nmultiprocessing shared objects\n\n\nimporting packages and modules correctly?\n\n\ncheckpoint programs using logging\n\n\ncython?\n\n\nhttps://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-vii-error-handling\n the debugging section fake email server is pretty cool, easy directions on using your gmail too.\n\n\nlinting/coverage\n\n\nclass static methods are just class methods that dont require the self arguments\n\n\nlogging smtphandler for sending emails\n\n\npandas iterrows for times where we want to iterate over df instead of apply/map\n\n\npandas reset_index for getting single index df after groupby\n\n\nhttps://stackoverflow.com/questions/15268953/how-to-install-python-package-from-github\n\n\nfilter/map/reduce\n\n\nhttps://stackoverflow.com/questions/5442910/python-multiprocessing-pool-map-for-multiple-arguments\n\n\nhttp://www.racketracer.com/2016/07/06/pandas-in-parallel/\n\n\nhttp://blog.adeel.io/2016/11/06/parallelize-pandas-map-or-apply/\n\n\nhttps://stackoverflow.com/questions/5442910/python-multiprocessing-pool-map-for-multiple-arguments/5443941#5443941\n\n\nhttps://github.com/mkleehammer/pyodbc/wiki/Getting-started\n\n\nhttps://stackoverflow.com/questions/35905335/aggregation-over-partition-pandas-dataframe\n - pandas transform method for doing a groupby and then say wanting column means\n\n\nf strings\n\n\ninline if statements without colons\n\n\ncircular imports when modules depend on another for objects\n\n\ninit\n.py imports like flask tutorial versus blank file\n\n\nnever for loop and change values in a dictionary. it becomes immutable, always make a copy in the for loop to set the values needed.\n\n\nsuper() for class inheritance \nhttps://stackoverflow.com/questions/222877/what-does-super-do-in-python\n\n\nhttps://medium.freecodecamp.org/a-guide-to-asynchronous-programming-in-python-with-asyncio-232e2afa44f6\n\n\nsmtphandler in logging for email\n\n\npython has internal webserver for testing things, probably want one on the network for little email support or get google api key and some alternative to the plain text.\n\n\nproper use of \ninit\n.py, either blank or module imports like flask app\n\n\ndataclasses for simpler class constructs\n\n\nhttps://rise.cs.berkeley.edu/blog/pandas-on-ray-early-lessons/\n parallel pandas\n\n\nhttp://takluyver.github.io/posts/so-you-want-to-write-a-desktop-app-in-python.html\n\n\npandas.series.isin for testing multiple equalities\n\n\nnp.where for conditional column settings\n\n\nlambda x: x if condition else not x\n\n\npandas pivot tables for making categorical tables\n\n\npandas cross tab for making categorical count tables", 
            "title": "Scratch"
        }, 
        {
            "location": "/python/django/general/", 
            "text": "http://www.obeythetestinggoat.com/", 
            "title": "General"
        }, 
        {
            "location": "/python/flask/general/", 
            "text": "https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world\n\n\nhttps://exploreflask.com/en/latest/\n\n\nhttps://pythonhosted.org/Flask-Bootstrap/\n\n\nhttp://flask.pocoo.org/\n\n\nhttps://flask-wtf.readthedocs.io/en/stable/\n\n\nhttp://jinja.pocoo.org/docs/2.10/\n\n\nhttps://www.w3schools.com/html/\n\n\nhttps://www.w3schools.com/Css/\n\n\nhttps://www.w3schools.com/bootstrap/bootstrap_get_started.asp", 
            "title": "General"
        }, 
        {
            "location": "/python/gensim/usage/", 
            "text": "Gensim Usage\n\n\nHigh performance library for natural language processing. Efficiently stream files for processing instead of loading entire corpus into memory. Possible to save existing models, load and continue training on new data without needing to retrain on entire corpus. Tools include bag of words representations, word2vec embeddings with similarity queries, doc2vec embeddings with similarity queries, text summaries, keyword extraction, lsi, lda, hpd, phrases module for identifying phrases like secretary-of-state for preprocessing, lots of preprocessing utilities for cleaning text. Stemming and lemmatization utilities only work with Python 2.7, NLTK will have to be used otherwise. Gensim proclaims their versions of these utilities as being the superior option. Also provides API integration for Scikit-Learn, though we probably won't use those tools often.\n\n\nLinks\n\n\n\n\nhttps://radimrehurek.com/gensim/tutorial.html\n\n\nhttps://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks\n\n\nhttps://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials\n\n\nhttps://github.com/jxieeducation/DIY-Data-Science/blob/master/frameworks/gensim.md#advanced-features\n\n\nhttps://github.com/RaRe-Technologies/gensim/blob/cc74b668ccbbfd558d5a54050c4489e6e06fed3d/docs/notebooks/gensim_news_classification.ipynb\n\n\n\n\ngeneral cleaner\n\n\nWrites multiple times but this is so we can apply clean/phrase/model fitting.\n\n\nimport re\n\nimport string\n\nfrom gensim.parsing.preprocessing import remove_stopwords, strip_numeric, stem_text\n\nfrom gensim.models import Phrases, Word2Vec\n\nfrom gensim.models.phrases import Phraser\n\nfrom gensim.corpora import Dictionary\n\n\n\ndef write_original_messages(df):\n\n\noriginal message writing before cleaning\n\n\nwith open('original.txt', 'w', encoding = 'utf-8') as outfile:\n\nfor message in df['Message']:\n\noutfile.write(' '.join(message.strip().split()) + '\\n')\n\nreturn 1\n\n\n\nclass OriginalCorpus(object):\n\n\nstream original messages to bigram\n\n\ndef __iter__(self):\n\nfor line in open('original.txt', 'r', encoding = 'utf-8'):\n\nyield line.split()\n\n\n\ndef write_bigram_messages(bigram):\n\n\nuse existing bigram on original messages and rewrite\n\n\nwith open('bigram.txt', 'w', encoding = 'utf-8') as outfile:\n\nfor line in open('original.txt', 'r', encoding = 'utf-8'):\n\noutfile.write(' '.join(bigram[line.split()]) + '\\n')\n\nreturn 1\n\n\n\ndef write_clean_messages():\n\n\nstream messages, clean, and rewrite to another file\n\n\nremove = string.punctuation\n\nremove = remove.replace('_', \n)\n\npattern = r\n[{}]\n.format(remove)\n\nwith open('clean.txt', 'w', encoding = 'utf-8') as outfile:\n\n# for message in open('messages.txt', 'r', encoding = 'utf-8'):\n\nfor message in open('bigram.txt', 'r', encoding = 'utf-8'):\n\n# alphanum = strip_non_alphanum(message) # punctuation\n\nalphanum = re.sub(pattern, \n \n, message) # punctuation except for _ for bigrams\n\nnonumeric = strip_numeric(alphanum) # numeric\n\nnostops = remove_stopwords(nonumeric) # stopwords\n\noutfile.write(' '.join(nostops.lower().strip().split()) + '\\n')\n\nreturn 1\n\n\n\nclass CleanCorpus(object):\n\n\nstream bigram clean messages\n\n\ndef __iter__(self):\n\nfor line in open('clean.txt', 'r', encoding = 'utf-8'):\n\nyield line.split()\n\n\n\ndef construct_dictionary():\n\ndictionary = gensim.corpora.Dictionary(line.lower().split() for line in open('clean.txt', 'r', encoding = 'utf-8'))\n\n# stoplist = set('for a of the and to in'.split())\n\n# stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n\n# if stopword in dictionary.token2id]\n\nonce_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n\n# dictionary.filter_tokens(stop_ids + once_ids)\n\ndictionary.filter_tokens(once_ids)\n\ndictionary.compactify()\n\n# print(dictionary)\n\n# print(dictionary.token2id)\n\n# dictionary.save('dictionaryname.dict')\n\n\n\nif __name__ == '__main__':\n\ndf = load_data()\n\nresult = write_original_messages(df)\n\ncorpus_stream = OriginalCorpus()\n\nphrases = Phrases(corpus_stream)\n\nbigram = Phraser(phrases)\n\nresult = write_bigram_messages(bigram)\n\nresult = write_clean_messages()\n\ncorpus_stream = CleanCorpus()\n\nmodel = Word2Vec(corpus_stream, workers = 4)\n\nmodel.save('w2v_model')\n\n# model.train(another_corpus_stream)\n\ntry:\n\nprint(model.wv.most_similar('renewal'))\n\nprint(model.wv.doesnt_match('renewal rnwl rwlrev rev changed'.split()))\n\nprint(model.wv['review'])\n\n# probability of a text under a model\n\nprint(model.score(['this house haha'.split()]))\n\nexcept Exception as e:\n\nprint(str(e))", 
            "title": "Usage"
        }, 
        {
            "location": "/python/gensim/usage/#gensim-usage", 
            "text": "High performance library for natural language processing. Efficiently stream files for processing instead of loading entire corpus into memory. Possible to save existing models, load and continue training on new data without needing to retrain on entire corpus. Tools include bag of words representations, word2vec embeddings with similarity queries, doc2vec embeddings with similarity queries, text summaries, keyword extraction, lsi, lda, hpd, phrases module for identifying phrases like secretary-of-state for preprocessing, lots of preprocessing utilities for cleaning text. Stemming and lemmatization utilities only work with Python 2.7, NLTK will have to be used otherwise. Gensim proclaims their versions of these utilities as being the superior option. Also provides API integration for Scikit-Learn, though we probably won't use those tools often.", 
            "title": "Gensim Usage"
        }, 
        {
            "location": "/python/gensim/usage/#links", 
            "text": "https://radimrehurek.com/gensim/tutorial.html  https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks  https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials  https://github.com/jxieeducation/DIY-Data-Science/blob/master/frameworks/gensim.md#advanced-features  https://github.com/RaRe-Technologies/gensim/blob/cc74b668ccbbfd558d5a54050c4489e6e06fed3d/docs/notebooks/gensim_news_classification.ipynb", 
            "title": "Links"
        }, 
        {
            "location": "/python/gensim/usage/#general-cleaner", 
            "text": "Writes multiple times but this is so we can apply clean/phrase/model fitting.  import re\n\nimport string\n\nfrom gensim.parsing.preprocessing import remove_stopwords, strip_numeric, stem_text\n\nfrom gensim.models import Phrases, Word2Vec\n\nfrom gensim.models.phrases import Phraser\n\nfrom gensim.corpora import Dictionary\n\n\n\ndef write_original_messages(df): original message writing before cleaning \n\nwith open('original.txt', 'w', encoding = 'utf-8') as outfile:\n\nfor message in df['Message']:\n\noutfile.write(' '.join(message.strip().split()) + '\\n')\n\nreturn 1\n\n\n\nclass OriginalCorpus(object): stream original messages to bigram \n\ndef __iter__(self):\n\nfor line in open('original.txt', 'r', encoding = 'utf-8'):\n\nyield line.split()\n\n\n\ndef write_bigram_messages(bigram): use existing bigram on original messages and rewrite \n\nwith open('bigram.txt', 'w', encoding = 'utf-8') as outfile:\n\nfor line in open('original.txt', 'r', encoding = 'utf-8'):\n\noutfile.write(' '.join(bigram[line.split()]) + '\\n')\n\nreturn 1\n\n\n\ndef write_clean_messages(): stream messages, clean, and rewrite to another file \n\nremove = string.punctuation\n\nremove = remove.replace('_',  )\n\npattern = r [{}] .format(remove)\n\nwith open('clean.txt', 'w', encoding = 'utf-8') as outfile:\n\n# for message in open('messages.txt', 'r', encoding = 'utf-8'):\n\nfor message in open('bigram.txt', 'r', encoding = 'utf-8'):\n\n# alphanum = strip_non_alphanum(message) # punctuation\n\nalphanum = re.sub(pattern,    , message) # punctuation except for _ for bigrams\n\nnonumeric = strip_numeric(alphanum) # numeric\n\nnostops = remove_stopwords(nonumeric) # stopwords\n\noutfile.write(' '.join(nostops.lower().strip().split()) + '\\n')\n\nreturn 1\n\n\n\nclass CleanCorpus(object): stream bigram clean messages \n\ndef __iter__(self):\n\nfor line in open('clean.txt', 'r', encoding = 'utf-8'):\n\nyield line.split()\n\n\n\ndef construct_dictionary():\n\ndictionary = gensim.corpora.Dictionary(line.lower().split() for line in open('clean.txt', 'r', encoding = 'utf-8'))\n\n# stoplist = set('for a of the and to in'.split())\n\n# stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n\n# if stopword in dictionary.token2id]\n\nonce_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n\n# dictionary.filter_tokens(stop_ids + once_ids)\n\ndictionary.filter_tokens(once_ids)\n\ndictionary.compactify()\n\n# print(dictionary)\n\n# print(dictionary.token2id)\n\n# dictionary.save('dictionaryname.dict')\n\n\n\nif __name__ == '__main__':\n\ndf = load_data()\n\nresult = write_original_messages(df)\n\ncorpus_stream = OriginalCorpus()\n\nphrases = Phrases(corpus_stream)\n\nbigram = Phraser(phrases)\n\nresult = write_bigram_messages(bigram)\n\nresult = write_clean_messages()\n\ncorpus_stream = CleanCorpus()\n\nmodel = Word2Vec(corpus_stream, workers = 4)\n\nmodel.save('w2v_model')\n\n# model.train(another_corpus_stream)\n\ntry:\n\nprint(model.wv.most_similar('renewal'))\n\nprint(model.wv.doesnt_match('renewal rnwl rwlrev rev changed'.split()))\n\nprint(model.wv['review'])\n\n# probability of a text under a model\n\nprint(model.score(['this house haha'.split()]))\n\nexcept Exception as e:\n\nprint(str(e))", 
            "title": "general cleaner"
        }, 
        {
            "location": "/python/sklearn/scratch/", 
            "text": "feature union\n\n\ngridsearchcv and pipelines\n\n\nfit vs fit_transform\n\n\ntransforming data using training set parameters\n\n\ndecision function/predicted probabilities attributes\n\n\nfeature importance\n\n\nlearning curves (model performance as a function of dataset size)\n\n\ncontour plots of predictions\n\n\nfeature selection model based vs iterative examples", 
            "title": "Scratch"
        }, 
        {
            "location": "/rlang/packages/", 
            "text": "tidyverse (dplyr, ggplot2, readr, httr, etc)\n\n\nstringr\n\n\nlubridate\n\n\nglmnet\n\n\nparallel\n\n\nrmarkdown\n\n\nshiny, flexdashboard, shinydashboard\n\n\nbookdown\n\n\nroxygen for documentation", 
            "title": "Packages"
        }, 
        {
            "location": "/rlang/scratch/", 
            "text": "links\n\n\nhttps://github.com/qinwf/awesome-R\n\n\nPython integrations\n\n\nreticulate\n is the R interface for Python and provides multiple sorts of functionality. We can call Python from R, access objects, use virtual environments, use inline Python in R Markdown documents, source Python from R scripts, and more. Accessing Python objects is still a little awkward, but we can now much easier source Python programs with arguments than using a \nsystem\n call. Primary use case would be using inline Python code in Shiny applications since Jupyter notebooks allow interactive running of chunks and R Markdown does not.\n\n\nhttps://github.com/rstudio/reticulate\n\n\nPip-like functionality\n\n\npackrat\n is the dependency management system similar to PIP.\n\n\nhttps://rstudio.github.io/packrat/\n\n\ndatabase connections\n\n\nlibrary(RODBC)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\ncnxn \n- odbcConnect(\nMYSQLSERVERDSN01\n) # credentials optional\nresult \n- sqlQuery(cnxn, 'select * from mytable')\n\n# there are bugs here for end of month calculations\n# this code should fix that issue\nend_date \n- ceiling_date(as.Date(Sys.Date()) %m-% months(6), unit = \nmonth\n) - days(1)\n\n# this is how we do relative queries\nresult \n- sqlQuery(cnxn, paste0(\nselect * from mytable where date\n='\n, as.character(end_date),\n'\n))", 
            "title": "Scratch"
        }, 
        {
            "location": "/rlang/scratch/#links", 
            "text": "https://github.com/qinwf/awesome-R", 
            "title": "links"
        }, 
        {
            "location": "/rlang/scratch/#python-integrations", 
            "text": "reticulate  is the R interface for Python and provides multiple sorts of functionality. We can call Python from R, access objects, use virtual environments, use inline Python in R Markdown documents, source Python from R scripts, and more. Accessing Python objects is still a little awkward, but we can now much easier source Python programs with arguments than using a  system  call. Primary use case would be using inline Python code in Shiny applications since Jupyter notebooks allow interactive running of chunks and R Markdown does not.  https://github.com/rstudio/reticulate", 
            "title": "Python integrations"
        }, 
        {
            "location": "/rlang/scratch/#pip-like-functionality", 
            "text": "packrat  is the dependency management system similar to PIP.  https://rstudio.github.io/packrat/", 
            "title": "Pip-like functionality"
        }, 
        {
            "location": "/rlang/scratch/#database-connections", 
            "text": "library(RODBC)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\ncnxn  - odbcConnect( MYSQLSERVERDSN01 ) # credentials optional\nresult  - sqlQuery(cnxn, 'select * from mytable')\n\n# there are bugs here for end of month calculations\n# this code should fix that issue\nend_date  - ceiling_date(as.Date(Sys.Date()) %m-% months(6), unit =  month ) - days(1)\n\n# this is how we do relative queries\nresult  - sqlQuery(cnxn, paste0( select * from mytable where date =' , as.character(end_date), ' ))", 
            "title": "database connections"
        }, 
        {
            "location": "/rlang/shiny/", 
            "text": "Scratch\n\n\n\n\nrenderUI for giving reactive choices based on one user input parameter to another user input parameter.\n\n\nclosely related is the updateSelectInput/updateSelectizeInput, but wrapping a selectInput with a renderUI is easier to use, especially in flexdashboard versus Shiny.\n\n\nselectizeInput for nicer drop down menus with auto complete, based on javascript library that's very sharp.\n\n\nwithProgress\n\n\nshinycssloaders withSpinner\n\n\nreactiveFileReader\n\n\nisolate to only reload reactive values when they change", 
            "title": "Shiny"
        }, 
        {
            "location": "/rlang/shiny/#scratch", 
            "text": "renderUI for giving reactive choices based on one user input parameter to another user input parameter.  closely related is the updateSelectInput/updateSelectizeInput, but wrapping a selectInput with a renderUI is easier to use, especially in flexdashboard versus Shiny.  selectizeInput for nicer drop down menus with auto complete, based on javascript library that's very sharp.  withProgress  shinycssloaders withSpinner  reactiveFileReader  isolate to only reload reactive values when they change", 
            "title": "Scratch"
        }, 
        {
            "location": "/software/bittorrent_sync/", 
            "text": "Installation And Configuration\n\n\n\n\nCreate a new container on Superior\n\n\nDo \necho \"deb \nhttp://linux-packages.resilio.com/resilio-sync/deb\n resilio-sync non-free\" | tee /etc/apt/sources.list.d/resilio-sync.list\n\n\nDo \nwget -qO - \nhttps://linux-packages.resilio.com/resilio-sync/key.asc\n | apt-key add -\n\n\nDo \napt-get update \n apt-get install resilio-sync\n\n\nEdit \n/usr/lib/systemd/user/resilio-sync.service\n with \nWantedBy=default.target\n\n\nEnable the service \nsystemctl --user enable resilio-sync\n\n\nInstead enable the service with \nsystemctl enable resilio-sync\n to use user \nrslsync\n and then start it with \nsystemctl start resilio-sync\n\n\nChange web GUI listening interface \n./rslsync --webui.listen 0.0.0.0:8888\n\n\nEdit \n/etc/resilio-sync/config.json\n and \n/etc/resilio-sync/user_config.json\n And change the listening address to \n0.0.0.0:8888\n as an alternative to above\n\n\nGo to \nhttp://bittorrentsyncip:8888\n with username alex and software password\n\n\nAdd syncing folder under \n/home/rslsync/\n\n\nUseful articles \nhttps://help.resilio.com/hc/en-us/articles/206178924-Installing-Sync-package-on-Linux\n and \nhttps://help.resilio.com/hc/en-us/articles/204762449-Guide-to-Linux", 
            "title": "Bittorrent Sync"
        }, 
        {
            "location": "/software/bittorrent_sync/#installation-and-configuration", 
            "text": "Create a new container on Superior  Do  echo \"deb  http://linux-packages.resilio.com/resilio-sync/deb  resilio-sync non-free\" | tee /etc/apt/sources.list.d/resilio-sync.list  Do  wget -qO -  https://linux-packages.resilio.com/resilio-sync/key.asc  | apt-key add -  Do  apt-get update   apt-get install resilio-sync  Edit  /usr/lib/systemd/user/resilio-sync.service  with  WantedBy=default.target  Enable the service  systemctl --user enable resilio-sync  Instead enable the service with  systemctl enable resilio-sync  to use user  rslsync  and then start it with  systemctl start resilio-sync  Change web GUI listening interface  ./rslsync --webui.listen 0.0.0.0:8888  Edit  /etc/resilio-sync/config.json  and  /etc/resilio-sync/user_config.json  And change the listening address to  0.0.0.0:8888  as an alternative to above  Go to  http://bittorrentsyncip:8888  with username alex and software password  Add syncing folder under  /home/rslsync/  Useful articles  https://help.resilio.com/hc/en-us/articles/206178924-Installing-Sync-package-on-Linux  and  https://help.resilio.com/hc/en-us/articles/204762449-Guide-to-Linux", 
            "title": "Installation And Configuration"
        }, 
        {
            "location": "/software/calibre/", 
            "text": "Calibre Installation\n\n\nsudo -v \n wget -nv -O- \nhttps://download.calibre-ebook.com/linux-installer.sh\n | sudo sh /dev/stdin\n gives a broken pipe error. Alternatively, use \napt-get install calibre\n. Then start the application with \ncalibre-server\n and go to port \n8080\n.\n\n\nCalibre-Web Installation\n\n\nClone \nhttps://github.com/janeczku/calibre-web\n and then \npython-pip\n with \npip install --target vendor -r requirements.txt\n then \nnohup python cps.py\n and go to port \n8083\n with username admin and password admin123. Set the location of the Calibre database \nmetadata.db\n to \n/mnt/books/\n and submit. Edit \n/etc/rc.local\n with the appropriate python call to start server on boot. Make sure to add upload books option in configuration. Upload books using GUI and be sure to use find metadata option to get correct description, titles, book cover, etc.", 
            "title": "Calibre"
        }, 
        {
            "location": "/software/calibre/#calibre-installation", 
            "text": "sudo -v   wget -nv -O-  https://download.calibre-ebook.com/linux-installer.sh  | sudo sh /dev/stdin  gives a broken pipe error. Alternatively, use  apt-get install calibre . Then start the application with  calibre-server  and go to port  8080 .", 
            "title": "Calibre Installation"
        }, 
        {
            "location": "/software/calibre/#calibre-web-installation", 
            "text": "Clone  https://github.com/janeczku/calibre-web  and then  python-pip  with  pip install --target vendor -r requirements.txt  then  nohup python cps.py  and go to port  8083  with username admin and password admin123. Set the location of the Calibre database  metadata.db  to  /mnt/books/  and submit. Edit  /etc/rc.local  with the appropriate python call to start server on boot. Make sure to add upload books option in configuration. Upload books using GUI and be sure to use find metadata option to get correct description, titles, book cover, etc.", 
            "title": "Calibre-Web Installation"
        }, 
        {
            "location": "/software/ci_cd/", 
            "text": "Idea\n\n\nhttps://github.com/ajdurbin/test\n contains a sample application that follows all of the CI/CD concepts. We build a web application using Flask, create a Docker image for it, have a test suite, use Travis-CI's GitHub integration to build the image and run tests before deploying to Docker Hub, then use Heroku's GitHub integration to deploy the application online after Travis-CI passes. Travis-CI allows us to define private environment variables in each repository's settings that can be referenced in any scripts. This is very useful for passwords.\n\n\n\n\nhttps://docs.travis-ci.com/user/docker/\n\n\nhttps://docs.travis-ci.com/user/deployment/heroku/\n\n\nhttps://devcenter.heroku.com/articles/github-integration\n\n\nhttps://medium.com/bettercode/how-to-build-a-modern-ci-cd-pipeline-5faa01891a5b\n\n\nhttps://docs.travis-ci.com/user/getting-started/\n\n\nhttps://medium.com/craft-academy/deploying-heroku-and-travis-ci-69d998ad408a\n\n\nhttps://codeburst.io/ci-cd-with-github-travis-ci-and-heroku-e088a24f32ef\n\n\n\n\nSelfhosted\n\n\nTravis-CI does not really have a local image to be used. The major open-source alternative is Jenkins, but I have yet to be able to set it up appropriately using Docker to provision the slaves. Alternatives include Drone, CircleCI, Gitlab CI/CD. Drone is packaged as a Docker image and also uses YAML files like Travis-CI. Heroku is an excellent service for running Docker web applications and has 500 hours free per month usage. It has a command-line utility that is easily installed to test applications to be deployed before committing changes and triggering a build. But this is a supervised method, so it can be useful for developing first. Heroku also uses a Procfile that we need to find out more about.\n\n\n\n\nhttp://docs.drone.io/getting-started/\n\n\nhttps://devcenter.heroku.com/articles/procfile\n\n\n\n\nSample .travis.yml\n\n\nsudo: required\nservices:\n- docker\nlanguage: python\npython:\n- \n3.5\n\ninstall:\n- pip install -r requirements.txt\nscript:\n- python -m pytest -v\nafter_success:\n- echo \n$DOCKER_PASS\n | docker login -u \n$DOCKER_USER\n --password-stdin\n- export APP=test\n- export REPO=$DOCKER_USER/$APP\n- export TAG=`if [ \n$TRAVIS_BRANCH\n == \nmaster\n ]; then echo \nlatest\n; else echo $TRAVIS_BRANCH ; fi`\n- docker build -f Dockerfile -t $REPO:$TAG -t $REPO:travis-$TRAVIS_BUILD_NUMBER .\n- docker push $REPO:$TAG\n- docker push $REPO:travis-$TRAVIS_BUILD_NUMBER\n\n\n\n\nTools\n\n\n\n\njenkins\n\n\ntravis-ci\n\n\ndrone-io\n\n\nbettercodehub\n\n\nheroku\n\n\n\n\nSelfhosted V2\n\n\nJenkins has proven to be extremely difficult to install and configure slaves properly, plus I'm not a fan of the syntax. Alternatives are Drone and Concourse. All Drone tutorials look like they use Github repositories, but documentation allows you to specify Gitea, for example. Appears to be free to selfhost, though not super clear. Documentation is okay, uses YAML for the build files, seems pretty popular. Other alternative is Concourse, which is open source and has a good tutorial on installation. Also Abstruse. Of these Drone looks to be the most popular.\n\n\nWanted features:\n- trigger build on commit\n- run unit tests and email on failure\n- build image, push to registry (local, remote), deploy\n- Bonus points for Heroku deployment too", 
            "title": "CI/CD"
        }, 
        {
            "location": "/software/ci_cd/#idea", 
            "text": "https://github.com/ajdurbin/test  contains a sample application that follows all of the CI/CD concepts. We build a web application using Flask, create a Docker image for it, have a test suite, use Travis-CI's GitHub integration to build the image and run tests before deploying to Docker Hub, then use Heroku's GitHub integration to deploy the application online after Travis-CI passes. Travis-CI allows us to define private environment variables in each repository's settings that can be referenced in any scripts. This is very useful for passwords.   https://docs.travis-ci.com/user/docker/  https://docs.travis-ci.com/user/deployment/heroku/  https://devcenter.heroku.com/articles/github-integration  https://medium.com/bettercode/how-to-build-a-modern-ci-cd-pipeline-5faa01891a5b  https://docs.travis-ci.com/user/getting-started/  https://medium.com/craft-academy/deploying-heroku-and-travis-ci-69d998ad408a  https://codeburst.io/ci-cd-with-github-travis-ci-and-heroku-e088a24f32ef", 
            "title": "Idea"
        }, 
        {
            "location": "/software/ci_cd/#selfhosted", 
            "text": "Travis-CI does not really have a local image to be used. The major open-source alternative is Jenkins, but I have yet to be able to set it up appropriately using Docker to provision the slaves. Alternatives include Drone, CircleCI, Gitlab CI/CD. Drone is packaged as a Docker image and also uses YAML files like Travis-CI. Heroku is an excellent service for running Docker web applications and has 500 hours free per month usage. It has a command-line utility that is easily installed to test applications to be deployed before committing changes and triggering a build. But this is a supervised method, so it can be useful for developing first. Heroku also uses a Procfile that we need to find out more about.   http://docs.drone.io/getting-started/  https://devcenter.heroku.com/articles/procfile", 
            "title": "Selfhosted"
        }, 
        {
            "location": "/software/ci_cd/#sample-travisyml", 
            "text": "sudo: required\nservices:\n- docker\nlanguage: python\npython:\n-  3.5 \ninstall:\n- pip install -r requirements.txt\nscript:\n- python -m pytest -v\nafter_success:\n- echo  $DOCKER_PASS  | docker login -u  $DOCKER_USER  --password-stdin\n- export APP=test\n- export REPO=$DOCKER_USER/$APP\n- export TAG=`if [  $TRAVIS_BRANCH  ==  master  ]; then echo  latest ; else echo $TRAVIS_BRANCH ; fi`\n- docker build -f Dockerfile -t $REPO:$TAG -t $REPO:travis-$TRAVIS_BUILD_NUMBER .\n- docker push $REPO:$TAG\n- docker push $REPO:travis-$TRAVIS_BUILD_NUMBER", 
            "title": "Sample .travis.yml"
        }, 
        {
            "location": "/software/ci_cd/#tools", 
            "text": "jenkins  travis-ci  drone-io  bettercodehub  heroku", 
            "title": "Tools"
        }, 
        {
            "location": "/software/ci_cd/#selfhosted-v2", 
            "text": "Jenkins has proven to be extremely difficult to install and configure slaves properly, plus I'm not a fan of the syntax. Alternatives are Drone and Concourse. All Drone tutorials look like they use Github repositories, but documentation allows you to specify Gitea, for example. Appears to be free to selfhost, though not super clear. Documentation is okay, uses YAML for the build files, seems pretty popular. Other alternative is Concourse, which is open source and has a good tutorial on installation. Also Abstruse. Of these Drone looks to be the most popular.  Wanted features:\n- trigger build on commit\n- run unit tests and email on failure\n- build image, push to registry (local, remote), deploy\n- Bonus points for Heroku deployment too", 
            "title": "Selfhosted V2"
        }, 
        {
            "location": "/software/design/", 
            "text": "file structure\n\n\napp/\nvenv/\napp/\n__init__.py\nfunctions.p\nmain.py\nconfig.py\n\n\n\n\nsoftware carpentry\n\n\nmodularization\n\n\nCircular imports.\n\n\ngoogle style guide\n\n\nhttps://github.com/google/styleguide/blob/gh-pages/pyguide.md\n\n\nConsistent variable naming, function naming, class naming, separation of modules, class versus function, modularization, nested function definitions.\n\n\nasynchronous vs parallel vs concurrent vs coroutine\n\n\n\n\nparallel distributes tasks to multiple processors that actively work simultaneously\n\n\nconcurrent handle tasks that are in progress at same time, but only necessary to work briefly and separately on each task, so work can be interleaved on whatever order the tasks require\n\n\nAsynch dispatches tasks to devices that take care of themselves, so the program can do other things until it receives a signal that the results are finished\n\n\n\n\nshare global variable between files\n\n\nNot suggested unless annoying to have it as function argument in everything.\n\n\n# settings.py\ndef init():\nglobal myvar\nmyvar = []\n\n# subfile.py\nimport settings\n\ndef stuff():\nsettings.myvar.append('hey')\n\n# main.py\nimport settings\nimport subfile\nsettings.init() # initialize global variable\nsubfile.stuff() # do stuff with global variable\n\n\n\n\ndry principals\n\n\nlogging\n\n\nLogging at module level. Multiple loggers. Different logging levels. Notifying administrator on exceptions and logging exceptions on program failure.\n\n\nnested function definitions\n\n\nci/cd\n\n\nBuild test cases into program, add docker executable build to part of pipeline, commit to remote repository triggers an image build that runs tests, notifies administrator upon failure or deploys image and pushes image to docker hub.\n\n\nchange log\n\n\nresponding to issues on github and making changes\n\n\nproper versioning - something at work on this\n\n\nlicensing\n\n\nversion control\n\n\nProper branching and tagging.\n\n\ndocumentation\n\n\nUsing a documentation generator like Sphinx for auto-generating documentation.\n\n\nlinting\n\n\ncode coverage\n\n\nunit testing\n\n\ncheckpoint programs with log files?", 
            "title": "Design"
        }, 
        {
            "location": "/software/design/#file-structure", 
            "text": "app/\nvenv/\napp/\n__init__.py\nfunctions.p\nmain.py\nconfig.py", 
            "title": "file structure"
        }, 
        {
            "location": "/software/design/#software-carpentry", 
            "text": "", 
            "title": "software carpentry"
        }, 
        {
            "location": "/software/design/#modularization", 
            "text": "Circular imports.", 
            "title": "modularization"
        }, 
        {
            "location": "/software/design/#google-style-guide", 
            "text": "https://github.com/google/styleguide/blob/gh-pages/pyguide.md  Consistent variable naming, function naming, class naming, separation of modules, class versus function, modularization, nested function definitions.", 
            "title": "google style guide"
        }, 
        {
            "location": "/software/design/#asynchronous-vs-parallel-vs-concurrent-vs-coroutine", 
            "text": "parallel distributes tasks to multiple processors that actively work simultaneously  concurrent handle tasks that are in progress at same time, but only necessary to work briefly and separately on each task, so work can be interleaved on whatever order the tasks require  Asynch dispatches tasks to devices that take care of themselves, so the program can do other things until it receives a signal that the results are finished", 
            "title": "asynchronous vs parallel vs concurrent vs coroutine"
        }, 
        {
            "location": "/software/design/#share-global-variable-between-files", 
            "text": "Not suggested unless annoying to have it as function argument in everything.  # settings.py\ndef init():\nglobal myvar\nmyvar = []\n\n# subfile.py\nimport settings\n\ndef stuff():\nsettings.myvar.append('hey')\n\n# main.py\nimport settings\nimport subfile\nsettings.init() # initialize global variable\nsubfile.stuff() # do stuff with global variable", 
            "title": "share global variable between files"
        }, 
        {
            "location": "/software/design/#dry-principals", 
            "text": "", 
            "title": "dry principals"
        }, 
        {
            "location": "/software/design/#logging", 
            "text": "Logging at module level. Multiple loggers. Different logging levels. Notifying administrator on exceptions and logging exceptions on program failure.", 
            "title": "logging"
        }, 
        {
            "location": "/software/design/#nested-function-definitions", 
            "text": "", 
            "title": "nested function definitions"
        }, 
        {
            "location": "/software/design/#cicd", 
            "text": "Build test cases into program, add docker executable build to part of pipeline, commit to remote repository triggers an image build that runs tests, notifies administrator upon failure or deploys image and pushes image to docker hub.", 
            "title": "ci/cd"
        }, 
        {
            "location": "/software/design/#change-log", 
            "text": "", 
            "title": "change log"
        }, 
        {
            "location": "/software/design/#responding-to-issues-on-github-and-making-changes", 
            "text": "", 
            "title": "responding to issues on github and making changes"
        }, 
        {
            "location": "/software/design/#proper-versioning-something-at-work-on-this", 
            "text": "", 
            "title": "proper versioning - something at work on this"
        }, 
        {
            "location": "/software/design/#licensing", 
            "text": "", 
            "title": "licensing"
        }, 
        {
            "location": "/software/design/#version-control", 
            "text": "Proper branching and tagging.", 
            "title": "version control"
        }, 
        {
            "location": "/software/design/#documentation", 
            "text": "Using a documentation generator like Sphinx for auto-generating documentation.", 
            "title": "documentation"
        }, 
        {
            "location": "/software/design/#linting", 
            "text": "", 
            "title": "linting"
        }, 
        {
            "location": "/software/design/#code-coverage", 
            "text": "", 
            "title": "code coverage"
        }, 
        {
            "location": "/software/design/#unit-testing", 
            "text": "", 
            "title": "unit testing"
        }, 
        {
            "location": "/software/design/#checkpoint-programs-with-log-files", 
            "text": "", 
            "title": "checkpoint programs with log files?"
        }, 
        {
            "location": "/software/gitea/", 
            "text": "Installation\n\n\napt install git -y\nwget -O gitea \nhttps://dl.gitea.io/gitea/1.4/gitea-1.4-linux-amd64\n\nchmod +x gitea\n./gitea web\n\n\n\n\nChoose SQLite3 database and install after setting up admin account, also change Application URL, Domain to IP address instead of localhost.\n\n\nRoot user okay for running, but setup ssh keys or else you cannot clone/push to a repository over SSH.\n\n\nAdd \n/root/gitea web\n to \n/etc/rc.local\n before \nexit 0\n to start on boot\n\n\nBetter Installation Links As Service\n\n\n\n\nhttps://docs.gitea.io/en-us/install-from-binary/\n\n\nhttps://docs.gitea.io/en-us/linux-service/\n\n\nhttps://github.com/go-gitea/gitea/blob/master/contrib/systemd/gitea.service", 
            "title": "Gitea"
        }, 
        {
            "location": "/software/gitea/#installation", 
            "text": "apt install git -y\nwget -O gitea  https://dl.gitea.io/gitea/1.4/gitea-1.4-linux-amd64 \nchmod +x gitea\n./gitea web  Choose SQLite3 database and install after setting up admin account, also change Application URL, Domain to IP address instead of localhost.  Root user okay for running, but setup ssh keys or else you cannot clone/push to a repository over SSH.  Add  /root/gitea web  to  /etc/rc.local  before  exit 0  to start on boot", 
            "title": "Installation"
        }, 
        {
            "location": "/software/gitea/#better-installation-links-as-service", 
            "text": "https://docs.gitea.io/en-us/install-from-binary/  https://docs.gitea.io/en-us/linux-service/  https://github.com/go-gitea/gitea/blob/master/contrib/systemd/gitea.service", 
            "title": "Better Installation Links As Service"
        }, 
        {
            "location": "/software/ideas/", 
            "text": "About\n\n\nThis page is to log different software packages with short descriptions for possible later inclusion in my home lab. A lot of these will be from \nhttps://github.com/Kickball/awesome-selfhosted.\n\n\nVirtualization\n\n\n\n\nDocker\n\n\nlxc\n\n\nProxmox\n\n\n\n\nFile Syncing\n\n\n\n\nResilio-Sync: Has Android/iOS applications and FreeNas plugin. Works well enough even though user permissions on installation are weird. Closed source.\n\n\nSyncthing: Has Android/iOS applications. Documentation not great for steps after installation to actually sync the folders from one site to another.\n\n\nRClone: Powerful tool for syncing folders/files to Google Drive. Great documentation.\n\n\nrsync: Command line tool for syncing folders or files one way or both ways with deletion or not.\n\n\nNextCloud: Selfhosted Google Drive replacement with Calendar, instant messaging support, etc.\n\n\n\n\nMachine Learning and Statistics\n\n\n\n\ncomet.ml: Cool utility for sharing models with other's including documentation, git support, etc.\n\n\ngluon: AWS/Microsoft tool similar to comet.ml\n\n\nTensorBoard: Visualize statistics from TensorFlow models.\n\n\nnbviewer: view notebooks from git repository as a website or using a static site generator\n\n\njupyter hub: Jupyter notebook server and Python kernel for multiple users\n\n\nRStudio Server: R server with web GUI\n\n\nR Shiny server: Server for selfhosting R Shiny applications\n\n\n\n\nVisualizations\n\n\n\n\nGrafana\n\n\n\n\nDatabases\n\n\n\n\nGraphite\n\n\ninfluxdb\n\n\nKafka\n\n\nCassandra\n\n\nHadoop/Spark\n\n\n\n\nWeb services\n\n\n\n\nreverse proxy to safely expose applications to outside world or just static site\n\n\nRedis for server side caching for web applications\n\n\n\n\nData Collection\n\n\n\n\nOwntracks\n\n\nOpenWeatherMap\n\n\nopenstreetmap\n\n\npersonal statistics\n\n\nCTV\n\n\ntimed still-shot photos of outdoors\n\n\n\n\nCI/CD\n\n\n\n\nArtifactory to store images\n\n\nJenkins to build images from git repository\n\n\nkubernetes to deploy docker applications\n\n\nInternal pypi repository\n\n\n\n\nEntertainment\n\n\n\n\nPlex\n\n\nplex channels\n\n\nSonarr\n\n\nJacket\n\n\nRadarr\n\n\nCalibre\n\n\nMinecraft server\n\n\nInsurgency server?\n\n\n\n\ntools\n\n\n\n\nzabbix for monitoring hosts\n\n\ninternal dns server besides pfsense host?\n\n\nauthentication like FreeIPA\n\n\nIP phones, FreePBX\n\n\n\n\nHPC\n\n\n\n\nslurm/torque/whatever job schedular\n\n\nenvironment modules for different software versions\n\n\nmpi\n\n\nmp\n\n\n\n\nCommunication\n\n\n\n\nDiscord\n\n\nIRC\n\n\nRSS\n\n\n\n\nword salad\n\n\n\n\ntodo list outside wiki\n\n\nuse Google Calendar more, contacts for address book\n\n\nchatbot\n\n\nMkdocs wiki behind reverse proxy and pushed to github pages or readthedocs (can remove signature that bugs me) with notebooks on github provided to nbviewer in some notebook repository\n\n\nowntracks behind \n\n\nzabbix statistics\n\n\ngraphite stats exported from freenas, proxmox very easily, push to grafana\n\n\nshiny reverse proxy\n\n\ngithub tags for model passing pypi, etc\n\n\nselfhosted git and push to bitbucket, github also\n\n\nmerge both data analytics repositories\n\n\nmake a notebook repository and copy files there\n\n\nmake notebook viewer application\n\n\nlink notebooks from github to nbviewer online\n\n\nupload all of grad school stuff to repos, do some cleaning up of miscellaneous notebook ones with better names\n\n\nraspberry pi mkdocs installation\n\n\nstatistics and deep learning should all be single directory in new wiki\n\n\nget back into journal\n\n\ncustomizing bashrc, put bashrc rcprofile, any relevant config files in unix directory, reverse proxy files\n\n\nDUO two factor\n\n\nmkdocs gh-deploy for new documentation, poor performance on raspberry pi and annoying editing documents in real time with log files printed to screen, okay with running in vm or not? Deploying to github messed up personal website root, so also need to see if I'm fine not having personal website or not or including in new documentation.\n\n\nUse gitea instead of gogs and figure out ssh key situation for pushing since it's getting annoying using passwords\n\n\nreorganize python folders for notebook viewer\n\n\ncreate snippets repository like Bob Settlage?\n\n\nRstudio/rshiny server in single machine\n\n\njekyll for github pages, maybe that's okay to have github root page jekyll with wiki backend if single page?\n\n\nTry reverse proxy of shiny applications, wiki\n\n\nFetch ASC code from Peter's repository since we worked on these things and there are lots of examples\n\n\nJournal? switch to python package or keep using Rmd\n\n\nGraphite/grafana fetch statistics from huron, superior maybe separate instance for open weather map\n\n\nown tracks\n\n\ntravic ci instead of jenkins, need to figure out local installation\n\n\nkubernetes on superior root\n\n\nci/cd pipeline with git -\n jenkins/travisci -\n artifactory -\n kubernetes deployment\n\n\nunit testing with tox, other python packages\n\n\nlist of standard R packages\n\n\nheroku\n\n\nredis server for web caching\n\n\nalpine linux images\n\n\nfix annoying windows/ubuntu dual boot time issue\n\n\nblender for 3d modeling\n\n\nlook more into python parallel/distributed computing packages\n\n\nif pushing code to github always make a gh-pages branch with some docs in it to be pushed too, at least for some longer projects for nicer to read stuff\n\n\ngit web hooks\n\n\ncombine statistics/ml/dl/nlp pages into single directory\n\n\ngithub pages personal page can't be with subdirectories, so try having only single top level page. mkdocs routing to convert dokuwiki text, need to rename start to index\n\n\nManual mkdocs gh-deploy routine with ssh keys and checkout\n\n\nsample project to test sphinx functionality?\n\n\ncontinue using dokuwiki on pi (just so it can be used), and then convert files to mkdocs for github pages for outside usage?\n\n\nwebhooks to trigger ci/cd pipelines\n\n\nweb service for hosting applications from flask/tornado/gnuicorn\n\n\nheroku/pythonanywhere\n\n\nmongo/mariadb instead of mysql - recall \nhttps://www.reddit.com/r/AskReddit/comments/8fztrk/what_was_the_removing_the_headphone_jack_of/\n and discussion of Oracle no longer developing mysql basically\n\n\nfull virtualmachine with docker installed as docker host\n\n\njenkins dockerfile github to artifactory and docker using travis ci when can\n\n\nappears that marathon is only for mesos-based systems\n\n\nkubernetes?\n\n\ntraefik\n\n\n\"hacking\" fake terminal\n\n\ncalibre server for ebooks\n\n\nget docker jenkins git pipeline first before worrying about artifactory\n\n\nload balancer\n\n\ninternal dns\n\n\ndocker swarm/compose versus a kubernetes solution\n\n\nsome sort of docker load balancing\n\n\ndocker traefik\n\n\ngitlab has their own ci/cd stuff similar to jenkins/travis-ci\n\n\nbettercodehub\n\n\nstatic site generators like Hugo\n\n\nreverse proxy for non-static sites, like dokuwiki for instance or any other php web application\n\n\ndigital ocean tutorials\n\n\nteraform\n\n\ncaddy server\n\n\nhaproxy\n\n\nfreeipa\n\n\nfreepbx\n\n\nreddit raspberry pi ups server tutorial\n\n\nrecipe manager\n\n\ndocker spark/hadoop images\n\n\nsonarqube\n\n\nreverse proxy notebooks\n\n\nstatic site generator usage\n\n\nhow are interactive applications served?\n\n\ncassandra data model\n\n\nsql data model\n\n\nselfhosted gitbook? and if so vs bookdown\n\n\nnvidia jetson for embedded deep learning machine - but uses proprietary ubuntu distribution\n\n\nraspberry pi compute module/camera/\n\n\nintel movidius vpu neural compute engine\n\n\ntime lapse\n\n\ndata labelling application\n\n\nopen street map\n\n\ngis open data\n\n\ninformation retrieval\n\n\nselfhosted pypi (pypiserver package easy setup)\n\n\nselfhosted docker registry (would like to get the ssl setup working)\n\n\nlets encrypt web server for generating certifcates and static sites\n\n\nbookdown vs gitbook vs pandoc\n\n\ngraylog\n\n\nhaproxy\n\n\nhoneypots\n\n\ngitlab account - should be free and private for university students\n\n\ngoogle collaboratory, online jupyter notebooks with gpu usage!!!\n\n\nletsencrypt wildcard subdomains\n\n\nintel movidius neural compute module for small board computers like raspberry pi, though may not support keras\n\n\narmbian/rock64/debian cheap hardware page discussing sbc. Rasberry Pi is just best supported though.\n\n\nnanoleaf smart wall light panel, pretty cool\n\n\nweex opens ource weather software has a supported hardware page with lots of cool weather kits\n\n\ngimp photo processing\n\n\nblender 3dmodeling\n\n\nspaceengine.org\n\n\nstud.io lego modeling program\n\n\nbricklink\n\n\ndraw.io\n\n\nblob storage for database byte storage, say images\n\n\nmongo, arongo databases\n\n\ngraph architecture vs key vs relational database\n\n\nhdfs\n\n\nelastic\n\n\nluminoth open source computer vision toolkit for bounding boxes and confidence\n\n\nilastix image segmentation tool interactive\n\n\nblue iris windows only alternative to zoneminder that everyone loves\n\n\nbookmarks selfhosted or read later list, also have shared tabs between chrome on pc, laptop, ubuntu?\n\n\npersonal jira task list\n\n\ntraefik reverse proxy for docker\n\n\nmqtt\n\n\nfreedns reverse proxy subdomains\n\n\ncode ocean for sharing reproducible scientific computing\n\n\nstrava for tracking bike rides\n\n\nopentoonz\n\n\nopen broadcasting software - obs or whatever everyone uses for streaming\n\n\nsqueeky wheel complain on twitter of internet speed slow\n\n\nngrok\n\n\nsemver.org for software release versioning\n\n\nmybinder.org\n\n\nbigquery public datasets\n\n\ndont dump every software install into rc.local to run on boot, make it into a unix service, go back through various installations that were put into rc.local and add \n to gracefully exit the file.\n\n\nswagger free service to visualize api calls\n\n\n request methods for web services/api: \nhttps://www.w3schools.com/tags/ref_httpmethods.asp\n\n\ntraefik for reverse proxy and load balance microservices lock docker containers, Nick K from A-O is interested in using this with authentication and with a domain.\n\n\nrocketchat as a slack alternative\n\n\nelasticsearch\n\n\npython anywhere\n\n\nread it later list - like wallabag is most popular\n\n\nrss reader like tiny tiny rss for lots of stuff, find more uses for rss\n\n\nsource control any configuration files changes on any services for easy backup and restoration\n\n\nmarktext cool markdown editor with lots of features\n\n\nhttps://hipstercat.fr/gogs/hipstercat/rudead\n send messages to relatives after dying, idea is for all those passwords and information they'd need to find that you could provide easily\n\n\nopenlogos\n\n\nhttps://paperswithcode.com/\n\n\nhouse hunters plot twitter robot, very funny\n\n\nhttps://medium.mybridge.co/machine-learning-open-source-of-the-month-v-may-2018-efd396d9306e\n\n\nhttps://mitpress.mit.edu/sites/default/files/sicp/index.html\n software design book\n\n\nobject oriented thought process\n\n\nhttps://erlemar.github.io/\n lists the different projects they've worked on\n\n\nchoosealicense.com\n\n\ngitignore.io\n\n\nhttps://thehftguy.com/2016/10/24/heres-how-to-make-a-good-github-project-for-your-resume/\n\n\nai train for simple video game\n\n\nhttps://medium.com/@marcelogdeandrade/writing-your-own-programming-language-and-compiler-with-python-a468970ae6df\n\n\noeisbot reddit bot for presenting sequences\n\n\ngravatar\n\n\nhttp://lightning-viz.org/\n\n\nsnibox as a snippet repository and web gui, can tag snippets\n\n\nopen source api gateways\n\n\nstatic sites\n\n\nmarkdown\n\n\ngraph vs relational vs nosql databases\n\n\nsql database design 1-to-many, many-to-1 relationships\n\n\nvenmo twitter bot tweats drug deals\n\n\nopenheatmap/leaflet/candela/charted/datawrapper/rawgraphs/d3.js/plotly/dygraphs\n\n\nhttps://engineering.videoblocks.com/web-architecture-101-a3224e126947\n great explanations on web server stacks\n\n\nopenid\n\n\nkerberos authentication\n\n\nkubernetes\n\n\nhttps://developer.okta.com/blog/2018/07/12/flask-tutorial-simple-user-registration-and-login\n openid for flask authentication\n\n\ndrone has marathon plugin\n\n\nopenid/oauth for universal authentication\n\n\ngoogle analytics tracking information\n\n\n\n\nMain goal is to get working git-jenkins-artifactory-docker plugin going. So we'd have a remote git repository to trigger a jenkins build that tests the software before sending to artifactory and then deploying on the docker server. So the jenkins file will be split up to be build the image using docker server, then push image to docker registry, then pull image from docker registry and run.\n\n\nDo more scientific computing so go back to learning a compiled code framework, more R shiny stuff, MCMC, Bayesian stats, maybe go back to C++ to do some CUDA, more web development with Flask/Django\n\n\nThis is a great tutorial: \nhttps://medium.com/bettercode/how-to-build-a-modern-ci-cd-pipeline-5faa01891a5b\n and somewhat related \nhttps://medium.com/@evheniybystrov/continuous-delivery-of-react-app-with-jenkins-and-docker-8a1ae1511b86\n\n\nPlan\n: Put Jenkins on Docker host and use Docker's built in registry, look into Docker git server that's lightweight enough. Use these for development and then push applications to Heroku and use travis-ci for Github.", 
            "title": "Ideas"
        }, 
        {
            "location": "/software/ideas/#about", 
            "text": "This page is to log different software packages with short descriptions for possible later inclusion in my home lab. A lot of these will be from  https://github.com/Kickball/awesome-selfhosted.", 
            "title": "About"
        }, 
        {
            "location": "/software/ideas/#virtualization", 
            "text": "Docker  lxc  Proxmox", 
            "title": "Virtualization"
        }, 
        {
            "location": "/software/ideas/#file-syncing", 
            "text": "Resilio-Sync: Has Android/iOS applications and FreeNas plugin. Works well enough even though user permissions on installation are weird. Closed source.  Syncthing: Has Android/iOS applications. Documentation not great for steps after installation to actually sync the folders from one site to another.  RClone: Powerful tool for syncing folders/files to Google Drive. Great documentation.  rsync: Command line tool for syncing folders or files one way or both ways with deletion or not.  NextCloud: Selfhosted Google Drive replacement with Calendar, instant messaging support, etc.", 
            "title": "File Syncing"
        }, 
        {
            "location": "/software/ideas/#machine-learning-and-statistics", 
            "text": "comet.ml: Cool utility for sharing models with other's including documentation, git support, etc.  gluon: AWS/Microsoft tool similar to comet.ml  TensorBoard: Visualize statistics from TensorFlow models.  nbviewer: view notebooks from git repository as a website or using a static site generator  jupyter hub: Jupyter notebook server and Python kernel for multiple users  RStudio Server: R server with web GUI  R Shiny server: Server for selfhosting R Shiny applications", 
            "title": "Machine Learning and Statistics"
        }, 
        {
            "location": "/software/ideas/#visualizations", 
            "text": "Grafana", 
            "title": "Visualizations"
        }, 
        {
            "location": "/software/ideas/#databases", 
            "text": "Graphite  influxdb  Kafka  Cassandra  Hadoop/Spark", 
            "title": "Databases"
        }, 
        {
            "location": "/software/ideas/#web-services", 
            "text": "reverse proxy to safely expose applications to outside world or just static site  Redis for server side caching for web applications", 
            "title": "Web services"
        }, 
        {
            "location": "/software/ideas/#data-collection", 
            "text": "Owntracks  OpenWeatherMap  openstreetmap  personal statistics  CTV  timed still-shot photos of outdoors", 
            "title": "Data Collection"
        }, 
        {
            "location": "/software/ideas/#cicd", 
            "text": "Artifactory to store images  Jenkins to build images from git repository  kubernetes to deploy docker applications  Internal pypi repository", 
            "title": "CI/CD"
        }, 
        {
            "location": "/software/ideas/#entertainment", 
            "text": "Plex  plex channels  Sonarr  Jacket  Radarr  Calibre  Minecraft server  Insurgency server?", 
            "title": "Entertainment"
        }, 
        {
            "location": "/software/ideas/#tools", 
            "text": "zabbix for monitoring hosts  internal dns server besides pfsense host?  authentication like FreeIPA  IP phones, FreePBX", 
            "title": "tools"
        }, 
        {
            "location": "/software/ideas/#hpc", 
            "text": "slurm/torque/whatever job schedular  environment modules for different software versions  mpi  mp", 
            "title": "HPC"
        }, 
        {
            "location": "/software/ideas/#communication", 
            "text": "Discord  IRC  RSS", 
            "title": "Communication"
        }, 
        {
            "location": "/software/ideas/#word-salad", 
            "text": "todo list outside wiki  use Google Calendar more, contacts for address book  chatbot  Mkdocs wiki behind reverse proxy and pushed to github pages or readthedocs (can remove signature that bugs me) with notebooks on github provided to nbviewer in some notebook repository  owntracks behind   zabbix statistics  graphite stats exported from freenas, proxmox very easily, push to grafana  shiny reverse proxy  github tags for model passing pypi, etc  selfhosted git and push to bitbucket, github also  merge both data analytics repositories  make a notebook repository and copy files there  make notebook viewer application  link notebooks from github to nbviewer online  upload all of grad school stuff to repos, do some cleaning up of miscellaneous notebook ones with better names  raspberry pi mkdocs installation  statistics and deep learning should all be single directory in new wiki  get back into journal  customizing bashrc, put bashrc rcprofile, any relevant config files in unix directory, reverse proxy files  DUO two factor  mkdocs gh-deploy for new documentation, poor performance on raspberry pi and annoying editing documents in real time with log files printed to screen, okay with running in vm or not? Deploying to github messed up personal website root, so also need to see if I'm fine not having personal website or not or including in new documentation.  Use gitea instead of gogs and figure out ssh key situation for pushing since it's getting annoying using passwords  reorganize python folders for notebook viewer  create snippets repository like Bob Settlage?  Rstudio/rshiny server in single machine  jekyll for github pages, maybe that's okay to have github root page jekyll with wiki backend if single page?  Try reverse proxy of shiny applications, wiki  Fetch ASC code from Peter's repository since we worked on these things and there are lots of examples  Journal? switch to python package or keep using Rmd  Graphite/grafana fetch statistics from huron, superior maybe separate instance for open weather map  own tracks  travic ci instead of jenkins, need to figure out local installation  kubernetes on superior root  ci/cd pipeline with git -  jenkins/travisci -  artifactory -  kubernetes deployment  unit testing with tox, other python packages  list of standard R packages  heroku  redis server for web caching  alpine linux images  fix annoying windows/ubuntu dual boot time issue  blender for 3d modeling  look more into python parallel/distributed computing packages  if pushing code to github always make a gh-pages branch with some docs in it to be pushed too, at least for some longer projects for nicer to read stuff  git web hooks  combine statistics/ml/dl/nlp pages into single directory  github pages personal page can't be with subdirectories, so try having only single top level page. mkdocs routing to convert dokuwiki text, need to rename start to index  Manual mkdocs gh-deploy routine with ssh keys and checkout  sample project to test sphinx functionality?  continue using dokuwiki on pi (just so it can be used), and then convert files to mkdocs for github pages for outside usage?  webhooks to trigger ci/cd pipelines  web service for hosting applications from flask/tornado/gnuicorn  heroku/pythonanywhere  mongo/mariadb instead of mysql - recall  https://www.reddit.com/r/AskReddit/comments/8fztrk/what_was_the_removing_the_headphone_jack_of/  and discussion of Oracle no longer developing mysql basically  full virtualmachine with docker installed as docker host  jenkins dockerfile github to artifactory and docker using travis ci when can  appears that marathon is only for mesos-based systems  kubernetes?  traefik  \"hacking\" fake terminal  calibre server for ebooks  get docker jenkins git pipeline first before worrying about artifactory  load balancer  internal dns  docker swarm/compose versus a kubernetes solution  some sort of docker load balancing  docker traefik  gitlab has their own ci/cd stuff similar to jenkins/travis-ci  bettercodehub  static site generators like Hugo  reverse proxy for non-static sites, like dokuwiki for instance or any other php web application  digital ocean tutorials  teraform  caddy server  haproxy  freeipa  freepbx  reddit raspberry pi ups server tutorial  recipe manager  docker spark/hadoop images  sonarqube  reverse proxy notebooks  static site generator usage  how are interactive applications served?  cassandra data model  sql data model  selfhosted gitbook? and if so vs bookdown  nvidia jetson for embedded deep learning machine - but uses proprietary ubuntu distribution  raspberry pi compute module/camera/  intel movidius vpu neural compute engine  time lapse  data labelling application  open street map  gis open data  information retrieval  selfhosted pypi (pypiserver package easy setup)  selfhosted docker registry (would like to get the ssl setup working)  lets encrypt web server for generating certifcates and static sites  bookdown vs gitbook vs pandoc  graylog  haproxy  honeypots  gitlab account - should be free and private for university students  google collaboratory, online jupyter notebooks with gpu usage!!!  letsencrypt wildcard subdomains  intel movidius neural compute module for small board computers like raspberry pi, though may not support keras  armbian/rock64/debian cheap hardware page discussing sbc. Rasberry Pi is just best supported though.  nanoleaf smart wall light panel, pretty cool  weex opens ource weather software has a supported hardware page with lots of cool weather kits  gimp photo processing  blender 3dmodeling  spaceengine.org  stud.io lego modeling program  bricklink  draw.io  blob storage for database byte storage, say images  mongo, arongo databases  graph architecture vs key vs relational database  hdfs  elastic  luminoth open source computer vision toolkit for bounding boxes and confidence  ilastix image segmentation tool interactive  blue iris windows only alternative to zoneminder that everyone loves  bookmarks selfhosted or read later list, also have shared tabs between chrome on pc, laptop, ubuntu?  personal jira task list  traefik reverse proxy for docker  mqtt  freedns reverse proxy subdomains  code ocean for sharing reproducible scientific computing  strava for tracking bike rides  opentoonz  open broadcasting software - obs or whatever everyone uses for streaming  squeeky wheel complain on twitter of internet speed slow  ngrok  semver.org for software release versioning  mybinder.org  bigquery public datasets  dont dump every software install into rc.local to run on boot, make it into a unix service, go back through various installations that were put into rc.local and add   to gracefully exit the file.  swagger free service to visualize api calls   request methods for web services/api:  https://www.w3schools.com/tags/ref_httpmethods.asp  traefik for reverse proxy and load balance microservices lock docker containers, Nick K from A-O is interested in using this with authentication and with a domain.  rocketchat as a slack alternative  elasticsearch  python anywhere  read it later list - like wallabag is most popular  rss reader like tiny tiny rss for lots of stuff, find more uses for rss  source control any configuration files changes on any services for easy backup and restoration  marktext cool markdown editor with lots of features  https://hipstercat.fr/gogs/hipstercat/rudead  send messages to relatives after dying, idea is for all those passwords and information they'd need to find that you could provide easily  openlogos  https://paperswithcode.com/  house hunters plot twitter robot, very funny  https://medium.mybridge.co/machine-learning-open-source-of-the-month-v-may-2018-efd396d9306e  https://mitpress.mit.edu/sites/default/files/sicp/index.html  software design book  object oriented thought process  https://erlemar.github.io/  lists the different projects they've worked on  choosealicense.com  gitignore.io  https://thehftguy.com/2016/10/24/heres-how-to-make-a-good-github-project-for-your-resume/  ai train for simple video game  https://medium.com/@marcelogdeandrade/writing-your-own-programming-language-and-compiler-with-python-a468970ae6df  oeisbot reddit bot for presenting sequences  gravatar  http://lightning-viz.org/  snibox as a snippet repository and web gui, can tag snippets  open source api gateways  static sites  markdown  graph vs relational vs nosql databases  sql database design 1-to-many, many-to-1 relationships  venmo twitter bot tweats drug deals  openheatmap/leaflet/candela/charted/datawrapper/rawgraphs/d3.js/plotly/dygraphs  https://engineering.videoblocks.com/web-architecture-101-a3224e126947  great explanations on web server stacks  openid  kerberos authentication  kubernetes  https://developer.okta.com/blog/2018/07/12/flask-tutorial-simple-user-registration-and-login  openid for flask authentication  drone has marathon plugin  openid/oauth for universal authentication  google analytics tracking information   Main goal is to get working git-jenkins-artifactory-docker plugin going. So we'd have a remote git repository to trigger a jenkins build that tests the software before sending to artifactory and then deploying on the docker server. So the jenkins file will be split up to be build the image using docker server, then push image to docker registry, then pull image from docker registry and run.  Do more scientific computing so go back to learning a compiled code framework, more R shiny stuff, MCMC, Bayesian stats, maybe go back to C++ to do some CUDA, more web development with Flask/Django  This is a great tutorial:  https://medium.com/bettercode/how-to-build-a-modern-ci-cd-pipeline-5faa01891a5b  and somewhat related  https://medium.com/@evheniybystrov/continuous-delivery-of-react-app-with-jenkins-and-docker-8a1ae1511b86  Plan : Put Jenkins on Docker host and use Docker's built in registry, look into Docker git server that's lightweight enough. Use these for development and then push applications to Heroku and use travis-ci for Github.", 
            "title": "word salad"
        }, 
        {
            "location": "/software/jupyterhub/", 
            "text": "Installation\n\n\nFollowing the documentation from\n- \nhttps://github.com/jupyterhub/jupyterhub\n\n- \nhttps://github.com/jupyterhub/jupyterhub/wiki/Installation-of-Jupyterhub-on-remote-server\n\n- \nhttps://jupyterhub.readthedocs.io/en/latest/quickstart.html#installation\n\n\nadduser alex\nusermod -aG sudo alex\napt-get install python3-pip npm nodejs-legacy -y\nnpm install -g \nconfigurable-http-proxy\n\npip3 install jupyterhub\npip3 install --upgrade notebook\njupyterhub --no-ssl\n\n\n\n\nInstance is at \nhttp://ipaddress:8000.\n\n\nAdd \njupyterhub --no-ssl\n to \n/etc/rc.local\n to start on boot\n\n\nThere are further options in a configuration file that require more reading on authentication measures, etc. But this was more of a want to do then need to do, probably won't use this often with michigan having a GPU.\n\n\nservice\n\n\n\n\nhttps://github.com/jupyterhub/jupyterhub/wiki/Installation-of-Jupyterhub-on-remote-server", 
            "title": "JupyterHub"
        }, 
        {
            "location": "/software/jupyterhub/#installation", 
            "text": "Following the documentation from\n-  https://github.com/jupyterhub/jupyterhub \n-  https://github.com/jupyterhub/jupyterhub/wiki/Installation-of-Jupyterhub-on-remote-server \n-  https://jupyterhub.readthedocs.io/en/latest/quickstart.html#installation  adduser alex\nusermod -aG sudo alex\napt-get install python3-pip npm nodejs-legacy -y\nnpm install -g  configurable-http-proxy \npip3 install jupyterhub\npip3 install --upgrade notebook\njupyterhub --no-ssl  Instance is at  http://ipaddress:8000.  Add  jupyterhub --no-ssl  to  /etc/rc.local  to start on boot  There are further options in a configuration file that require more reading on authentication measures, etc. But this was more of a want to do then need to do, probably won't use this often with michigan having a GPU.", 
            "title": "Installation"
        }, 
        {
            "location": "/software/jupyterhub/#service", 
            "text": "https://github.com/jupyterhub/jupyterhub/wiki/Installation-of-Jupyterhub-on-remote-server", 
            "title": "service"
        }, 
        {
            "location": "/software/mkdocs/", 
            "text": "Installation\n\n\nUsing pip, \npip install mkdocs\n\n\nCreate a project with \nmkdocs new my-project\n and \ncd my-project\n\n\nStart the server with \nmkdocs serve -dev-addr=0.0.0.0:80 \n or whatever port you want\n\n\nBuild the site with mkdocs build to create the \nsite\n directory\n\n\nEdit \nmkdocs.yml\n to change the theme to readthedocs, title, author, name pages differently, etc.\n\n\nAdd new \n.md\n files in separate directories as necessary for desired layout\n\n\nAdd MathJaX extensions with \nextra_javascript:\n- \n\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML\"\n in\nmkdocs.yml`\n\n\nlinks\n\n\n\n\nhttps://github.com/Python-Markdown/markdown/wiki/Third-Party-Extensions#math--latex\n\n\nhttps://www.mkdocs.org/user-guide/configuration/#formatting-options\n\n\nhttps://stackoverflow.com/questions/27882261/mkdocs-and-mathjax\n\n\n\n\nhyperlinks\n\n\nRequire \n tags to be marked up.\n\n\nconvert dokuwiki markup\n\n\n# convert markup\nimport os\n\nif __name__ == '__main__':\n    wiki_files = []\n    for root, directories, filenames in os.walk('./wiki_files/'):\n        for filename in filenames:\n            if filename.endswith('.txt'):\n                wiki_files.append(os.path.join(root, filename))\n    for wiki_file in wiki_files:\n        with open(wiki_file, 'r') as infile:\n            lines = infile.readlines()\n        new_lines = []\n        for line in lines:\n            new_line = line.replace(r\n===\n, r\n#\n).\\\n                replace(r\n''\n, r\n`\n).\\\n                replace(r\ncode\n, r\n```\n).\\\n                replace(r\n/code\n, r\n```\n).\\\n                replace(r\n  -\n, r\n-\n).\\\n                replace(r\n  *\n, r\n*\n)\n            new_lines.append(new_line)\n        new_filename = wiki_file.\\\n            replace(r\nwiki_files\n, r\nwiki/docs\n).\\\n            replace(r\n.txt\n, r\n.md\n)\n        new_directory = '/'.join(new_filename.split('/')[:-1])\n        try:\n            os.mkdir(new_directory)\n        except:\n            pass\n        with open(new_filename, 'w') as outfile:\n            for line in new_lines:\n                outfile.write(line)\n\n\n\n\nAnd for converting the majority of hyperlinks:\n\n\nimport os\n\nif __name__ == '__main__':\n    wiki_files = []\n    for root, directories, filenames in os.walk('./wiki/docs/'):\n        for filename in filenames:\n            if filename.endswith('.md'):\n                wiki_files.append(os.path.join(root, filename))\n    for wiki_file in wiki_files:\n        with open(wiki_file, 'r') as infile:\n            text = infile.readlines()\n        new_lines = []\n        for line in text:\n            words = line.split()\n            new_words = []\n            for word in words:\n                if 'http' in word:\n                    new_words.append('\n' + word.replace(r\n`\n, r\n) + '\n')\n                else:\n                    new_words.append(word)\n            new_lines.append(' '.join(new_words))\n        with open(wiki_file, 'w') as outfile:\n            for line in new_lines:\n                outfile.write(line + '\\n')", 
            "title": "MkDocs"
        }, 
        {
            "location": "/software/mkdocs/#installation", 
            "text": "Using pip,  pip install mkdocs  Create a project with  mkdocs new my-project  and  cd my-project  Start the server with  mkdocs serve -dev-addr=0.0.0.0:80   or whatever port you want  Build the site with mkdocs build to create the  site  directory  Edit  mkdocs.yml  to change the theme to readthedocs, title, author, name pages differently, etc.  Add new  .md  files in separate directories as necessary for desired layout  Add MathJaX extensions with  extra_javascript:\n-  \"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML\"  in mkdocs.yml`", 
            "title": "Installation"
        }, 
        {
            "location": "/software/mkdocs/#links", 
            "text": "https://github.com/Python-Markdown/markdown/wiki/Third-Party-Extensions#math--latex  https://www.mkdocs.org/user-guide/configuration/#formatting-options  https://stackoverflow.com/questions/27882261/mkdocs-and-mathjax", 
            "title": "links"
        }, 
        {
            "location": "/software/mkdocs/#hyperlinks", 
            "text": "Require   tags to be marked up.", 
            "title": "hyperlinks"
        }, 
        {
            "location": "/software/mkdocs/#convert-dokuwiki-markup", 
            "text": "# convert markup\nimport os\n\nif __name__ == '__main__':\n    wiki_files = []\n    for root, directories, filenames in os.walk('./wiki_files/'):\n        for filename in filenames:\n            if filename.endswith('.txt'):\n                wiki_files.append(os.path.join(root, filename))\n    for wiki_file in wiki_files:\n        with open(wiki_file, 'r') as infile:\n            lines = infile.readlines()\n        new_lines = []\n        for line in lines:\n            new_line = line.replace(r === , r # ).\\\n                replace(r '' , r ` ).\\\n                replace(r code , r ``` ).\\\n                replace(r /code , r ``` ).\\\n                replace(r   - , r - ).\\\n                replace(r   * , r * )\n            new_lines.append(new_line)\n        new_filename = wiki_file.\\\n            replace(r wiki_files , r wiki/docs ).\\\n            replace(r .txt , r .md )\n        new_directory = '/'.join(new_filename.split('/')[:-1])\n        try:\n            os.mkdir(new_directory)\n        except:\n            pass\n        with open(new_filename, 'w') as outfile:\n            for line in new_lines:\n                outfile.write(line)  And for converting the majority of hyperlinks:  import os\n\nif __name__ == '__main__':\n    wiki_files = []\n    for root, directories, filenames in os.walk('./wiki/docs/'):\n        for filename in filenames:\n            if filename.endswith('.md'):\n                wiki_files.append(os.path.join(root, filename))\n    for wiki_file in wiki_files:\n        with open(wiki_file, 'r') as infile:\n            text = infile.readlines()\n        new_lines = []\n        for line in text:\n            words = line.split()\n            new_words = []\n            for word in words:\n                if 'http' in word:\n                    new_words.append(' ' + word.replace(r ` , r ) + ' ')\n                else:\n                    new_words.append(word)\n            new_lines.append(' '.join(new_words))\n        with open(wiki_file, 'w') as outfile:\n            for line in new_lines:\n                outfile.write(line + '\\n')", 
            "title": "convert dokuwiki markup"
        }, 
        {
            "location": "/software/plex/", 
            "text": "Installation\n\n\n\n\nDownload the \n.deb\n from Plex\n\n\nsudo dpkg -i plexmedia...\n\n\nGo to \nhttp://containerip:32400/web\n\n\nAdd NFS media directories as necessary\n\n\nConfiguring server name and media directories directly after installation may result in 'There was a problem saving these settings' warnings, but after a few minutes will disappear.\n\n\nlook for guides on downloading subtitles", 
            "title": "Plex"
        }, 
        {
            "location": "/software/plex/#installation", 
            "text": "Download the  .deb  from Plex  sudo dpkg -i plexmedia...  Go to  http://containerip:32400/web  Add NFS media directories as necessary  Configuring server name and media directories directly after installation may result in 'There was a problem saving these settings' warnings, but after a few minutes will disappear.  look for guides on downloading subtitles", 
            "title": "Installation"
        }, 
        {
            "location": "/software/rclone/", 
            "text": "Installation\n\n\n\n\nCreate a new Ubuntu container and follow the directions from \nhttps://rclone.org/install/\n after installing \ncurl, unzip\n\n\nCopy \n.rclone.conf\n to the home directory\n\n\nAdd the appropriate NFS directories as necessary for uploading\n\n\nInstall \ntmux\n to attach sessions as needed (Not working in LXC containers for some reason)\n\n\nUpload data using \nnohup rclone copy /source/path remote:/dest/path", 
            "title": "RClone"
        }, 
        {
            "location": "/software/rclone/#installation", 
            "text": "Create a new Ubuntu container and follow the directions from  https://rclone.org/install/  after installing  curl, unzip  Copy  .rclone.conf  to the home directory  Add the appropriate NFS directories as necessary for uploading  Install  tmux  to attach sessions as needed (Not working in LXC containers for some reason)  Upload data using  nohup rclone copy /source/path remote:/dest/path", 
            "title": "Installation"
        }, 
        {
            "location": "/software/reverse_proxy/", 
            "text": "//\n__Did not finish - not wanting to expose network to WAN__\n//\n\n\nInstallation\n\n\nFollowing \nhttps://www.htpcguides.com/secure-nginx-reverse-proxy-with-lets-encrypt-on-ubuntu-16-04-lts/\n and \nhttps://calvin.me/port-forward-web-servers-in-pfsense-2/\n and port forwarding 80, 443 in pfsense while also changing web GUI port to not expose it on WAN.\n\n\napt-get update\napt-get install nginx -y\nunlink /etc/nginx/sites-enabled/default\n\n\n\n\nAnd fill it with\n\n\nserver {\nlisten 80 default_server;\nlisten [::]:80 default_server;\nserver_name ajdurbin2.ddns.net 192.168.1.207;\nroot /var/www/html;\nindex index.html index.htm index.nginx-debian.html;\n\nlocation ~ /.well-known {\nallow all;\n}\n}", 
            "title": "Reverse Proxy"
        }, 
        {
            "location": "/software/reverse_proxy/#installation", 
            "text": "Following  https://www.htpcguides.com/secure-nginx-reverse-proxy-with-lets-encrypt-on-ubuntu-16-04-lts/  and  https://calvin.me/port-forward-web-servers-in-pfsense-2/  and port forwarding 80, 443 in pfsense while also changing web GUI port to not expose it on WAN.  apt-get update\napt-get install nginx -y\nunlink /etc/nginx/sites-enabled/default  And fill it with  server {\nlisten 80 default_server;\nlisten [::]:80 default_server;\nserver_name ajdurbin2.ddns.net 192.168.1.207;\nroot /var/www/html;\nindex index.html index.htm index.nginx-debian.html;\n\nlocation ~ /.well-known {\nallow all;\n}\n}", 
            "title": "Installation"
        }, 
        {
            "location": "/software/rstudio_rshiny/", 
            "text": "Installation\n\n\nFollowing the excellent tutorial from \nhttps://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/#safety-first\n\n\nadduser alex\ngpasswd -a alex sudo\nsu - alex\n\n# install R\nsudo sh -c 'echo \ndeb \nhttp://cran.rstudio.com/bin/linux/ubuntu\n xenial/\n \n /etc/apt/sources.list'\ngpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9\ngpg -a --export E084DAB9 | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install r-base -y\n\n# common dependencies for R packages\nsudo apt-get install libcurl4-gnutls-dev libxml2-dev libssl-dev -y\n\n# install packages using this method so they're able to be installed for shiny user\nsudo su - -c \nR -e \\\ninstall.packages('devtools', \nrepos='http://cran.rstudio.com/')\\\n\nsudo su - -c \nR -e \\\ninstall.packages('rmarkdown', \nrepos='http://cran.rstudio.com/')\\\n\n...\n\n# install rstudio server\nsudo apt-get install gdebi-core -y\nwget \nhttps://download2.rstudio.org/rstudio-server-1.1.442-amd64.deb\n\nsudo gdebi rstudio-server-1.1.442-amd64.deb\n# go to \nhttp//ip:8787\n for gui\n\n# install shiny server\nsudo su - -c \nR -e \\\ninstall.packages('shiny', \nrepos='http://cran.rstudio.com/')\\\n\nwget \nhttps://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.7.907-amd64.deb\n\nsudo gdebi shiny-server-1.5.6.875-amd64.deb\n# check homepage at \nhttp://ip:3838\n\n\n# set up proper user permissions to share /srv/shiny-server/ between user and shiny user\nsudo groupadd shiny-apps\nsudo usermod -aG shiny-apps alex\nsudo usermod -aG shiny-apps shiny\ncd /srv/shiny-server\nsudo chown -R alex:shiny-apps .\nsudo chmod g+w .\nsudo chmod g+s .\n\n# make /srv/shiny-server/ a git repository for pulling apps\n# have shiny-server repo in home folder and push to repo before pulling in /srv/shiny-server/ for running\nsudo apt-get install git -y\ncd /srv/shiny-server/\ngit init\n\n\n\n\nWe can also host interactive R Markdown documents if they're named \nindex.Rmd\n with \nruntime: shiny\n in the YAML header.", 
            "title": "RStudio/RShiny"
        }, 
        {
            "location": "/software/rstudio_rshiny/#installation", 
            "text": "Following the excellent tutorial from  https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/#safety-first  adduser alex\ngpasswd -a alex sudo\nsu - alex\n\n# install R\nsudo sh -c 'echo  deb  http://cran.rstudio.com/bin/linux/ubuntu  xenial/    /etc/apt/sources.list'\ngpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9\ngpg -a --export E084DAB9 | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install r-base -y\n\n# common dependencies for R packages\nsudo apt-get install libcurl4-gnutls-dev libxml2-dev libssl-dev -y\n\n# install packages using this method so they're able to be installed for shiny user\nsudo su - -c  R -e \\ install.packages('devtools',  repos='http://cran.rstudio.com/')\\ \nsudo su - -c  R -e \\ install.packages('rmarkdown',  repos='http://cran.rstudio.com/')\\ \n...\n\n# install rstudio server\nsudo apt-get install gdebi-core -y\nwget  https://download2.rstudio.org/rstudio-server-1.1.442-amd64.deb \nsudo gdebi rstudio-server-1.1.442-amd64.deb\n# go to  http//ip:8787  for gui\n\n# install shiny server\nsudo su - -c  R -e \\ install.packages('shiny',  repos='http://cran.rstudio.com/')\\ \nwget  https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.7.907-amd64.deb \nsudo gdebi shiny-server-1.5.6.875-amd64.deb\n# check homepage at  http://ip:3838 \n\n# set up proper user permissions to share /srv/shiny-server/ between user and shiny user\nsudo groupadd shiny-apps\nsudo usermod -aG shiny-apps alex\nsudo usermod -aG shiny-apps shiny\ncd /srv/shiny-server\nsudo chown -R alex:shiny-apps .\nsudo chmod g+w .\nsudo chmod g+s .\n\n# make /srv/shiny-server/ a git repository for pulling apps\n# have shiny-server repo in home folder and push to repo before pulling in /srv/shiny-server/ for running\nsudo apt-get install git -y\ncd /srv/shiny-server/\ngit init  We can also host interactive R Markdown documents if they're named  index.Rmd  with  runtime: shiny  in the YAML header.", 
            "title": "Installation"
        }, 
        {
            "location": "/software/torrent_server/", 
            "text": "Installation\n\n\n\n\nCreate new container using the TurnKey Torrent Server template with default HDD size and 4GB RAM, 1GB swap with software password\n\n\nStarting container and logging in as root with software password begins the TurnKey GUI setup, use software password, skip automatic backups, skip email notifications, and install security updates\n\n\nruTorrent GUI is found at \nhttps://torrentserverip:12322\n with username admin and software password\n\n\nDownload directory is \n/srv/storage/download\n\n\nCreate mountpoint to FreeNAS dataset through Proxmox NFS mount by editing \n/etc/pve/lxc/\ncontainerid\n.conf\n with \nmp0: /proxmox/nfs/mount,mp=/srv/storage/download\n\n\nRestart container\n\n\nCheck \ndu -h\n afterwards to see newly available storage\n\n\nFrom \nhttp://www.beginninglinux.com/home/data-compression/unrar-all-files-from-multiple-subdirectories-at-once,\n unrar a directory into another directory \nunrar e -r -o- /path/to/torrents/*.rar /path/to/save/\n.", 
            "title": "Torrent Server"
        }, 
        {
            "location": "/software/torrent_server/#installation", 
            "text": "Create new container using the TurnKey Torrent Server template with default HDD size and 4GB RAM, 1GB swap with software password  Starting container and logging in as root with software password begins the TurnKey GUI setup, use software password, skip automatic backups, skip email notifications, and install security updates  ruTorrent GUI is found at  https://torrentserverip:12322  with username admin and software password  Download directory is  /srv/storage/download  Create mountpoint to FreeNAS dataset through Proxmox NFS mount by editing  /etc/pve/lxc/ containerid .conf  with  mp0: /proxmox/nfs/mount,mp=/srv/storage/download  Restart container  Check  du -h  afterwards to see newly available storage  From  http://www.beginninglinux.com/home/data-compression/unrar-all-files-from-multiple-subdirectories-at-once,  unrar a directory into another directory  unrar e -r -o- /path/to/torrents/*.rar /path/to/save/ .", 
            "title": "Installation"
        }, 
        {
            "location": "/software/travis-ci/", 
            "text": "Travis-CI is the most popular continuous integration/continuous development service. It easily integrates with GitHub, though there doesn't really seem to be a local server image. The syntax is YAML and easier to write than Jenkins' pipeline syntax, plus we don't need to worry about having a local Jenkins server and build slave configured for Docker builds.\n\n\n\n\nhttps://docs.travis-ci.com/user/docker/\n\n\nhttps://docs.travis-ci.com/user/deployment/heroku/", 
            "title": "Travis-CI"
        }, 
        {
            "location": "/software/docker/general_use/", 
            "text": "useful commands\n\n\n\n\nstop all containers: \ndocker kill $(docker ps -q)\n\n\nremove all containers: \ndocker rm $(docker ps -a -q)\n\n\nremove all images: \ndocker rmi $(docker images -q)\n\n\n\n\nscratch\n\n\n\n\nhttps://github.com/veggiemonk/awesome-docker\n\n\nuse traefik for reverse proxy, ssl encryption over nginx solution\n\n\ninternal and external registry\n\n\ndocker compose with persistent storage with scripted jobs to pull and re-deploy latest images\n\n\nkubernetes - minikube for single node host\n\n\nnvidia docker for deep learning images? not sure about these, look to be solution to installing cuda in a separate container?\n\n\ndocker cloud\n\n\ndocker machine, provision multiple virtual docker hosts, \nhttps://docs.docker.com/machine/overview/\n\n\n\n\nResources\n\n\n\n\nhttps://www.digitalocean.com/community/tutorials/how-to-remove-docker-images-containers-and-volumes\n\n\nhttps://docs.docker.com/engine/reference/commandline/volume_rm/#usage\n\n\nhttps://docs.docker.com/docker-hub/repos/\n\n\nhttps://docs.docker.com/engine/reference/commandline/build/#tarball-contexts\n\n\nhttps://github.com/veggiemonk/awesome-docker#cicd", 
            "title": "General Usage"
        }, 
        {
            "location": "/software/docker/general_use/#useful-commands", 
            "text": "stop all containers:  docker kill $(docker ps -q)  remove all containers:  docker rm $(docker ps -a -q)  remove all images:  docker rmi $(docker images -q)", 
            "title": "useful commands"
        }, 
        {
            "location": "/software/docker/general_use/#scratch", 
            "text": "https://github.com/veggiemonk/awesome-docker  use traefik for reverse proxy, ssl encryption over nginx solution  internal and external registry  docker compose with persistent storage with scripted jobs to pull and re-deploy latest images  kubernetes - minikube for single node host  nvidia docker for deep learning images? not sure about these, look to be solution to installing cuda in a separate container?  docker cloud  docker machine, provision multiple virtual docker hosts,  https://docs.docker.com/machine/overview/", 
            "title": "scratch"
        }, 
        {
            "location": "/software/docker/general_use/#resources", 
            "text": "https://www.digitalocean.com/community/tutorials/how-to-remove-docker-images-containers-and-volumes  https://docs.docker.com/engine/reference/commandline/volume_rm/#usage  https://docs.docker.com/docker-hub/repos/  https://docs.docker.com/engine/reference/commandline/build/#tarball-contexts  https://github.com/veggiemonk/awesome-docker#cicd", 
            "title": "Resources"
        }, 
        {
            "location": "/software/docker/installation/", 
            "text": "Ubuntu Server Image\n\n\nDocker cannot run inside LXC containers, so we need to make a full virtual machine for it. Ubuntu server is minimal enough for this task.\n\n\nStart the VM with ample disk space and memory and follow installation. Skip HTTP proxy. Manual network installation. Select standard system utilities and OpenSSH server.\n\n\nDocker Installation\n\n\nhttps://docs.docker.com/install/linux/docker-ce/ubuntu/\n\n\nsudo apt-get update\nsudo apt-get install \\\n\napt-transport-https\n \\\nca-certificates \\\ncurl \\\nsoftware-properties-common\ncurl -fsSL \nhttps://download.docker.com/linux/ubuntu/gpg\n | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nsudo add-apt-repository \\\n\ndeb [arch=amd64] \nhttps://download.docker.com/linux/ubuntu\n \\\n$(lsb_release -cs) \\\nstable\n\nsudo apt-get update\nsudo apt-get install docker-ce\nsudo docker run hello-world\nsudo systemctl enable docker\n\n\n\n\nFrom\n- \nhttp://www.littlebigextra.com/how-to-enable-remote-rest-api-on-docker-host/\n\n- \nhttps://success.docker.com/article/how-do-i-enable-the-remote-api-for-dockerd\n\n\nTo enable the docker API from the server, edit \n/lib/systemd/system/docker.service\n with \nExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2376\n and then do \nsudo systemctl daemon-reload \n sudo service docker restart\n. Then test the server is accessible with `curl \nhttp://dockerserverip:2376/images/json\n to get a JSON string of the docker containers installed.\n\n\nTo manage Docker without \nsudo\n:\n\n\nsudo groupadd docker\nsudo usermod -aG docker $USER\nexit # then log back in\n\n\n\n\nDocker Hub\n\n\nCreate an account on Docker Hub. Create a private repository //homelab//.\n\n\nDocker Registry\n\n\n\n\nhttps://docs.docker.com/registry/deploying/#get-a-certificate\n\n\nhttps://docs.docker.com/registry/deploying/\n\n\nhttp://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/\n\n\nhttp://tech.paulcz.net/2016/01/secure-docker-with-tls/\n\n\n\n\nDocker Compose\n\n\nTool for defining and running multi-container Docker applications \nhttps://docs.docker.com/compose/\n\n\nDocker Swarm\n\n\nFor a cluster of Docker containers. Though almost everyone uses Kubernetes for this now.", 
            "title": "Installation"
        }, 
        {
            "location": "/software/docker/installation/#ubuntu-server-image", 
            "text": "Docker cannot run inside LXC containers, so we need to make a full virtual machine for it. Ubuntu server is minimal enough for this task.  Start the VM with ample disk space and memory and follow installation. Skip HTTP proxy. Manual network installation. Select standard system utilities and OpenSSH server.", 
            "title": "Ubuntu Server Image"
        }, 
        {
            "location": "/software/docker/installation/#docker-installation", 
            "text": "https://docs.docker.com/install/linux/docker-ce/ubuntu/  sudo apt-get update\nsudo apt-get install \\ apt-transport-https  \\\nca-certificates \\\ncurl \\\nsoftware-properties-common\ncurl -fsSL  https://download.docker.com/linux/ubuntu/gpg  | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nsudo add-apt-repository \\ deb [arch=amd64]  https://download.docker.com/linux/ubuntu  \\\n$(lsb_release -cs) \\\nstable \nsudo apt-get update\nsudo apt-get install docker-ce\nsudo docker run hello-world\nsudo systemctl enable docker  From\n-  http://www.littlebigextra.com/how-to-enable-remote-rest-api-on-docker-host/ \n-  https://success.docker.com/article/how-do-i-enable-the-remote-api-for-dockerd  To enable the docker API from the server, edit  /lib/systemd/system/docker.service  with  ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2376  and then do  sudo systemctl daemon-reload   sudo service docker restart . Then test the server is accessible with `curl  http://dockerserverip:2376/images/json  to get a JSON string of the docker containers installed.  To manage Docker without  sudo :  sudo groupadd docker\nsudo usermod -aG docker $USER\nexit # then log back in", 
            "title": "Docker Installation"
        }, 
        {
            "location": "/software/docker/installation/#docker-hub", 
            "text": "Create an account on Docker Hub. Create a private repository //homelab//.", 
            "title": "Docker Hub"
        }, 
        {
            "location": "/software/docker/installation/#docker-registry", 
            "text": "https://docs.docker.com/registry/deploying/#get-a-certificate  https://docs.docker.com/registry/deploying/  http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/  http://tech.paulcz.net/2016/01/secure-docker-with-tls/", 
            "title": "Docker Registry"
        }, 
        {
            "location": "/software/docker/installation/#docker-compose", 
            "text": "Tool for defining and running multi-container Docker applications  https://docs.docker.com/compose/", 
            "title": "Docker Compose"
        }, 
        {
            "location": "/software/docker/installation/#docker-swarm", 
            "text": "For a cluster of Docker containers. Though almost everyone uses Kubernetes for this now.", 
            "title": "Docker Swarm"
        }, 
        {
            "location": "/software/docker/registry/", 
            "text": "Want\n\n\n\n\nprivate, insecure registry\n\n\nmultiple registries, local and docker hub\n\n\n\n\nInstallation\n\n\nWithout an SSL certifcate, this registry is only accessible from the Docker host. For our purposes this is okay. \nhttps://docs.docker.com/registry/deploying/#copy-an-image-from-docker-hub-to-your-registry\n\n\ndocker run -d \\\n-p 5000:5000 \\\n--restart=always \\\n--name registry \\\nregistry:2\n\ndocker pull ubuntu:16.04\ndocker tag ubuntu:16.04 localhost:5000/my-ubuntu\ndocker push localhost:5000/my-ubuntu\ndocker image remove ubuntu:16.04\ndocker image remove localhost:5000/my-ubuntu\ndocker pull localhost:5000/my-ubuntu\ndocker container stop registry\ndocker container stop registry \n docker container rm -v registry\ncurl \nhttp://localhost:5000/v2/_catalog\n # to see list of docker images\n\n\n\n\nlinks\n\n\n\n\nhttps://docs.docker.com/registry/deploying/#get-a-certificate\n # deploy registry server\n\n\nhttps://docs.docker.com/registry/insecure/#docker-still-complains-about-the-certificate-when-using-authentication\n\n\nhttps://www.juandebravo.com/2017/01/22/docker-insecure-registry\n\n\nhttps://coderwall.com/p/dtwc1q/insecure-and-self-signed-private-docker-registry-with-boot2docker\n\n*", 
            "title": "Registry"
        }, 
        {
            "location": "/software/docker/registry/#want", 
            "text": "private, insecure registry  multiple registries, local and docker hub", 
            "title": "Want"
        }, 
        {
            "location": "/software/docker/registry/#installation", 
            "text": "Without an SSL certifcate, this registry is only accessible from the Docker host. For our purposes this is okay.  https://docs.docker.com/registry/deploying/#copy-an-image-from-docker-hub-to-your-registry  docker run -d \\\n-p 5000:5000 \\\n--restart=always \\\n--name registry \\\nregistry:2\n\ndocker pull ubuntu:16.04\ndocker tag ubuntu:16.04 localhost:5000/my-ubuntu\ndocker push localhost:5000/my-ubuntu\ndocker image remove ubuntu:16.04\ndocker image remove localhost:5000/my-ubuntu\ndocker pull localhost:5000/my-ubuntu\ndocker container stop registry\ndocker container stop registry   docker container rm -v registry\ncurl  http://localhost:5000/v2/_catalog  # to see list of docker images", 
            "title": "Installation"
        }, 
        {
            "location": "/software/docker/registry/#links", 
            "text": "https://docs.docker.com/registry/deploying/#get-a-certificate  # deploy registry server  https://docs.docker.com/registry/insecure/#docker-still-complains-about-the-certificate-when-using-authentication  https://www.juandebravo.com/2017/01/22/docker-insecure-registry  https://coderwall.com/p/dtwc1q/insecure-and-self-signed-private-docker-registry-with-boot2docker \n*", 
            "title": "links"
        }, 
        {
            "location": "/statistics/scratch/", 
            "text": "GLM\n\n\nRegression\n\n\nExperimental Design\n\n\nSampling\n\n\nStatistics tests like paired t tests, etc\n\n\nconfidence intervals\n\n\nbootstrapping\n\n\nbayesian inference\n\n\ngibbs sampling\n\n\ngeneralized additive models\n\n\nhttps://chi-feng.github.io/mcmc-demo/\n visualization of mcmc\n\n\nhttps://www.countbayesie.com/books/", 
            "title": "Scratch"
        }
    ]
}